---
title: "DANL 210: Data Preparation and Management<br>Homework Assignment 4"
author: "Byeong-Hak Choe"

format: 
  html:
    code-fold: false
    code-summary: "Show the code"
    code-tools: true
execute:
  echo: true
  eval: false
  message: false
  warning: false
---

# Direction for Homework Assignment 4

- Go to the following website for webscrapping
  - [http://books.toscrape.com](http://books.toscrape.com)
 

## Q1a
- Provide your Python Selenium code to create the CSV file of DataFrame with the following variables:
  - book category (e.g., "Travel") 
  - book title (e.g., "It's Only the Himalayas")
  - book price (e.g., "£45.17")
  

- Your code should scrap all the books.

```{python}
# loading libraries
import pandas as pd
import os
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium import webdriver
```

```{python}
# working directory
wd_path = 'PATHNAME_OF_YOUR_WORKING_DIRECTORY'
os.chdir(wd_path)
os.getcwd()  ## checking working directory
```

```{python}
# chrome browser with options
options = Options()
options.add_argument("window-size=1400,1200")
driver = webdriver.Chrome(chrome_options=options,
                          executable_path="chromedriver")
url = "http://books.toscrape.com/"
driver.get(url)
```

```{python}
# loop to scrap data

# number of categories
n_categories = driver.find_element(
    By.XPATH, '/html/body/div/div/div/aside/div[2]/ul/li/ul')
n_categories = n_categories.find_elements(
    By.TAG_NAME, 'li')  # number of categories


df = pd.DataFrame()
for i in range(1, len(n_categories) + 1): 
    category = driver.find_element(
        By.XPATH, '//*[@id="default"]/div/div/div/aside/div[2]/ul/li/ul/li['+str(i)+']/a')
    cat = pd.DataFrame([category.text])
    category.click()

    while True:
        n_books = driver.find_elements(By.CLASS_NAME, 'row')
        # by checking len(n_books.find_elements(By.TAG_NAME, 'li'))
        n_books = n_books[2].find_elements(By.TAG_NAME, 'li')

        for j in range(1, len(n_books)+1):
            title = driver.find_element(
                By.XPATH, '//*[@id="default"]/div/div/div/div/section/div[2]/ol/li['+str(j)+']/article/h3/a').get_attribute('title')
            title = pd.DataFrame([title])

            price = driver.find_element(
                By.XPATH, '//*[@id="default"]/div/div/div/div/section/div[2]/ol/li['+str(j)+']/article/div[2]/p[1]')
            price = pd.DataFrame([price.text])

            data = pd.concat([cat, title, price], axis=1)
            df = pd.concat([df, data])  # df = df.append(data)

        try:
            next_btn = driver.find_element(
                By.PARTIAL_LINK_TEXT, 'next')
        except:
            next_btn = []

        if next_btn != []:
            next_btn.click()
        else:
            break


df.columns = ['category', 'title', 'price']

# converting to numeric
df['price'] = (
    df['price']
    .str.replace('£', '')
    .astype(float)
)

df.to_csv("data/webscrapping_books_2023_0512.csv",
          index=False, encoding='utf-8-sig')

driver.quit()

```

<br>

## Q1b

Find the five most expensive books in each category.

```{python}
# For various ranking algorithms, check out the Lab 1.

q1b = (
    df
    .set_index('title')
    .groupby('category')['price']
    .nlargest(5)
    .reset_index()
)

q1b.columns
```

<br>

## Q1c

Calculate the mean, median, standard deviation of book price for each book category.

```{python}
q1c = df.groupby('category')['price'].describe()
```

