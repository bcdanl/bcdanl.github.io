---
title: "Python Lab 4 - Practice"
author: "Byeong-Hak Choe"
# include-after-body: [beer_map.html, beer_map_ny.html]
editor: visual
---

## Load Libraries

```{python, eval = F}
import pandas as pd
import numpy as np
from skimpy import skim
import seaborn as sns
```

# Question 1

## Load `DataFrame`

```{python, eval = F}
#| warning: false
#| message: false
billboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')
ny_pincp = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_wide.csv')
covid = pd.read_csv('https://bcdanl.github.io/data/covid19_cases.csv')
```

## Q1a

-   Describe how the distribution of rating varies across week 1, week 2, and week 3 using the **faceted** histogram.

```{python}
#| warning: false
#| message: false

billboard_long = billboard.melt(
  id_vars = ["year", "artist", "track", "time", "date.entered"],
  var_name = "week",
  value_name = "rating",
)


# The .isin() method is used to check whether each value in the "week" column is present in the given list of values. 
billboard_wk1_2_3 = billboard_long.loc[
    billboard_long['week'].isin(['wk1', 'wk2', 'wk3'])
    ]

sns.displot(billboard_wk1_2_3,
             x = 'rating',
             row = 'week'
             )

```

## Q1b

-   Which artist(s) have the most number of tracks in `billboard` DataFrame?

```{python}
#| warning: false
#| message: false

billboard_songs = (
    billboard[["artist", "track"]]
    .drop_duplicates()        # drops duplicate observations.
    .drop("track", axis = 1)  # drops variable, track.
    .value_counts()
    )
```

## Q1c

-   Make `ny_pincp` longer.

```{python}
#| warning: false
#| message: false

ny_pincp_long = ny_pincp.melt(
        id_vars = ['fips', 'geoname'],
        var_name = 'year',
        value_name = 'pincp'
        )
    

```

## Q1d

-   Make a wide-form DataFrame of `covid` whose variable names are from `countriesAndTerritories` and values are from `cases`.

```{python}
#| warning: false
#| message: false

covid_wide = (
    covid
    .pivot_table(index = 'date', 
           columns = 'countriesAndTerritories', 
           values = 'cases')
    )
    
```

## Q1e

-   Use the wide-form DataFrame of `covid` to find the top 10 countries for which their cases are highly correlated with USA's cases using `DataFrame.corr()`

```{python}

corr_usa = (
    covid_wide.corr()
    .sort_values(by = 'USA', ascending = False)
    .USA
    )

```



<br><br><br>

# Question 2

## Load DataFrame for Q2a and Q2b
```{python}
paidsearch = pd.read_csv('https://bcdanl.github.io/data/paidsearch.csv')
```

- Variable description
  - `dma`: an identification number of a designated market (DM) area `i` (e.g., Boston, Los Angeles)
  - `treatment_period`: 0 if date is before May 22, 2012 and 1 after.
  - `search_stays_on`: 1 if the paid-search goes off in dma `i`, 0 otherwise.
  - `revenue`: eBay's sales revenue for dma `i` and date `t`


## Q2a
Summarize the mean vale of `revenue` for each group of `search_stays_on` and for each `date`.

```{python}
q2a = (
    paidsearch                                              # Use paidsearch dataframe
    .groupby(['search_stays_on', 'date'])                    # Group by 'search_stays_on' and 'date' columns
    .agg( {'revenue': 'mean'} )                        # Compute mean revenue for each group
    .reset_index()                                          # Reset index to convert to DataFrame
)

```

<br>


## Q2b
Calculate the log difference between mean revenues in each group of `search_stays_on`. (This is the log of the average revenue in group of search_stays_on == 1 minus the log of the average revenue in group of search_stays_on == 0.)


- For example, consider the following two observations:
```{python}
# date        the daily mean vale of `revenue`   search_stays_on
# 1-Apr-12    93650.68                           0
# 1-Apr-12    120277.57                          1
```

- The log difference of daily mean revenues between the two group of `search_stays_on` for date 1-Apr-12 is log(120277.57) - log(93650.68).

```{python}
# Compute mean revenue for each group of 'search_stays_on' and 'date' in Q1b DataFrame
q2b = (
    paidsearch
    .groupby(['search_stays_on', 'date'])                             # Group by 'search_stays_on' and 'date' columns
    .agg( {'revenue' : 'mean'} )                                 # Compute mean revenue for each group
    .reset_index()
    .sort_values(by=['date', 'search_stays_on'])                       # Sort by 'date' and 'search_stays_on' columns
    .pivot(index='date', columns='search_stays_on', values='revenue')  # Pivot 'search_stays_on' column to create wide format
)

# rename columns of q1b
q2b.columns = ['rev_control', 'rev_treat']

q2b = q2b.assign(diff_log = 
                 np.log(q2b['rev_control']) - 
                 np.log(q2b['rev_treat']) ) # Compute 'diff_log' column

```

<br>



## Load DataFrame for Q2c, Q2d, and Q2e
```{python}
paid_search = pd.read_csv('https://bcdanl.github.io/data/paid_search.csv')
```

<br>



## Q2C
Sort `paid_search` by `DM` and `May22_2012` in ascending order.

```{python}
paid_search = paid_search.sort_values(
    by = ['DM', 'May22_2012']
    )
```

<br>



## Q2d
For each `DM`, calculate the difference between `log_revenue` before and after `May22_2012`.

```{python}
q2d = (
     paid_search
    .set_index(['DM', 'May22_2012', 'no_paid_search'])
    .assign(after = lambda x: x.groupby('DM')['log_revenue'].transform('last'),
            before = lambda x: x.groupby('DM')['log_revenue'].transform('first'))
    .reset_index()
    .drop(columns=['log_revenue', 'May22_2012'])
    .drop_duplicates()
    .assign(diff = lambda x: x['after'] - x['before'])
)
```

<br>



## Q2e
- Consider the DataFrame from Q2d. 

- For each group of `no_paid_search`, calculate the mean value of the difference between `log_revenue` before and after `May22_2012` .

- What is the difference in the mean values?

```{python}
q2e = (
     q2d
    .drop(columns=['DM', 'after', 'before'])
    .groupby('no_paid_search')
    .mean()
)

q2e['diff'].iloc[1] - q2e['diff'].iloc[0]  # treatment effect of turning off search-engine marketing
```

- After the paid-search went off, sales revenue decreased by 0.66% 
  - Was eBay's paid search worth it?
