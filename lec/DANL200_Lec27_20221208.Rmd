---
title: "Lecture 27"
subtitle: "DANL 200: Introduction to Data Analytics"
author: "Byeong-Hak Choe"
institute: "SUNY Geneseo"
date: "December 8, 2022"
output:
  xaringan::moon_reader:
    css: 
      - default
      - css/nhsr.css
      - css/nhsr-fonts.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    lib_dir: libs
    seal: false
    nature:
      highlightStyle: googlecode
      highlightLines: true
      highlightLanguage: ["r"]
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      after_body: [css/insert-logo.html]
---

```{r setup, include = FALSE}
library(knitr)
library(kableExtra)
library(tidyverse)
library(NHSRtheme)
library(fontawesome)

# set default options
opts_chunk$set(echo = FALSE,
               fig.width = 7.252,
               fig.height = 4,
               comment = "#",
               dpi = 300)

knitr::knit_engines$set("markdown")

xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::use_clipboard()
xaringanExtra::use_webcam()
xaringanExtra::use_broadcast()
xaringanExtra::use_share_again()
xaringanExtra::style_share_again(
  share_buttons = c("twitter", "linkedin", "pocket")
)


xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = F  #<<
)

# uncomment the following lines if we want to use the NHS-R theme colours by default
# scale_fill_continuous <- partial(scale_fill_nhs, discrete = FALSE)
# scale_fill_discrete <- partial(scale_fill_nhs, discrete = TRUE)
# scale_colour_continuous <- partial(scale_colour_nhs, discrete = FALSE)
# scale_colour_discrete <- partial(scale_colour_nhs, discrete = TRUE)
```


class: title-slide, left, bottom

# `r rmarkdown::metadata$title`
----
## **`r rmarkdown::metadata$subtitle`**
### `r rmarkdown::metadata$author`
### `r rmarkdown::metadata$date`



---
# Announcement
### <p style="color:#00449E"> Student Course Experience (SCE) Survey

- Effective Fall 2022, the Student Course Experience (SCE) survey replaces the Student Observation of Faculty Instruction (SOFI) survey.

- In a web browser, students should visit their [myGeneseo](https://my.geneseo.edu/dashboard) portal, then select KnightWeb, Surveys, then SCE (formerly SOFI) Surveys.



---
# Announcement
### <p style="color:#00449E"> Final Exam

- Final Exam Schedule is mentioned in page 2 in our syllabus.


- School's Final Exam Schedule is available [here](https://www.geneseo.edu/sites/default/files/sites/registrar/2022.03.30%20202209%20Exam%20Schedule.pdf).


- Final Exam covers the following topics:
  - Loading CSV files
  - Summary Statistics
  - Data Visualization
  - Data Transformation
  - Pivoting/Separating/Uniting/Joining
  - String/Factor/Date-Time Data
  - Linear Regression

---
class: inverse, center, middle

# Linear Regression using **R**
<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>


---
# Linear Regression in R
### <p style="color:#00449E"> R commands to do EDA and linear regression analysis

.panelset[
.panel[.panel-name[Data]
```{r, echo = T, eval = F}
library(tidyverse)
psub <- readRDS( url('https://bcdanl.github.io/data/psub.RDS') )
set.seed(54321)
gp <- runif( nrow(psub) )
# Set up factor variables if needed.
dtrain <- filter(psub, gp >= .5)
dtest <- filter(psub, gp < .5)
```
]

.panel[.panel-name[EDA]
```{r, echo = T, eval = F}
library(skimr)
sum_dtrain <- skim( select(dtrain,
                           PINCP, AGEP, SEX, SCHL) )
library(GGally)
ggpairs( select(dtrain,
                PINCP, AGEP, SEX, SCHL) )
# MORE VISUALIZATIONS ARE RECOMMENDED
```
]


.panel[.panel-name[Training]
```{r, echo = T, eval = F}
model_1 <- lm( PINCP ~ AGEP + SEX,
               data = dtrain )
model_2 <- lm( PINCP ~ AGEP + SEX + SCHL,
               data = dtrain )
```
]


.panel[.panel-name[Summary 1]
- Summary with base-R:
```{r, echo = T, eval = F}
summary(model_1)
summary(model_2)
coef(model_1)
coef(model_2)
# Using the model.matrix() function on our linear model object, 
# we can get the data matrix that underlies our regression. 
df_model_2 <- as_tibble( model.matrix(model_2) )
```

]



.panel[.panel-name[Summary 2]

- Summary with R packages:
```{r, echo = T, eval = F}
# install.packages(c("stargazer", "broom"))
library(stargazer)
library(broom)
stargazer(model_1, model_2, 
          type = 'text')  # from the stargazer package
sum_model_2 <- tidy(model_2)  # from the broom package
# Consider filter() to keep statistically significant beta estimates
```


]


.panel[.panel-name[Betas in plot]
```{r, echo = T, eval = F}
ggplot(sum_model_2) +
  geom_pointrange( aes(x = term, 
                       y = estimate,
                       ymin = estimate - 2*std.error,
                       ymax = estimate + 2*std.error ) ) +
  coord_flip()
```

]

.panel[.panel-name[Prediction]
```{r, echo = T, eval = F}
dtest <- dtest %>% 
  mutate( pred_1 = predict(model_1, newdata = dtest),
          pred_2 = predict(model_2, newdata = dtest) )
```
]

.panel[.panel-name[Actual vs. Prediction Plot]
```{r, echo = T, eval = F}
ggplot( data = dtest, 
        aes(x = pred_2, y = PINCP) ) +
  geom_point( alpha = 0.2, color = "darkgray" ) +
  geom_smooth( color = "darkblue" ) +  
  geom_abline( color = "red", linetype = 2 )  # y = x, perfect prediction line

```


]


.panel[.panel-name[Residual Plot]

```{r, echo = T, eval = F}
ggplot(data = dtest, 
       aes(x = pred_2, y = PINCP - pred_2)) +
  geom_point(alpha = 0.2, color = "darkgray") +
  geom_smooth( color = "darkblue" ) +   
  geom_hline( aes( yintercept = 0 ),  # perfect prediction 
              color = "red", linetype = 2) 

```

]

]

---
class: inverse, center, middle

# Linear Regression with Interaction Terms
<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---
# Linear Regression with Interaction Terms
### <p style="color:#00449E"> Motivation

- Does the relationship between education and income vary by gender?

  - Suppose we are interested in knowing whether women are being compensated unequally despite having the same levels of education and preparation as men do.

  - How can linear regression address the question above?



---
# Linear Regression with Interaction Terms
### <p style="color:#00449E"> Model

- The linear regression with an interaction between explanatory variables $X_{1}$ and $X_{2}$ are:

$$Y_{\texttt{i}} \,=\, b_{0} \,+\, b_{1}\,X_{1,\texttt{i}} \,+\, b_{2}\,X_{2,\texttt{i}} \,+\, b_{3}\,X_{1,\texttt{i}}\times \color{Red}{X_{2,\texttt{i}}} \,+\, e_{\texttt{i}},$$

- where
  - $\texttt{i}\;$: $\;\;\texttt{i}$-th observation in the training data.frame, $i = 1, 2, 3, \cdots$.
  - $Y_{\texttt{i}}\,$: $\;\texttt{i}$-th observation of outcome variable $Y$.
  - $X_{p, \texttt{i}}\,$: $\texttt{i}$-th observation of the $p$-th explanatory variable $X_{p}$.
  - $e_{\texttt{i}}\;$: $\;\texttt{i}$-th observation of statistical error variable.
  
  

---
# Linear Regression with Interaction Terms
### <p style="color:#00449E"> Model

- The linear regression with an interaction between explanatory variables $X_{1}$ and $X_{2}$ are:

$$Y_{\texttt{i}} \,=\, b_{0} \,+\, b_{1}\,X_{1,\texttt{i}} \,+\, b_{2}\,X_{2,\texttt{i}} \,+\, b_{3}\,X_{1,\texttt{i}}\times \color{Red}{X_{2,\texttt{i}}} \,+\, e_{\texttt{i}}\;.$$

.panelset[

.panel[.panel-name[Interaction]

- The relationship between $X_{1}$ and $Y$ varies by values of $b_{3}\, X_{2}$:

$$\frac{\Delta Y}{\Delta X_{1}} \,=\, b_{1} + b_{3}\, X_{2}$$.
]


.panel[.panel-name[Example]
- $X_{2}$ is often an indicator variable. If $b_{3} \neq 0$ and $X_{2, \texttt{i}} = 1$,
  
$$\frac{\Delta Y}{\Delta X_{1}} \,=\, b_{1} + b_{3}$$.

]

]



---
# Linear Regression with Interaction Terms
### <p style="color:#00449E"> Interaction with an Indicator

- The linear regression with an interaction between explanatory variables $X_{1}$ and $X_{2}\in\{\,0, 1\,\}$ are:

$$Y_{\texttt{i}} \,=\, b_{0} \,+\, b_{1}\,X_{1,\texttt{i}} \,+\, b_{2}\,X_{2,\texttt{i}} \,+\, b_{3}\,X_{1,\texttt{i}}\times \color{Red}{X_{2,\texttt{i}}} \,+\, e_{\texttt{i}},$$
where $X_{\,2, \texttt{i}}$ is either 0 or 1.

- For $\texttt{i}$ such that $X_{\,2, \texttt{i}} = 0$, the model is
$$Y_{\texttt{i}} \,=\, b_{0} \,+\, b_{1}\,X_{1,\texttt{i}} \,+\, e_{\texttt{i}}\qquad\qquad\qquad\qquad\qquad\quad\;\;$$

- For $\texttt{i}$ such that $X_{\,2, \texttt{i}} = 1$,  the model is

$$Y_{\texttt{i}} \,=\, (\,b_{0} \,+\, b_{2}\,) \,+\, (\,b_{1}\,+\, b_{3}\,)\,X_{1,\texttt{i}} \,+\, e_{\texttt{i}}\qquad\qquad$$

---
# Linear Regression with Interaction Terms
### <p style="color:#00449E"> Motivation

- Is education related with income?

```{r, echo = T, eval = F}
model <- lm( log(PINCP) ~ AGEP + SCHL + SEX,
             data = dtrain )
```

- Does the relationship between education and income vary by gender?

```{r, echo = T, eval = F}
model_int <- lm( log(PINCP) ~ AGEP + SCHL + SEX + 
                              SCHL * SEX,
                 data = dtrain )
# Equivalently,
model_int <- lm( log(PINCP) ~ AGEP + 
                              SCHL * SEX,  # Use this one!
                 data = dtrain )
```



---
# Linear Regression with Interaction Terms

- How could we see how the relationship between education and income vary by gender?

.panelset[
.panel[.panel-name[Code]
```{r, echo = T, eval = F}
summary(model_int)
b_int <- coef(model_int)
# the male's relationship between `Professional degree` and
# `PINCP` relative to male with no high school diploma
exp( b_int['SCHLProfessional degree'] ) - 1

# the female's relationship between `Professional degree` and
# `PINCP` relative to female with no high school diploma
exp( b_int['SCHLProfessional degree'] 
     + b_int['SCHLProfessional degree:SEXFemale'] ) - 1
```
]

.panel[.panel-name[Interpretation]
- All else being equal, female's professional degree is associated with an increase in personal income by 259% relative to female with no high school diploma.

- All else being equal, male's professional degree is associated with an increase in personal income by 199% relative to male with no high school diploma.
]

]

---
class: inverse, center, middle

# Log-Log Linear Regression
<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>


---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- To estimate the price elasticity of orange juice (OJ), we will use sales data for OJ from Dominick’s grocery stores in the 1990s.
  - Weekly `price` and `sales` (in number of cartons "sold") for three OJ brands---Tropicana, Minute Maid, Dominick's
  - An indicator, `feat`, showing whether each `brand` was advertised (in store or flyer) that week.
  

Variable  | Description
----------|-------------------------------
`sales`   | Quantity of OJ cartons sold
`price`   | Price of OJ
`brand`   | Brand of OJ
`feat`    | Advertisement status



---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- Let's prepare the OJ data:
```{r, echo = T, eval = F}
oj <- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')

# Split 70-30 into training and testing data.frames
set.seed(14454)
gp <- runif( nrow(oj) )

dtrain <- filter(oj, [?])
dtest <- filter(oj, [?])
```




---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- The following model estimates the price elasticity of demand for a carton of OJ:

$$\log(\texttt{sales}_{\texttt{i}}) \,=\, \quad\;\; b_{\texttt{intercept}} \,+\, b_{\,\texttt{mm}}\,\texttt{brand}_{\,\texttt{mm}, \texttt{i}} \,+\, b_{\,\texttt{tr}}\,\texttt{brand}_{\,\texttt{tr}, \texttt{i}}\\
 \,+\, b_{\texttt{price}}\,\log(\texttt{price}_{\texttt{i}}) \,+\, e_{\texttt{i}}$$

- where
  
$$
\texttt{brand}_{\,\texttt{tr}, \texttt{i}}\\
= \begin{cases}
\texttt{1} & \text{ if an orange juice } \texttt{i} \text{ is } \texttt{Tropicana};\\\\
\texttt{0} & \text{otherwise}.\qquad\qquad\quad\,
\end{cases}
$$


  
$$
\texttt{brand}_{\,\texttt{mm}, \texttt{i}}\\
= \begin{cases}
\texttt{1} & \text{ if an orange juice } \texttt{i} \text{ is } \texttt{Minute Maid};\\\\
\texttt{0} & \text{otherwise}.\qquad\qquad\quad\,
\end{cases}
$$


---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- The following model estimates the price elasticity of demand for a carton of OJ:

$$\log(\texttt{sales}_{\texttt{i}}) \,=\, \quad\;\; b_{\texttt{intercept}} \,+\, b_{\,\texttt{mm}}\,\texttt{brand}_{\,\texttt{mm}, \texttt{i}} \,+\, b_{\,\texttt{tr}}\,\texttt{brand}_{\,\texttt{tr}, \texttt{i}}\\
 \,+\, b_{\texttt{price}}\,\log(\texttt{price}_{\texttt{i}}) \,+\, e_{\texttt{i}}$$

- When $\texttt{brand}_{\,\texttt{tr}, \texttt{i}}\,=\,0$ and $\texttt{brand}_{\,\texttt{mm}, \texttt{i}}\,=\,0$, the beta coefficient for the intercept $b_{\texttt{intercept}}$ gives the value of Dominick's log sales at $\log(\,\texttt{price[i]}\,) = 0$.

- The beta coefficient $b_{\texttt{price}}$ is the price elasticity of demand.
  - It measures how sensitive the quantity demanded is to its price.


---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- For small changes in variable $x$ from $x_{0}$ to $x_{1}$, the following equation holds: 
  
$$\Delta \log(x) \,= \, \log(x_{1}) \,-\, \log(x_{0}) 
\approx\, \frac{x_{1} \,-\, x_{0}}{x_{0}} 
\,=\, \frac{\Delta\, x}{x_{0}}.$$

- The coefficient on $\log(\texttt{price}_{\texttt{i}})$, $b_{\texttt{price}}$, is therefore

$$b_{\texttt{price}} \,=\, \frac{\Delta \log(\texttt{sales}_{\texttt{i}})}{\Delta \log(\texttt{price}_{\texttt{i}})}\,=\, \frac{\frac{\Delta \texttt{sales}_{\texttt{i}}}{\texttt{sales}_{\texttt{i}}}}{\frac{\Delta \texttt{price}_{\texttt{i}}}{\texttt{price}_{\texttt{i}}}}.$$

- All else being equal, an increase in $\texttt{price}$ by 1% is associated with a decrease in $\texttt{sales}$ by $b_{\texttt{price}}$%.




---
# Log-Log Linear Regression
### <p style="color:#00449E"> EDA


.pull-left[
- Describe the relationship between `brand` and `log(price)`.

```{r, echo = T, eval = F}
ggplot(dtrain,
       aes( x = [?], y = [?], 
            fill = [?]) ) +
  geom_boxplot()
```
]


.pull-right[
- Describe the relationship between `log(price)` and `log(sales)` by `brand`.

```{r, echo = T, eval = F}
ggplot(dtrain,
       aes(x = [?], y = [?], 
           color = [?])) +
  geom_point([?]) +
  geom_smooth(method = [?])
```
]





---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- Let's train the first model, `model_1`:

$$\log(\texttt{sales}_{\texttt{i}}) \,=\, \quad\;\; b_{\texttt{intercept}} \,+\, b_{\,\texttt{mm}}\,\texttt{brand}_{\,\texttt{mm}, \texttt{i}} \,+\, b_{\,\texttt{tr}}\,\texttt{brand}_{\,\texttt{tr}, \texttt{i}}\\
 \,+\, b_{\texttt{price}}\,\log(\texttt{price}_{\texttt{i}}) \,+\, e_{\texttt{i}}$$
 
```{r, echo = T, eval = F}
model_1 <- lm([?], data = dtrain)
```



---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- Here is the inverse demand curve for each brand OJ: 

.pull-left[
```{r, echo = F, eval = T, message= F, warning= F, fig.fullwidth=TRUE, fig.height= 6.3}
oj <- read_csv(
  'https://bcdanl.github.io/data/dominick_oj.csv')
set.seed(14454) 
gp <- runif( nrow(oj) ) 
dtrain <- filter(oj, gp > .33)
dtest <- filter(oj, gp <= .33)

model_1 <- lm(log(sales) ~ brand + log(price), 
            data = dtrain)
beta_1 <- coef(model_1)
q1 <- ggplot(data = dtest) + 
  geom_point(aes(x = log(price), y = log(sales), 
                 color = brand),
             alpha = .2) +
  geom_abline(aes(slope = 
                    beta_1["log(price)"],
                  intercept = 
                    beta_1["(Intercept)"]),
              color = "red") +
  geom_abline(aes(slope = 
                    beta_1["log(price)"],
                  intercept = 
                    beta_1["(Intercept)"] + 
                    beta_1["brandminute.maid"]),
              color = "green") +
  geom_abline(aes(slope = 
                    beta_1["log(price)"],
                  intercept = 
                    beta_1["(Intercept)"] + 
                    beta_1["brandtropicana"]),
              color = "blue") +
  geom_vline(aes(xintercept = 0), lty = 'dotted') +
  xlim(c(-.2, 1.45))  +  ggtitle("Model 1") +
  hrbrthemes::theme_ipsum()  + 
  theme(legend.position = c(.12, .12))+
  theme(plot.title = element_text(size = rel(1.5)),
        axis.title = element_text(size = 25),
        axis.text.x = element_text(size = rel(1.5)),
        axis.text.y = element_text(size = rel(1.5))) +
  guides(colour = guide_legend(override.aes = list(alpha=1)))
q1
```
]

.pull-left[
- The linear demand curve for OJ is determined by its slope and intercept.

- When $\texttt{brand}_{\,\texttt{tr}, \texttt{i}}\,=\,0$ and $\texttt{brand}_{\,\texttt{mm}, \texttt{i}}\,=\,0$, the beta coefficient $b_{\texttt{intercept}}$ gives the value of Dominick's log sales at $\log(\,\texttt{price[i]}\,) = 0$.
 
]





---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- How does the relationship between `log(sales)` and `log(price)` vary by `brand`?

  - Let's train the second model, `model_2`, that addresses the above question:

$$\log(\texttt{sales}_{\texttt{i}}) \,=\, \quad b_{\texttt{intercept}} \,+\, b_{\,\texttt{mm}}\,\texttt{brand}_{\,\texttt{mm}, \texttt{i}} \,+\, b_{\,\texttt{tr}}\,\texttt{brand}_{\,\texttt{tr}, \texttt{i}}\\
 \,+\, b_{\texttt{price}}\,\log(\texttt{price}_{\texttt{i}})  \\
 \qquad\qquad\qquad\,+\, b_{\texttt{price*mm}}\,\log(\texttt{price}_{\texttt{i}})\,\times\,\color{Green} {\texttt{brand}_{\,\texttt{mm}, \texttt{i}}} \\
 \qquad\qquad\qquad\qquad +\, b_{\texttt{price*tr}}\,\log(\texttt{price}_{\texttt{i}})\,\times\,\color{Blue} {\texttt{brand}_{\,\texttt{tr}, \texttt{i}}} \,+\, e_{\texttt{i}}$$
 
```{r, echo = T, eval = F}
model_2 <- lm([?], data = dtrain)
```


---
# Log-Log Linear Regression


- For $\texttt{i}$ such that $\color{Green}{\texttt{brand}_{\,\texttt{mm}, \texttt{i}}} = 0$ and $\color{Blue}{\texttt{brand}_{\,\texttt{tr}, \texttt{i}}} = 0$, the model equation is:
$$\log(\texttt{sales}_{\texttt{i}}) \,=\, \; \,b_{\texttt{intercept}}\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\\
\,+\, b_{\texttt{price}} \,\log(\texttt{price}_{\texttt{i}}) \,+\, e_{\texttt{i}}\,.\qquad\qquad\;$$


- For $\texttt{i}$ such that $\color{Green}{\texttt{brand}_{\,\texttt{mm}, \texttt{i}}} = 1$, the model equation is:
$$\log(\texttt{sales}_{\texttt{i}}) \,=\, \; (\,b_{\texttt{intercept}} \,+\, b_{\,\texttt{mm}}\,)\qquad\qquad\qquad\qquad\qquad\qquad\qquad\\
\!\,+\,(\, b_{\texttt{price}} \,+\, \color{Green}{b_{\texttt{price*mm}}}\,)\,\log(\texttt{price}_{\texttt{i}}) \,+\, e_{\texttt{i}}\,.$$


- For $\texttt{i}$ such that $\color{Blue}{\texttt{brand}_{\,\texttt{tr}, \texttt{i}}} = 1$, the model equation is:
$$\log(\texttt{sales}_{\texttt{i}}) \,=\, \; (\,b_{\texttt{intercept}} \,+\, b_{\,\texttt{tr}}\,)\qquad\qquad\qquad\qquad\qquad\qquad\qquad\\
\!\,+\,(\, b_{\texttt{price}} \,+\, \color{Blue}{b_{\texttt{price*tr}}}\,)\,\log(\texttt{price}_{\texttt{i}}) \,+\, e_{\texttt{i}}\,.$$



---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- Here are the inverse demand curves from `model_1` and `model_2`: 

.pull-left[
```{r, echo = F, eval = T, message= F, warning= F, fig.fullwidth=TRUE, fig.height= 6.4}
q1
```

]

.pull-right[
```{r, echo = F, eval = T, message= F, warning= F, fig.fullwidth=TRUE, fig.height= 6.3}
model_2 <- lm(log(sales) ~ brand*log(price), 
            data=dtrain)
beta_2 <- coef(model_2)

# inverse demand curves from model 2
q2 <- ggplot(data = dtest) +
  geom_point(aes(x = log(price), y = log(sales), 
                 color = brand),
             alpha = .2) +
  geom_abline(aes(slope = 
                    beta_2["log(price)"],
                  intercept = 
                    beta_2["(Intercept)"]),
              color = "red") +
  geom_abline(aes(slope = 
                    beta_2["log(price)"],
                  intercept = 
                    beta_2["(Intercept)"] + 
                    beta_2["brandminute.maid"]),
              color = "green") +
  geom_abline(aes(slope = 
                    beta_2["log(price)"] + 
                    beta_2["brandtropicana:log(price)"],
                  intercept = 
                    beta_2["(Intercept)"] + 
                    beta_2["brandtropicana"]),
              color = "blue") +
  geom_vline(aes(xintercept = 0), lty = 'dotted') +
  xlim(c(-.2, 1.45)) +  ggtitle("Model 2") +
  hrbrthemes::theme_ipsum()+ guides(color = "none")+
  theme(plot.title = element_text(size = rel(1.5)),
        axis.title = element_text(size = 25),
        axis.text.x = element_text(size = rel(1.5)),
        axis.text.y = element_text(size = rel(1.5))) 
q2
```

]

- Model 2 assumes that the price elasticity of OJ can vary by `brand`.




---
# Log-Log Linear Regression

- How does advertisement play a role in the relationship between sales and prices in the OJ market?

  - The ads can increase sales at all prices.

  - They can change price sensitivity.

  - They can do both of these things in a brand-specific manner.

- What would be the formula that address the question above?


---
# Log-Log Linear Regression
- Let's train the second model, `model_3`, that addresses the question in the previous page:

$$\log(\texttt{sales}_{\texttt{i}}) \,=\, \quad b_{\texttt{intercept}} \,+\, b_{\,\texttt{mm}}\,\texttt{brand}_{\,\texttt{mm}, \texttt{i}} \,+\, b_{\,\texttt{tr}}\,\texttt{brand}_{\,\texttt{tr}, \texttt{i}}  \\
 \quad\,+\; b_{\,\texttt{feat}}\,\texttt{feat}_{\, \texttt{i}} \qquad\qquad\qquad\qquad\quad   \\
\,+\, b_{\texttt{mm*feat}}\,\color{Green} {\texttt{brand}_{\,\texttt{mm}, \texttt{i}}}\,\times\, \color{Violet}{\texttt{feat}_{\,\texttt{i}}}\,+\, b_{\texttt{tr*feat}}\,\color{Blue} {\texttt{brand}_{\,\texttt{tr}, \texttt{i}}}\,\times\, \color{Violet}{\texttt{feat}_{\,\texttt{i}}} \\
 \quad\,+\;  b_{\texttt{price}}\,\log(\texttt{price}_{\texttt{i}}) \qquad\qquad\qquad\;\;\;\;  \\
 \qquad\qquad\qquad\,+\, b_{\texttt{price*mm}}\,\log(\texttt{price}_{\texttt{i}})\,\times\,\color{Green} {\texttt{brand}_{\,\texttt{mm}, \texttt{i}}}\qquad\qquad\qquad\;\, \\
 \qquad\qquad\qquad\,+\, b_{\texttt{price*tr}}\,\log(\texttt{price}_{\texttt{i}})\,\times\,\color{Blue} {\texttt{brand}_{\,\texttt{tr}, \texttt{i}}}\qquad\qquad\qquad\;\, \\
\qquad\qquad\quad +\, b_{\texttt{price*feat}}\,\log(\texttt{price}_{\texttt{i}})\,\times\,\color{Violet}{\texttt{feat}_{\,\texttt{i}}}\qquad\qquad\qquad\;\, \\
 \qquad\qquad\quad +\, b_{\texttt{price*mm*feat}}\,\log(\texttt{price}_{\texttt{i}}) \,\times\,\,\color{Green} {\texttt{brand}_{\,\texttt{mm}, \texttt{i}}}\,\times\, \color{Violet}{\texttt{feat}_{\,\texttt{i}}} \\
 \qquad\qquad\qquad\quad +\, b_{\texttt{price*tr*feat}}\,\log(\texttt{price}_{\texttt{i}}) \,\times\,\,\color{Blue} {\texttt{brand}_{\,\texttt{tr}, \texttt{i}}}\,\times\, \color{Violet}{\texttt{feat}_{\,\texttt{i}}}  \,+\, e_{\texttt{i}}$$

 

---
# Log-Log Linear Regression

- The model with triple interactions can be trained as follows:

```{r, echo = T, eval = F}
model_3 <- lm([?], data = dtrain)
```



---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity


- Here are the inverse demand curves from `model_3`:

.pull-left[
```{r, echo = F, eval = T, message= F, warning= F, fig.fullwidth=TRUE, fig.height= 6.1}
model_3 <- lm(log(sales) ~ brand*feat*log(price), 
            data=dtrain)
beta_3 <- coef(model_3)

# inverse demand curves from model 3 when there was no promo
p1 <- ggplot(data = filter(dtrain, feat == 0)) +
  geom_point(aes(x = log(price), y = log(sales), 
                 color = brand),
             alpha = .2) +
  geom_abline(aes(slope = 
                    beta_3["log(price)"],
                  intercept = 
                    beta_3["(Intercept)"]),
              color = "red") +
  geom_abline(aes(slope = 
                    beta_3["log(price)"] + 
                    beta_3["brandminute.maid:log(price)"],
                  intercept = 
                    beta_3["(Intercept)"] + 
                    beta_3["brandminute.maid"]),
              color = "green") +
  geom_abline(aes(slope = 
                    beta_3["log(price)"] + 
                    beta_3["brandtropicana:log(price)"],
                  intercept = 
                    beta_3["(Intercept)"] + 
                    beta_3["brandtropicana"]),
              color = "blue") +
  geom_vline(aes(xintercept = 0), lty = 'dotted') +
  xlim(c(-.2, 1.45)) + ylim(c(4,12.5)) +
  ggtitle("No Feat") +
  hrbrthemes::theme_ipsum() +
  guides(color = "none")+
  theme(plot.title = element_text(size = rel(1.5)),
        axis.title = element_text(size = 25),
        axis.text.x = element_text(size = rel(1.5)),
        axis.text.y = element_text(size = rel(1.5)))
p1
```

]

.pull-right[
```{r, echo = F, eval = T, message= F, warning= F, fig.fullwidth=TRUE, fig.height= 6.1}
p2 <- ggplot(data = filter(dtrain, feat == 1)) +
  geom_point(aes(x = log(price), y = log(sales), 
                 color = brand),
             alpha = .2) +
  geom_abline(aes(slope = 
                    beta_3["log(price)"] + 
                    beta_3["feat:log(price)"],
                  intercept = 
                    beta_3["(Intercept)"] + 
                    beta_3["feat"]),
              color = "red") +
  geom_abline(aes(slope = 
                    beta_3["log(price)"] + 
                    beta_3["brandminute.maid:log(price)"] + 
                    beta_3["feat:log(price)"] + 
                    beta_3["brandminute.maid:feat:log(price)"],
                  intercept = 
                    beta_3["(Intercept)"] + 
                    beta_3["feat"] + 
                    beta_3["brandminute.maid"] + 
                    beta_3["brandminute.maid:feat"]),
              color = "green") +
  geom_abline(aes(slope = 
                    beta_3["log(price)"] + 
                    beta_3["brandtropicana:log(price)"] + 
                    beta_3["feat:log(price)"] + 
                    beta_3["brandtropicana:feat:log(price)"],
                  intercept = 
                    beta_3["(Intercept)"] + 
                    beta_3["feat"] + 
                    beta_3["brandtropicana"] + 
                    beta_3["brandtropicana:feat"]),
              color = "blue") +
  geom_vline(aes(xintercept = 0), lty = 'dotted') +
  xlim(c(-.2, 1.45)) + ylim(c(4,12.5)) +
  ggtitle("Feat") + 
  hrbrthemes::theme_ipsum() +
  theme(plot.title = element_text(size = rel(1.5)),
        axis.title = element_text(size = 25),
        axis.text.x = element_text(size = rel(1.5)),
        axis.text.y = element_text(size = rel(1.5))) +
  theme(legend.position = c(.12, .175))+
  guides(colour = guide_legend(override.aes = list(alpha=1)))
p2
```

]


---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity

- Describe the relationship between `feat` and `brand` using stacked bar charts.


.pull-left[
```{r, echo = F, eval = T, message= F, warning= F, fig.fullwidth=TRUE, fig.height= 6.1}
r1 <- ggplot(data = dtrain) +
  geom_bar(aes(x = as.factor(feat), y = ..prop..,
               group = brand, fill = brand), 
           position = "fill") +
  hrbrthemes::theme_ipsum() + xlab("feat") +
  guides(fill = "none")+
  theme(plot.title = element_text(size = rel(1.5)),
        axis.title = element_text(size = 25),
        axis.text.x = element_text(size = rel(1.5)),
        axis.text.y = element_text(size = rel(1.5)))
r1
```

]

.pull-right[
```{r, echo = F, eval = T, message= F, warning= F, fig.fullwidth=TRUE, fig.height= 6.1}
r2 <- ggplot(data = dtrain) +
  geom_bar(aes(x = as.factor(feat), 
               group = brand, fill = brand)) +
  hrbrthemes::theme_ipsum() + xlab("feat") + 
  theme(legend.position = c(.775, .75))+
  theme(plot.title = element_text(size = rel(1.5)),
        axis.title = element_text(size = 25),
        axis.text.x = element_text(size = rel(1.5)),
        axis.text.y = element_text(size = rel(1.5)))
r2
```

]

- Model 3 assumes that the relationship between `price` and `sales` can vary by `feat`.




---
# Log-Log Linear Regression
### <p style="color:#00449E"> Estimating Price Elasticity


```{r, echo=FALSE, out.width = '70%', fig.align='center'}
knitr::include_graphics("../lec_figs/oj_elasticity.png")
```

- How would you explain different estimation results across different models?

- Which model do you prefer? Why?
