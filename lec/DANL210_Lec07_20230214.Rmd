---
title: "DANL 210 Lecture 7"
subtitle: "DANL 210: Data Preparation and Management"
author: "Byeong-Hak Choe"
institute: "SUNY Geneseo"
date: "Febraury 14, 2023"
output:
  xaringan::moon_reader:
    css: 
      - default
      - css/nhsr.css
      - css/nhsr-fonts.css
    lib_dir: libs
    seal: false
    nature:
      highlightStyle: googlecode
      highlightLines: true
      highlightLanguage: ["r"]
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      after_body: [css/insert-logo.html]
---

```{r setup, include = FALSE}
library(knitr)
library(kableExtra)
library(tidyverse)
library(NHSRtheme)
library(fontawesome)
# set default options
opts_chunk$set(echo = T, eval = F,
               fig.width = 7.252,
               fig.height = 4,
               comment = "#",
               dpi = 300)

knitr::knit_engines$set("markdown")

xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::use_clipboard()
xaringanExtra::use_webcam()
xaringanExtra::use_broadcast()
xaringanExtra::use_share_again()
xaringanExtra::style_share_again(
  share_buttons = c("twitter", "linkedin", "pocket")
)


xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)

# uncomment the following lines if you want to use the NHS-R theme colours by default
# scale_fill_continuous <- partial(scale_fill_nhs, discrete = FALSE)
# scale_fill_discrete <- partial(scale_fill_nhs, discrete = TRUE)
# scale_colour_continuous <- partial(scale_colour_nhs, discrete = FALSE)
# scale_colour_discrete <- partial(scale_colour_nhs, discrete = TRUE)
```

class: title-slide, left, bottom

# `r rmarkdown::metadata$title`
----
## **`r rmarkdown::metadata$subtitle`**
### `r rmarkdown::metadata$author`
### `r rmarkdown::metadata$date`


---
# Workflow
### <p style="color:#00449E"> Shortcuts for Presentation Slides </p>
 
- Use `o` to see the tile view of all the slide pages.

- Use `r fa("arrow-left")` and `r fa("arrow-right")` to turn over the slide pages.

- When you can't turn over slide pages, use **Ctrl + R** (**cmd + R** for mac users) to refresh the webpage.

---
# Coding Workflow
### <p style="color:#00449E"> Spyder Keyboard Shortcuts </p>

- Check your keyboard shortcuts:
  - Mac: python `r fa("arrow-right")` Preference `r fa("arrow-right")` Keyboard shortcuts `r fa("arrow-right")` Search "run"
  - Windows: Tools `r fa("arrow-right")` Preference `r fa("arrow-right")` Keyboard shortcuts `r fa("arrow-right")` Search "run"
  
  
- Set your own keyboard shortcuts for "run selection" and "run cell".


- As a Mac user, my Spyder shortcuts are
  - **cmd + return** for "run selection"
  - **ctrl + return** for "run cell"


- A cell can be created by `# %%`.

---
# Coding Workflow
### <p style="color:#00449E"> Spyder Keyboard Shortcuts </p>


.pull-left[
**Mac**

- **command + N** opens a new Python script.
- **command + ** `r fa("arrow-down")` goes to the next cell
- **command + ** `r fa("arrow-up")` goes to the previous cell
- **command + 1** is the shortcut for `#`.
- **command + 4** is the shortcut for a cell title.
- **command + Z** undoes the previous action.
]

.pull-right[
**Windows**

- **Ctrl + N** opens a new Python script.
- **Ctrl + ** `r fa("arrow-down")` goes to the next cell
- **Ctrl + ** `r fa("arrow-up")` goes to the previous cell
- **Ctrl + 1** is the shortcut for `#`.
- **Ctrl + 4** is the shortcut for a cell title.
- **Ctrl + Z** undoes the previous action.
]


---
# Workflow
### <p style="color:#00449E"> Spyder Working Directory </p>

- In Spyder, we can set the working directory.
  - Windows: Tools > Preferences > Working directory > The following directory
  - Mac: python > Preferences > Working directory > The following directory
  

- Download the CSV file, `scientists.csv` from the Files section in our Canvas.
  - Create the `data` folder in your working directory.
  - Then move the `scientists.csv` file to the `data` folder in your working directory.



---
# Coding Workflow
### <p style="color:#00449E"> `from LIBRARY import METHOD`  </p>

- To refer to items from a Python library, the `from … import` statement. 

  - When we import modules this way, we can refer to the methods by name rather than through dot (`.`) notation.

  - `from … import` is not recommended in general, unless we are 100% sure that the method names are distinctive across Python libraries we import.

```{python}
from pandas import read_csv
scientists = read_csv('data/scientists.csv')
ages = scientists['Age']

from skimpy import skim
skim(scientists)
```


---
class: inverse, center, middle

# Pandas Data Structures Basics

---
# Python Basics
### <p style="color:#00449E"> Attributes vs. Methods </p>

- Attributes can be thought of as features of an object (e.g., `DataFrame.dtypes`). 

- Methods can be thought of as some calculation or operation that is performed on an object. 
  - Methods or functions have round parentheses (`()`), while attributes do not  (e.g., `DataFrame.group_by()`). 

- The subsetting syntax `.loc[]` and `.iloc[]` consists of all attributes.    



---
# The `Series`
### <p style="color:#00449E"> Boolean Subsetting on `Series` </p>

- We can not only subset values using labels and indices, but also supply a vector of **boolean values**.

- Boolean subsetting of numeric Series works as follows:
  - `Series[ Series > VALUE  ]`
  - `Series[ Series == VALUE  ]`
  - `Series[ Series < VALUE  ]`

- **Q**. What if we want to subset our ages by identifying those above the mean?


---
# The `DataFrame`
### <p style="color:#00449E">  Boolean Subsetting on `DataFrames` </p>


- Boolean subsetting of `DataFrames` works like boolean subsetting a `Series`.
  - `DataFrame[ DataFrame['VARIABLE_NAME'] > VALUE  ]`
  - `DataFrame[ DataFrame['VARIABLE_NAME'] == VALUE  ]`
  - `DataFrame[ DataFrame['VARIABLE_NAME'] < VALUE  ]`

```{python}
# boolean vectors will subset rows
scientists.loc[ scientists['Age'] > scientists['Age'].mean() ]
```


---
# The `DataFrame`
### <p style="color:#00449E">  Subsetting Multiple Rows and Columns </p>

```{r, echo = F, eval = T, out.width='100%', fig.align='center'}
text_tbl <- data.frame(
  `Type` = c("df[val]",
"df`.`loc[val]",
"df`.`loc[:, val]",
"df`.`loc[val1, val2]",
"df`.`iloc[where]",
"df`.`iloc[:, where]",
"df`.`iloc[w1, w2]"),
  `Description` = c("Select single column or set of columns",
"Select single row or set of rows",
"Select single column or set of columns",
"Select row and column by label",
"Select row or set of rows by integer position",
"Select column or set of columns by integer position",
"Select row and column by integer position")
  )


kable(text_tbl, format = "html") %>%
  kable_paper(full_width = T) %>%
  column_spec(1, bold = T, border_right = T) %>%
  kable_styling(html_font = 'sans-serif, helvetica, arial',
                bootstrap_options = c("hover", "condensed") )

```



---
#  Getting Started with Pandas
### <p style="color:#00449E"> Class Exercise </p>

.panelset[

.panel[.panel-name[(1)]
- Consider the two Series, `area` and `pop`:
```{python}
area = pd.Series({'California': 423967, 'Texas': 695662,
                  'New York': 141297, 'Florida': 170312,
                  'Illinois': 149995})
pop = pd.Series({'California': 38332521, 'Texas': 26448193,
                 'New York': 19651127, 'Florida': 19552860,
                 'Illinois': 12882135})
```

- Create the DataFrame `data` with two columns, `area` and `pop`.

]

.panel[.panel-name[(2)]
- Add a variable of the population density to the DataFrame `data`.

- Use boolean subsetting on the DataFrame `data` to find states whose population density is greater than 100 or less than 50.

]

]

---
# The `Series` and the `DataFrame`
### <p style="color:#00449E">  Sorting and Ranking </p>

- Let's consider `.sort_index()` and `.sort_values()` methods:

.panelset[

.panel[.panel-name[Series]
  - `Series.sort_index(ascending=False)` sorts `Series` by index in descending order.
  - `Series.sort_values(ascending=False)` sorts `Series` by value in descending order.
```{python}
rev_ages = ages.sort_index(ascending =False) 
sorted_ages = ages.sort_values(ascending =False) 
```
]
.panel[.panel-name[DataFrame]
  - `DataFrame.sort_index(ascending=False)` sorts `DataFrame` by index in descending order.
  - `DataFrame.sort_values(by = "VAR", ascending=False)` sorts `DataFrame` by value of `VAR` in descending order.
```{python}
rev_ages = scientists.sort_index(ascending =False) 
sorted_df = scientists.sort_values(by = 'Age', 
                                   ascending =False) 
```


]

]



---
# The `Series` and the `DataFrame`
### <p style="color:#00449E">  Sorting and Ranking </p>

- In Pandas, there are a variety of ranking functions with `.rank()`.

.panelset[

.panel[.panel-name[example]

- Consider the following DataFrame.

```{python}
import numpy as np
df = pd.DataFrame(data={'Animal': ['fox', 'Kangaroo', 'deer',
                                   'spider', 'snake'],
                        'Number_legs': [4, 2, 4, 8, np.nan]})
df
```
]

.panel[.panel-name[method (1)]

- `.rank(method = "min", ascending=False)` does give the largest values the smallest ranks.

- `.rank(method = "dense", ascending=False)` does give the largest values the smallest ranks without any gaps between ranks when breaking ties.

- `.rank(method = "min", ascending=False)` does give the largest values the smallest ranks.

- `.rank(method = "average", ascending=False)` calculates the average rank for each unique value.
]


.panel[.panel-name[method (2)]
```{python}
df['default_rank'] = df['Number_legs'].rank(ascending = False)  # method = 'average'
df['min_rank'] = df['Number_legs'].rank(method='min', ascending = False)
df['dense_rank'] = df['Number_legs'].rank(method='dense', ascending = False)
df
```
]


.panel[.panel-name[na_option]
- `na_option = keep` assigns `NaN` rank to `NaN` values (default).
- `na_option = top` assign smallest rank to `NaN` values if ascending
- `na_option = bottom` assign highest rank to `NaN` values if ascending
```{python}
df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')
df['NA_top'] = df['Number_legs'].rank(na_option='top')
df
```
]

.panel[.panel-name[pct]

- `pct = True` displays the returned rankings in percentile form.	
```{python}
df['pct_rank'] = df['Number_legs'].rank(pct=True)
df
```
]

]


---
#  Making Changes to Series and DataFrames
### <p style="color:#00449E"> Dropping Values </p>


.panelset[

.panel[.panel-name[Columns]
- To drop a column, we can select columns to drop with the `.drop()` method with `axis = 1` or `axis = "columns"` on our dataframe.

```{python}
# all the current columns in our data
scientists.columns

# drop the shuffled age column
# we provide the axis=1 argument to drop column-wise
scientists_dropped = scientists.drop( ['Age'], axis ="columns")
scientists_dropped.columns
```

]

.panel[.panel-name[Rows]
- To drop rows, we can select rows by index to drop with the `.drop()` method with `axis = 0`, which is default.


```{python}
# all the current columns in our data
scientists.columns

# drop rows by their indices
scientists_rows_dropped = scientists.drop( [2, 4, 6] )
scientists_rows_dropped
```

]

]


---
#  Making Changes to Series and DataFrames
### <p style="color:#00449E">   Modifying Columns with `.assign()` </p>

.panelset[

.panel[.panel-name[(0)]
- Let's create a new set of columns that contain the `datetime` representations of the object (string) dates.
```{python}
# format the 'Born' column as a datetime
born_datetime = pd.to_datetime( scientists['Born'] )
died_datetime = pd.to_datetime( scientists['Died'] )

scientists['born_dt'], scientists['died_dt'] = (
  born_datetime,
  died_datetime
)

scientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']
```


]

.panel[.panel-name[(1)]
- Let’s create the `age_year_assign` column using `.assign()`.

```{python}
scientists = scientists.assign(
  # new columns on the left of the equal sign
  # how to calculate values on the right of the equal sign
  # separate new columns with a comma
  age_days_assign = scientists['died_dt'] - scientists['born_dt'],
  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')
)
```
]

.panel[.panel-name[(2)]
- In the previous panel, we had to use `age_days` to create `age_year_assign`.
  - To be able to use `age_days_assign` when calculating `age_year_assign` in the previous panel, we need to know about `lambda` functions.
  
```{python}
scientists = scientists.drop( ['age_days'], axis = 1)  # to drop age_days
scientists = scientists.assign(
  age_days_assign = scientists['died_dt'] - scientists['born_dt'],
  age_year_assign = lambda some_df: 
    some_df['age_days_assign'].astype('timedelta64[Y]') 
)
```

]

]



---
# Pandas Data Structures Basics
### <p style="color:#00449E"> Lambda functions </p>
.panelset[

.panel[.panel-name[(1)]
- Python has support for so-called anonymous or lambda functions. 
  - Lambda functions are a way of writing functions consisting of a single statement, the result of which is the return value. 
  - A syntax for a lambda function is `lambda ARGUMENTS : EXPRESSION`

```{python}
def short_function(x):
  return x * 2

equiv_anon = lambda x: x * 2
short_function(2)
equiv_anon(2)
```

]


.panel[.panel-name[(2)]
- A `lambda` function can have multiple arguments:

```{python}
fn_two = lambda a, b : a * b
fn_two(1, 2)

fn_three = lambda a, b, c : a + b + c
fn_two(1, 2, 3)
```

]



.panel[.panel-name[(3)]
- The power of lambda is better shown when we use them as an anonymous function inside another function.
  - Say we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:

```{python}
def myfunc(n):
  return lambda a : a * n
```

]


.panel[.panel-name[(4)]
.pull-left[
- Use that function definition to make a function that always doubles the number we send in:

```{python}
def myfunc(n):
  return lambda a : a * n

my_doubler = myfunc(2)
my_doubler(10)
```
]

.pull-right[
- Use the same function definition to make a function that always triples the number we send in:
```{python}
def myfunc(n):
  return lambda a : a * n

my_tripler = myfunc(3)
my_tripler(10)
```

]

]

.panel[.panel-name[(5)]
- Here is the example of applying a lambda function to single variable in DataFrame using `.assign()`

```{python}
values= [['Rohan',182],['Elvish',100],['Deepak',198],
         ['Soni',160],['Radhika',140],['Vansh',180]]
df = pd.DataFrame(values, columns = ['name','tot_marks'])
 
# Applying lambda function to find percentage of 'tot_marks' column
# using df.assign()
df = df.assign( percentage = 
  lambda some_name: (some_name['tot_marks'] /200 * 100) )
```

]

]




---
class: inverse, center, middle

# Apply Functions

---
#  Apply Functions
### <p style="color:#00449E">  </p>

- Learning about `.apply()` is fundamental in the data cleaning process. 

- It also encapsulates key concepts in programming, mainly writing **functions**. 

- The `.apply()` method takes a function and applies it (i.e., runs it) across each row or column of a DataFrame without having us write the code for each element separately.



---
#  Apply Functions
### <p style="color:#00449E">  </p>
- How do we use functions in Pandas?
  - We've already seen lambda functions in Pandas!
- Here’s a toy DataFrame of two columns.
```{python}
df = pd.DataFrame({"a": [10, 20, 30], "b": [20, 30, 40]})
```

- We can `.apply()` our functions over a `Series` (i.e., a **column** or a **row**).
- Let's consider the following example:

.pull-left[
```{python}
def my_sq(x):
  """Squares a given value
  """
  return x ** 2
```
]
.pull-right[
```{python}
df['a'] ** 2
# this would not allow us to use 
# a function we wrote ourselves.
```
]


---
#  Apply Functions
### <p style="color:#00449E">  Apply Over a Series </p>

.panelset[

.panel[.panel-name[(1)]
-  If we want to square each value in column `a`, we can do the following:

```{python}
sq = df['a'].apply(my_sq)
```
]

.panel[.panel-name[(2)]
- Let’s consider a function that takes two parameters.

```{python}
def my_exp(x, e):
  return x ** e

cubed = my_exp(2, 3)
my_exp(2)  # does it work?
```
]

.panel[.panel-name[(3)]
- If we want to apply the function `my_exp(x, e)` on our series, we will need to pass in the second parameter.
  -  To do this, we pass the second argument as a **keyword** argument into `.apply()`.

```{python}
ex2 = df['a'].apply(my_exp, e=2)
ex3 = df['a'].apply(my_exp, e=3)
```
]


]



---
#  Apply Functions
### <p style="color:#00449E">  Apply Over a DataFrame --- Column-Wise Operations </p>

.panelset[

.panel[.panel-name[DataFrames]
- We’ve seen how to apply functions over a one-dimensional `Series`.

- `DataFrames` typically have at least two dimensions. 
  - When we apply a function over a dataframe, we first need to specify which axis to apply the function over–for example, column-by-column or row-by-row.
]

.panel[.panel-name[(1)]

- Use the `axis = 0` parameter (the default value) in `.apply()` when working with functions in a column-wise manner (i.e., for each column).

```{python}
def print_me(x):
  print(x)
df.apply(print_me, axis =0)
```

- Compare this output to the following:

```{python}
print(df['a']); print(df['b'])
```
]



.panel[.panel-name[(2)]

-  When we apply a function across a DataFrame (in this case, column-wise with `axis=0`), the **entire** axis (e.g., column) is passed into the **first** argument of the function.

```{python}
def avg_3(x, y, z):
   return (x + y + z) / 3

df.apply(avg_3)  # does it work?
df.apply(avg_3, y= 1, z = 1)
df.apply(avg_3, y= [1, 1, 1], z = [1, 1, 1])
df.apply(avg_3, y= [1, 1, 1], z = [1, 5, 10])
```


]

.panel[.panel-name[(3)]
-  Does the following work?

```{python}
def avg_3_apply(col):
  """The avg_3 function but apply compatible by taking in all the values 
  as the first argument and parsing out the values within the function
  """
   x = col[0]
   y = col[1]
   z = col[2]
   return (x + y + z) / 3
df.apply(avg_3_apply)  # does it work?
```


]

]
  

---
#  Apply Functions
### <p style="color:#00449E">  Apply Over a DataFrame --- Row-Wise Operations </p>

.panelset[


.panel[.panel-name[(1)]

- Use the `axis = 1` parameter in `.apply()` when working with functions in a row-wise manner (i.e., for each column).
  -  The **entire row** is used as the first argument.


- Let's `.apply( axis = 1)` to each row.
```{python}
df.apply( avg_3_apply, axis = 1 )  
```

- Does it work?

]


.panel[.panel-name[(2)]

-  The main issue here is the `index out of bounds`. 

  - We passed the row of data in as the first argument, but in our function we begin indexing out of range. 
  
  - That is, we have only two values in each row, but we tried to get index 2, which means the third element, and it does not exist).

]

.panel[.panel-name[(3)]

-  If we want to calculate our averages row-wise, we have to write a new function to work with two values.

```{python}
def avg_2_apply(row):
    """Taking the average of row value.
    Assuming that there are only 2 values in a row.
    """
    x = row[0]
    y = row[1]
    return (x + y) / 2
df.apply(avg_2_apply, axis =1)
```

]

]
  

---
#  Apply Functions
### <p style="color:#00449E">  Lambda Functions </p>
- Sometimes the function used in the `.apply()` method is simple enough that there is no need to create a separate function.

.pull-left[

```{python}
def my_sq(x):
  return x ** 2

df['a_sq'] = df['a'].apply(my_sq)
```

]

.pull-right[

```{python}
df['a_sq_lamb'] = ( df['a']
    .apply(lambda x: x ** 2) )
```

]

- We can see that the actual function is a simple one-liner. 
  - Usually when this happens, we would want to use lambda functions.



---
#  Apply Functions
### <p style="color:#00449E">   Vectorized Functions </p>


.panelset[


.panel[.panel-name[(1)]
- When we use `.apply()`, we are able to make a function work on a column-by-column or row-by-row basis.


- However, there might be times when it is not feasible to rewrite a function in this way. 
  - We can then leverage the `.vectorize()` function and decorator to vectorize any function.
  - We expect performance gains by doing so.
  
  
]
 
.panel[.panel-name[(2)] 
- Here’s our average function, which we can apply on a row-by-row basis:

```{python}
def avg_2(x, y):
  return (x + y) / 2

df   # to remind ourselves
avg_2( df['a'], df['b'] )
```

- For a vectorized function, we’d like to be able to pass in a vector of values for `x` and a vector of values for `y`, and the results should be the average of the given `x` and `y` values in the same order. 

]

.panel[.panel-name[(3)]

- `avg_2(df['a'], df['y'])` works because the actual calculations within our function are inherently vectorized.

  - If we add two numeric columns together, `Pandas` (and the `NumPy` library) will automatically perform element-wise addition.
  
  - Likewise, when we divide by a scalar, it will “broadcast” the scalar, and divide each element by the scalar.
]


.panel[.panel-name[(4)]

- Let’s change our function and perform a non-vectorizable calculation.
```{python}
def avg_2_mod(x, y):
   """Calculate the average, unless x is 20."""
   if x == 20:
     return np.NaN
   else:
     return (x + y) / 2
avg_2_mod( df['a'], df['b'] )   # does it work?
avg_2_mod(10, 20)
avg_2_mod(20, 30)
```


]

.panel[.panel-name[w/ np (1)]
- We pass `np.vectorize()` to the function we want to vectorize, to create a new function.

```{python}
# np.vectorize actually creates a new function
avg_2_mod_vec = np.vectorize(avg_2_mod)

# use the newly vectorized function
avg_2_mod_vec( df['a'], df['b'] )

```

]


.panel[.panel-name[w/ np (2)]

- Sometimes, we want to modify an existing function without changing its source code. 
  - We can then consider using a decorator.


- A decorator is a function that takes one function as input and returns another function.


- We can add `@decorator_name` before the function that we want to decorate:
]

.panel[.panel-name[w/ np (3)]
```{python}
# to use the vectorize decorator
# we use the @ symbol before our function definition
@np.vectorize
def v_avg_2_mod(x, y):
   if x == 20:
     return np.NaN
   else:
     return (x + y) / 2
   
v_avg_2_mod( df['a'], df['b']) )
```

]



]







---
class: inverse, center, middle

# Exporting and Importing Data

---
# Exporting and Importing Data
### <p style="color:#00449E"> Pickle </p>


.panelset[

.panel[.panel-name[(1)]
- Python has a way to `pickle` data. 
  - This is Python’s way of serializing and saving data in a binary format.
  - `pickle` files are usually saved with an extension of `.p`, `.pkl`, or `.pickle`.
  - If we try to open it in a text editor, we will see a bunch of garbled characters.

- Create the `output` folder in your working directory.
  - We will use the `output` folder to store the exported `Series` or `DataFrame`.
]

.panel[.panel-name[(2)]
- To export `Series` or `DataFrame` as a `pickle` file, we use the `to_pickle()` method.
```{python}
# pass in a string to the path you want to save
scientists.to_pickle('output/scientists_df.pickle')
```
]

.panel[.panel-name[(3)]
- To read `pickle` data, we can use the `pd.read_pickle()` function.

```{python}
dataframe_pickle = pd.read_pickle(
  'output/scientists_df.pickle'
  )
```

]

]


---
# Exporting and Importing Data
### <p style="color:#00449E">  Comma-Separated Values (CSV) </p>

- Comma-separated values (CSV) are the most flexible data storage type.
  - For each row, the column information is separated with a comma.


- To export `Series` or `DataFrame` as a `csv` file, we use the `to_csv()` method.

```{python}
# index =False  does not write the row names in the CSV output
scientists.to_csv('output/scientists_df_no_index.csv', 
                   index =False)
```



---
# Exporting and Importing Data
### <p style="color:#00449E"> Excel </p>

- The more of your work you can do in Python and/or R, the easier it will be to scale up to larger projects, catch and fix mistakes, and collaborate. 

- However, Excel’s popularity and market share are unrivaled. 

- To export `Series` or `DataFrame` as an `.xlsx` file, we use the `to_excel()` method.

```{python}
# saving a DataFrame into Excel format
scientists.to_excel(
  "output/scientists_df.xlsx",
  sheet_name = "scientists",
  index = False)
```


