---
subtitle: Machine Learning Lab
title: "Tree-based Methods"
author: "Byeong-Hak Choe"
---

```{r setup, include = F}
library(tidyverse)
library(gapminder)
library(skimr)   # a better summary of data.frame
library(scales)  # scales for ggplot
library(ggthemes)  # additional ggplot themes
library(hrbrthemes) # additional ggplot themes and color pallets
library(lubridate)
library(ggridges)
library(stargazer)
library(rpart)
library(ranger)
library(randomForest)
library(vip)
library(pdp)
library(caret)

theme_set(theme_ipsum()) # setting the minimal theme for ggplot
# setting default chunk options
knitr::opts_chunk$set(
	eval = T,
	echo = T,
	message = FALSE,
	warning = FALSE
)
```

<br><br>

## Classification And Regression Tree (CART)


```{r, fig.align='center', echo = F}
knitr::include_graphics('lec_figs/tree_rain.png')
```

- Tree-logic uses a series of steps to come to a conclusion.
- The trick is to have mini-decisions combine for good choices.
- Each decision is a node, and the final prediction is a leaf node.

<br>

Decision trees are useful for both classification and regression.


- Decision trees can take any type of data, numerical or categorical.


- Decision trees make fewer assumptions about the relationship between `x` and `y`. 
  - E.g., linear model assumes the linear relationship between `x` and `y`.
  - Decision trees naturally express certain kinds of interactions among the input variables: those of the form “IF x is true AND y is true, THEN....”


<br>


- **Classification trees** have class probabilities at the leaves.
  - *Probability I'll be in heavy rain is 0.9 (so take an umbrella).*


- **Regression trees** have a mean response at the leaves.
  - *The expected amount of rain is 2 inches (so take an umbrella).*


- **CART**: Classification and Regression Trees.

<br><br>


- We need a way to estimate the sequence of decisions.
  - How many are they? 
  - What is the order?


- CART grows the tree through a sequence of splits:
  1. Given any set (node) of data, we can find the optimal split (**the error minimizing split**) and divide into two child sets.
  2. We then look at each child set, and again find the optimal split to divide it into two *homogeneous subsets*.
  3. The children become parents, and we look again for the optimal split on their new children (the grandchildren!).


- We stop splitting and growing when the size of the leaf nodes hits some minimum threshold (e.g., say no less than 10 observations per leaf).

<br>

__Objective at each split__: find the __best__ variable to partition the data into one of two regions, $R_1$ & $R_2$, to __minimize the error__ between the actual response, $y_i$, and the node's predicted constant, $c_i$


- For regression we minimize the sum of squared errors (SSE):

$$
S S E=\sum_{i \in R_{1}}\left(y_{i}-c_{1}\right)^{2}+\sum_{i \in R_{2}}\left(y_{i}-c_{2}\right)^{2}
$$



- For classification trees we minimize the node's _impurity_ the __Gini index__

  - where $p_k$ is the proportion of observations in the node belonging to class $k$ out of $K$ total classes
  
  - want to minimize $Gini$: small values indicate a node has primarily one class (_is more pure_)
  
  - Gini impurity measures the degree of a particular variable being wrongly classified when it is randomly chosen.

$$
Gini = 1 - \sum_k^K p_k^2
$$


<br><br>

### NBC Show Data
- The dataset (`nbc` and `demog`) is from NBC's TV pilots:
  - Gross Ratings Points (GRP): estimated total viewership, which measures broadcast marketability.
  - Projected Engagement (PE): a more suitable measure of audience.
  - After watching a show, viewer is quizzed on order and detail.
  - This measures their engagement with the show (and ads!).

```{r}
library(tidyverse)
nbc <- read_csv('https://bcdanl.github.io/data/nbc_show.csv')
nbc$Genre <- as.factor(nbc$Genre)
```


```{r, result = 'asis', echo = F, message = F, warning = F}
rmarkdown::paged_table(nbc)
```


```{r}
skim(nbc)
```

```{r}
ggplot(nbc) +
  geom_point(aes(x = GRP, y = PE, color = Genre),
             alpha = .75)
```


- Consider a classification tree to predict `Genre` from demographics.
  - Output from tree shows a series of decision nodes and the proportion in each `Genre` at these nodes, down to the leaves.


```{r}
demog <- read_csv(
  'https://bcdanl.github.io/data/nbc_demog.csv'
)
```


```{r, result = 'asis', echo = F, message = F, warning = F}
rmarkdown::paged_table(demog)
```


```{r}
skim(demog)
```

```{r}
# install.packages(c("tree","randomForest","ranger", "rpart", "vip", "pdp", "caret"))
library(tree)
genretree <- tree(nbc$Genre ~ ., 
                  data = demog[,-1], 
                  mincut = 1)
nbc$genrepred <- predict(genretree, 
                         newdata = demog[,-1], 
                         type = "class")
```

```{r}
# tree plot (dendrogram)
plot(genretree, col=8, lwd=2)
text(genretree, label="yprob")
```
Consider predicting engagement from ratings and genre.

- Leaf predictions are expected engagement.

```{r}
# mincut=1 allows for leaves containing a single show,
# with expected engagement that single show's PE.
nbctree <- tree(PE ~ Genre + GRP, data=nbc[,-1], mincut=1)
nbc$PEpred <- predict(nbctree, newdata=nbc[,-1])

## tree plot (dendrogram)
plot(nbctree, col=8, lwd=2)
text(nbctree)

ggplot(nbc) +
  geom_point(aes(x = GRP, y = PE, color = Genre) ) +
  geom_line(aes(x = GRP, y = PEpred, color = Genre) )
```

- PE increases with GRP, but in jumps! 


<br><br><br>

## CV Tree

- The biggest challenge with CART models is avoiding **overfit**.
  - For CART, the usual solution is to rely on **cross validation** (CV).
  - The way to cross-validate the fully fitted tree is to **prune** it by removing split rules from the bottom up:


- At each step, remove the split that contributes least to deviance reduction.
  - This is a reverse to CART's growth process.


- Pruning yields candidate tree.
  - Each prune step produces a candidate tree model, and we can compare their out-of-sample prediction performance through CV.

<br><br>

### Boston Housing Data
- The `MASS` package includes the `Boston` data.frame, which has 506 observations and 14 variables. 
  - `crim`: per capita crime rate by town.
  - `zn`: proportion of residential land zoned for lots over 25,000 sq.ft.
  - `indus`: proportion of non-retail business acres per town.
  - `chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
  - `nox`: nitrogen oxides concentration (parts per 10 million).
  - `rm`: average number of rooms per dwelling.
  - `age`: proportion of owner-occupied units built prior to 1940.
  - `dis`: weighted mean of distances to five Boston employment centres.
  - `rad`: index of accessibility to radial highways.
  - `tax`: full-value property-tax rate per $10,000.
  - `ptratio`: pupil-teacher ratio by town.
  - `black`: $1000(Bk - 0.63)^2$ where $Bk$ is the proportion of blacks by town.
  - `lstat`: lower status of the population (percent).
  - `medv`: median value of owner-occupied homes in $1000s.


- For more details about the data set, try `?Boston`.


- The goal is to predict housing values.


```{r}
library(MASS)
?Boston
Boston <- MASS::Boston
```


```{r, result = 'asis', echo = F, message = F, warning = F}
rmarkdown::paged_table(Boston)
```

```{r}
skim(MASS::Boston)
```

- Spliting training and testing data
```{r}
set.seed(42120532)
index <- sample(nrow(Boston),nrow(Boston)*0.80)
Boston.train <- Boston[index,]
Boston.test <- Boston[-index,]
```


- A bit of data visualization
```{r}
Boston_vis <- Boston %>%
  gather(-medv, key = "var", value = "value") %>% 
  filter(var != "chas")

ggplot(Boston_vis, aes(x = value, y = medv)) +
  geom_point(alpha = .33) +
  geom_smooth() +
  facet_wrap(~ var, scales = "free") 

ggplot(Boston_vis, aes(y = value)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 1) +
  facet_wrap(~ var, scales = "free") 

ggplot(Boston_vis, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ var, scales = "free") 

```

- `rpart()` 
  - Runs 10-fold CV tree to tune $\alpha$ (CP) for **pruning**.
  - Selects the number of terminal nodes via 1-SE rule.
```{r}
library(rpart)
boston_tree <- rpart(medv ~ .,
                     data = Boston.train, method  = "anova")
boston_tree
```

- `printcp` displays `cp` table for Fitted `rpart()` object:

```{r}
printcp(boston_tree)
```


- `rpart.plot()` plots the estimated tree structure from an `rpart()` object.
  - With `method = "anova"` (a continuous outcome variable), each node shows:
    - the predicted value;
    - the percentage of observations in the node.
```{r}
library(rpart.plot)
rpart.plot(boston_tree)
```


- With `method = "class"` (a binary outcome variable), each node will show:
  - the predicted class;
  - the predicted probability;
  - the percentage of observations in the node.


<br>

- `plotcp()` gives a visual representation of the cross-validation results in an `rpart()` object.
  - The size of a decision tree is the number of leaf nodes (non-terminal nodes) in the tree.

```{r}
plotcp(boston_tree)
```



- What about the full tree? (`cp = 0`)
  - The `control` parameter in `rpart()` allows for controlling the `rpart` fit. (see `rpart.fit`)
    - `cp`: complexity parameter. the minimum improvement in the model needed at each node. 
      - The higher the `cp`, the smaller the size of tree.
    - `xval`: number of cross-validations
```{r}
full_boston_tree <- rpart(formula = medv ~ .,
                       data = Boston.train, method  = "anova", 
                       control = list(cp = 0, xval = 10))
```

```{r}
rpart.plot(full_boston_tree)
```

- Compare the full tree with the pruned tree.
  - Which variable is not included in the pruned tree?
  
  

```{r}
plotcp(full_boston_tree)
```

- We can train the CV trees with the `caret` package as well:

```{r}
library(caret)
caret_boston_tree <- train(medv ~ .,
                        data = Boston.train, method = "rpart",
                        trControl = trainControl(method = "cv", number = 10),
                        tuneLength = 20)
ggplot(caret_boston_tree)
```

```{r}
rpart.plot(caret_boston_tree$finalModel)
```

<br><br><br>

## Random Forest

### Why should we try other tree models?
- CART automatically learns non-linear response functions and will discover interactions between variables.
  1. Unfortunately, it is tough to avoid overfit with CART.
  2. High variance, i.e. split a dataset in half and grow tress in each half, the result will be very different
    - CART generally results in higher test set error rates.
  - Real structure of the tree is not easily chosen via cross-validation (CV).
  

- One way to mitigate the shortcomings of CART is **bootstrap aggregation**, or **bagging**.

<br><br>

### Bagging Algorithm

```{r, fig.align='center', echo = F}
knitr::include_graphics('lec_figs/bootstrap-scheme.png')
```

- **Bootstrap** is random sampling with *replacement*.

- **Aggregation** is combining the results from many trees together, each constructed with a different bootstrapped sample of the data.


```{r, fig.align='center', echo = F}
knitr::include_graphics('lec_figs/pds_fig10_5.png')
```

- Real structure that persists across datasets shows up in the average. 
- A bagged *ensemble* of trees is also less likely to overfit the data.


- To generate a prediction for a new point:
  - Regression: take the average across the trees
  - Classification: take the majority vote across the trees
    - assuming each tree predicts a single class (could use probabilities instead...)


- Bagging improves prediction accuracy via **wisdom of the crowds** but at the expense of interpretability.
  - Easy to read one tree, but how do we read 500 trees?
  - However, we can still use the measures of variable importance and partial dependence to summarize our models.

<br><br>

### Random Forest Algorithm
- Random forests are **an extension of bagging**.
  - At each split, the algorithm limits the variables considered to a **random subset** $m_{try}$ of the given $p$ number of variables.
  - It introduce $m_{try}$ as a tuning parameter: typically use $p/3$ for regression or $\sqrt{p}$ for classification.
```{r, fig.align='center', echo = F}
knitr::include_graphics('lec_figs/pds_fig10_6.png')
```

- Split-variable randomization adds more randomness to make each tree more independent of each other.


- The final ensemble of trees is bagged to make the random forest predictions.


<br>

- Since the trees are constructed via bootstrapped data (samples with replacements), each sample is likely to have duplicate observations.


- **Out-of-bag (OOB)**, original observations not contained in a single bootstrap sample, can be used to make out-of-sample predictive performance of the model.



### Intuition behind the Random Forest Algorithm

- The reason Random Forest algorithm considers a random subset of features at each split in the decision tree is to increase the diversity among the individual trees in the forest. 
  - This is a method to make the model more robust and prevent overfitting.


- If all the variables were considered at each split, each decision tree in the forest would look more similar, as they would likely use the same (or very similar) variables for splitting, especially the first few splits which typically have the most impact on the structure of the tree. 
  - This is because some features might be so informative that they would always be chosen for splits early in the tree construction process if all features were considered. 
  - This would make the trees in the forest correlated, which would reduce the power of the ensemble.


- By considering only a random subset of the variables at each split, we increase the chance that less dominant variables are considered, leading to a more diverse set of trees. 
  - This diversity is key to the power of the Random Forest algorithm, as it allows for a more robust prediction that is less likely to overfit the training data. 
  - It also helps to reduce the variance of the predictions, as the errors of the individual trees are likely to cancel each other out when averaged (for regression) or voted on (for classification).

<br>

- `ranger` package is a popular & fast implementation (see `randomForest` for the original).
  - Let's consider the estimation with `randomForest` first.
```{r}
library(randomForest)
bag.boston <- randomForest(medv ~ ., data = Boston.train, 
                           mtry=13, ntree = 50,
                           importance =TRUE)
bag.boston
plot(bag.boston)

```


- Now Let's consider the estimation with `ranger`.

```{r}
library(ranger)
bag.boston_ranger <- ranger(medv ~ ., data = Boston.train, 
                            mtry = 13, num.trees = 50,
                            importance = "impurity")
bag.boston_ranger
```


<br><br>

#### CV Tree vs. Random Forest
- We can compare the performance of the CV CART and the RF via **MSE**.


- MSE from CV Tree
```{r}
# prediction
boston.train.pred.CART <- predict(boston_tree, Boston.train)
boston.test.pred.CART <- predict(boston_tree, Boston.test)

# MSE
mean((Boston.test$medv - boston.test.pred.CART)^2)
mean((Boston.train$medv - boston.train.pred.CART)^2)
```

- MSE from Random Forest
```{r}
# prediction
boston.train.pred.RF <- predict(bag.boston_ranger, Boston.train)$predictions
boston.test.pred.RF <- predict(bag.boston_ranger, Boston.test)$predictions

# MSE
mean((Boston.test$medv - boston.test.pred.RF)^2)
mean((Boston.train$medv - boston.train.pred.RF)^2)
```

<br><br>

### Variable Importance in the Tree-based Models

- **Variable importance** is measured based on reduction in SSE.
  - Mean Decrease Accuracy (% increase in MSE): This shows how much our model accuracy decreases if we leave out that variable.
  - Mean Decrease Gini (Increase in Node Purity) : This is a measure of variable importance based on the Gini impurity index used for the calculating the splits in trees.

<br>

- Out-of-bag samples for datum `x1`
```{r, echo = F}
knitr::include_graphics('lec_figs/pds_fig10_7.png')
```

<br>

- Calculating variable importance of variable `v1`
```{r, echo = F}
knitr::include_graphics('lec_figs/pds_fig10_8.png')
```

<br>

- Since we set `importance` not to equal to `"none"` when using `rpart`, `caret`, and `ranger`, we can evaluate variable importance using the left-out sample.
```{r}
vip(caret_boston_tree, geom = "point")
```

```{r}
vip(full_boston_tree, geom = "point")
```

```{r}
vip(boston_tree, geom = "point")
```

```{r}
vip(bag.boston_ranger, geom = "point")
```

<br>

- We can also summarize the relationship between a predictor and the predicted outcome using a **partial dependence plot**
```{r}
library(pdp)
# predictor, lstat
partial(bag.boston_ranger, pred.var = "lstat") %>% autoplot()
```

```{r}
# predictor, rm
partial(bag.boston_ranger, pred.var = "rm") %>% autoplot()
```

```{r}
# predictor, rad
partial(bag.boston_ranger, pred.var = "rad") %>% autoplot()
```


## References

-   [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://hastie.su.domains/ElemStatLearn/) by [Trevor Hastie](https://hastie.su.domains), [Robert Tibshirani](https://tibshirani.su.domains) and [Jerome Friedman](https://jerryfriedman.su.domains).

<!-- - [Causal Inference: The Mixtape](https://mixtape.scunning.com) by [Scott Cunningham](https://www.scunning.com). -->

<!-- - [Statistical Inference via Data Science: A ModernDive into R and the Tidyverse](https://moderndive.com) by [Chester Ismay](https://chester.rbind.io) and [Albert Y. Kim](http://rudeboybert.rbind.io). -->

<!-- -   [An Introduction to Statistical Learning](https://www.statlearning.com) by [Gareth James](https://www.garethmjames.com), [Daniela Witten](https://www.danielawitten.com), [Trevor Hastie](https://hastie.su.domains), and [Robert Tibshirani](https://tibshirani.su.domains). -->

-   [Modern Business Analytics](https://www.mheducation.com/highered/product/modern-business-analytics-taddy-hendrix/M9781264071678.html) by [Matt Taddy](https://www.linkedin.com/in/matt-taddy-433078137/), [Leslie Hendrix](https://sc.edu/study/colleges_schools/moore/directory/hendrix.leslie.php), and [Matthew Harding](https://www.harding.ai).

-   [Practical Data Science with R](https://www.manning.com/books/practical-data-science-with-r-second-edition) by [Nina Zumel](https://ninazumel.com) and [John Mount](https://win-vector.com/john-mount/).

-   [Summer Undergraduate Research Experience (SURE) 2022 in Statistics at Carnegie Mellon University](https://www.stat.cmu.edu/cmsac/sure/2022/materials/) by [Ron Yurko](https://www.stat.cmu.edu/~ryurko/).
