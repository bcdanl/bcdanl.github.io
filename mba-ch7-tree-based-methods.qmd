---
subtitle: Machine Learning Lab
title: "Tree-based Methods"
author: "Byeong-Hak Choe"
editor: visual
---

```{r setup, include = F}
library(tidyverse)
library(gapminder)
library(skimr)   # a better summary of data.frame
library(scales)  # scales for ggplot
library(ggthemes)  # additional ggplot themes
library(hrbrthemes) # additional ggplot themes and color pallets
library(lubridate)
library(ggridges)
library(stargazer)
theme_set(theme_ipsum()) # setting the minimal theme for ggplot
# setting default chunk options
knitr::opts_chunk$set(
	eval = T,
	echo = T,
	message = FALSE,
	warning = FALSE
)
```

<br><br>

## Bias-Variance Tradeoff

```{r, fig.align='center'}
knitr::include_graphics('lec_figs/bias-variance-tradeoff-1.png')
```

### Decomposition of MSE

-   The mean squared error (MSE) of the model with unknown set of parameters $\mathbf{\theta}$ can be decomposed into *bias* and *variance*.

$$
\begin{align}
M S E &\,=\, \frac{\sum_{i = 1}^{ n}\,(\, y_{i} - \hat{y}_{i} \,)^2}{n}\\
&\,=\,E \,[\, y_{i} - \hat{f}(\mathbf{X}) \,]^{2}\\
&\,=\, [\text{Bias}(\, \hat{f}(\mathbf{X}) \,)]^{2} \,+\, Var(\,\hat{f}(\mathbf{X})\,) \,+\, \text{Var}(\epsilon)
\end{align}
$$

-   In order to minimize the MSE above, we need to select a machine learning methods that simultaneously achieves low variance and low bias.
    -   Note that both squared bias and variance are nonnegative.
    -   The MSE can never lie below $\text{Var}(\epsilon)$, the irreducible error.

```{r, fig.align='center'}
knitr::include_graphics('lec_figs/bias-variance-tradeoff-2.png')
```

-   The horizontal dashed line represents $\text{Var}(\epsilon)$, the irreducible error.

-   The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE.

```{r, fig.align='center'}
knitr::include_graphics('lec_figs/bias-variance-tradeoff-3.png')
```

<br>

## Regularization

-   Regularization helps find some optimal amount of flexibility that minimizes MSE.

-   Regularized regression can resolve the following problems

    -   **Quasi-separation** in logistic regression:
        -   The input variable yields an almost perfect prediction of the binary response variable.
    -   **Multicolinearity** in regression:
        -   Variables `age` and `years_of_workforce` in linear regression of `income` are too highly correlated.

### Ridge Regression

-   **Ridge regression** introduces a **shrinkage penalty** $\lambda \geq 0$ by minimizing:

$$\sum_i^n \big(Y_i - \beta_0 -  \sum_j^p \beta_j x_{ij}\big)^2 + \lambda \sum_j^p \beta_j^2 = \text{SSE} + \lambda \sum_j^p \beta_j^2$$ - As $\lambda$ increases $\Rightarrow$ flexibility of models decreases

-   **increases bias, but decreases variance**

-   For fixed value of $\lambda$, ridge regression fits only a single model

    -   need to use cross-validation to **tune** $\lambda$

-   For example: note how the magnitude of the coefficient for `Income` trends as $\lambda \rightarrow \infty$

```{r out.width='40%', echo = FALSE, fig.align='center'}
knitr::include_graphics('lec_figs/cmu-ridge.png')
```

-   The coefficient **shrinks towards zero**, but never actually reaches it.
    -   Ridge regression tends to average the collinear variables together.
-   `Income` is always a variable in the learned model, regardless of the value of $\lambda$.

<br>

### Lasso Regression

-   Ridge regression **keeps all variables**.

-   But Lasso regression gets lid of variables.

    -   **Lasso** enables variable selection with $\lambda$ by minimizing:

$$\sum_i^n \big(Y_i - \beta_0 -  \sum_j^p \beta_j X_{ij}\big)^2 + \lambda \sum_j^p\vert  \beta_j \vert = \text{SSE} + \lambda \sum_j^p \vert \beta_j \vert$$

-   As $\lambda$ increases $\Rightarrow$ flexibility of models decreases

    -   **increases bias, but decreases variance**

-   Lasso can handle the $p > n$ case, i.e. more variables than observations!

-   Lasso regression **performs variable selection** yielding **sparse** models, which only includes a small subset of the available variables that are relevant to predicting the outcome of interest.

```{r out.width='40%', echo = FALSE, fig.align='center'}
knitr::include_graphics("lec_figs/cmu-lasso.png")
```

-   The coefficient shrinks towards and **eventually equals zero** at $\lambda \approx 1000$

-   If the optimum value of $\lambda$ is larger, then `Income` would NOT be included in the learned model.

<br>

### Elastic net

$$\sum_{i}^{n}\left(Y_{i}-\beta_{0}-\sum_{j}^{p} \beta_{j} X_{i j}\right)^{2}+\lambda\left[(1-\alpha)\|\beta\|_{2}^{2} / 2+\alpha\|\beta\|_{1} \right]$$ - $\vert \vert \beta \vert \vert_1$ is the $\ell_1$ norm: $\vert \vert \beta \vert \vert_1 = \sum_j^p \vert \beta_j \vert$

-   $\vert \vert \beta \vert \vert_2$ is the $\ell_2$, Euclidean, norm: $\vert \vert \beta \vert \vert_2 = \sqrt{\sum_j^p \beta_j^2}$

-   Ridge penalty: $\lambda \cdot (1 - \alpha) / 2$

-   Lasso penalty: $\lambda \cdot \alpha$

-   $\alpha$ controls the **mixing** between the two types, ranges from 0 to 1

    -   $\alpha = 1$ returns lasso

    -   $\alpha = 0$ return ridge

<br>

### Gamma Lasso Regression

$$\sum_i^n \big(Y_i - \beta_0 -  \sum_j^p \beta_j X_{ij}\big)^2 + \lambda \sum_j^p\vert  \beta_j \vert = \text{SSE} + \lambda \sum_j^p \log(\, 1 + \vert \beta_j \vert\,)$$

<br>

## Putting a Cost on Complexity

-   With regularization, we put a cost on the magnitude of each $\beta_{p}$.

    -   This penalizes *complexity*, because the $\beta_{p}$ coefficients are what allow our predicted $\hat{y}$ values to move around with different input $X$ values.

-   If we force all the $\hat{\beta}_{p}$ to be close to zero, then our $\hat{y}$ values will be shrunk toward $\bar{y}$ and when we jitter the data your predictions will not change as much as they would if we did not include a penalty term during estimation.

-   $\lambda$ is the penalty weight that determines the price of complexity.

    -   It is a tuning parameter that needs to be selected in some data-dependent manner.

    ```{r, fig.align='center'}
    knitr::include_graphics('lec_figs/mba-3-4.png')
    ```

-   The ridge penalty ($\beta^{2}$) places little penalty on small values of $\beta$.

-   The Lasso penalty ($\vert \beta \vert$) places a constant penalty on incremental deviations from zero.

-   The gamma Lasso penalty ($\log(1 + \vert \beta \vert)$) places extreme cost on the move from zero to small values of $\beta$, but for large values the rate of penalty change is small.

    -   This encourages lots of zeros in our fit while allowing large betas to be estimated without any bias!

<br>

### Advantages of the Lasso

1.  The Lasso gives the least possible amount of bias on large signals while still retaining the stability of a convex penalty like ridge (convex means that the penalty does not flatten out for large values.).

2.  The Lasso will yield automatic variable screening---**model selection**---some of the $\hat{\beta}_{p}$ will be zero!

```{r}
knitr::include_graphics('lec_figs/ridge-lasso-animation.gif')
```

-   Source: [Quora](https://www.quora.com/What-is-the-intuition-that-the-l-1-norm-leads-to-sparse-solutions)

-   Here is another [illustration of the Lasso and its path in 2D](https://twitter.com/i/status/1107625298936451073).

<br>

## Regularization Algorithms

### Standardizing Data

-   For either ridge, lasso, gamma lasso or elastic net: **we should consider standardizing our data**

-   Common convention: within each column, compute then subtract off the sample mean, and compute the divide off the sample standard deviation:

$$\tilde{x}_{ij} = \frac{x_{ij} - \bar{x}_j}{s_{x,j}}$$

-   [`glmnet`](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) and [`gamlr`](https://github.com/TaddyLab/gamlr) package do this by default and reports coefficients on the original scale

    -   $\lambda$ and $\alpha$ are **tuning parameters**

    -   When using `glmnet`, the `cv.glmnet()` function will perform the cross-validation for us

-   [`gamlr`](https://github.com/TaddyLab/gamlr) package does implements the gamma lasso algorithm.

    -   When using `gamlr`, the `cv.gamlr()` function will perform the cross-validation for us.

-   `glmnetUtils` enables us to use a `data.frame` to provide data to the model.

-   `glmnet` and `gamlr` use a `sparse.matrix`, instead of a `data.frame`, to provide data to the model.

-   A sparse matrix is a matrix with many zero entries.

    -   A sparse matrix is almost essential in big data analysis because of its lower storage costs and faster computation.

```{r, caption = "An example of a sparse matrix representation"}
knitr::include_graphics('lec_figs/sparse-matrix.png')
```

<br>

### k-fold Cross-Validation

-   Split the data into `k` random and roughly evenly sized subsets, called **folds**. Then, for \texttt{k = 1,...,K}:

    -   Step 1. Use all data except the k-th fold to train a model
    -   Step 2. Record the error rate (e.g., MSE, $R^2$, AICc) for predictions on the left-out fold based on the fitted model (out-of-sample (OOS) deviance).

```{r, caption = "Partitioning data for 3-fold corss validaation"}
knitr::include_graphics('lec_figs/pds_fig69.png')
```

-   For each candidate model, $m \in\{1, \cdots, M\}$, with $\lambda_{m}$, we do k-fold cross-validation (CV).

-   So k-fold CV will yield a set of `k` OOS deviances for each of our candidate models.

    -   So we select the model with the best OOS performance!

-   How should we choose `k`?

    -   More is better but only up to a point. <!-- - Note that the variance of the `k`-CV estimate of average OOS deviance is the variance on each left-out fold  -->
    -   We can experiment on the choice of `k`.

<br>

### Schematic of `cv.glmnet()` and `cv.gamlr()`

```{r, caption = "Partitioning data for 3-fold corss validaation"}
knitr::include_graphics('lec_figs/pds_fig718.png')
```

-   $\lambda_{.min}$: the $\lambda$ for the model with the minimum cross-validation (CV) error.

-   $\lambda_{1se}$: corresponds to the model with cross-validation error, which is one standard error (se) of CV error above the minimum CV error.

<br>

### Schematic of `cva.glmnet()`

```{r, caption = "Partitioning data for 3-fold corss validaation"}
knitr::include_graphics('lec_figs/pds_fig721.png')
```

<br>

### 

<br><br>

## References

-   [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://hastie.su.domains/ElemStatLearn/) by [Trevor Hastie](https://hastie.su.domains), [Robert Tibshirani](https://tibshirani.su.domains) and [Jerome Friedman](https://jerryfriedman.su.domains).

<!-- - [Causal Inference: The Mixtape](https://mixtape.scunning.com) by [Scott Cunningham](https://www.scunning.com). -->

<!-- - [Statistical Inference via Data Science: A ModernDive into R and the Tidyverse](https://moderndive.com) by [Chester Ismay](https://chester.rbind.io) and [Albert Y. Kim](http://rudeboybert.rbind.io). -->

<!-- -   [An Introduction to Statistical Learning](https://www.statlearning.com) by [Gareth James](https://www.garethmjames.com), [Daniela Witten](https://www.danielawitten.com), [Trevor Hastie](https://hastie.su.domains), and [Robert Tibshirani](https://tibshirani.su.domains). -->

-   [Modern Business Analytics](https://www.mheducation.com/highered/product/modern-business-analytics-taddy-hendrix/M9781264071678.html) by [Matt Taddy](https://www.linkedin.com/in/matt-taddy-433078137/), [Leslie Hendrix](https://sc.edu/study/colleges_schools/moore/directory/hendrix.leslie.php), and [Matthew Harding](https://www.harding.ai).

-   [Practical Data Science with R](https://www.manning.com/books/practical-data-science-with-r-second-edition) by [Nina Zumel](https://ninazumel.com) and [John Mount](https://win-vector.com/john-mount/).

-   [Summer Undergraduate Research Experience (SURE) 2022 in Statistics at Carnegie Mellon University](https://www.stat.cmu.edu/cmsac/sure/2022/materials/) by [Ron Yurko](https://www.stat.cmu.edu/~ryurko/).
