[
  {
    "objectID": "DANL210_lab2a.html",
    "href": "DANL210_lab2a.html",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab2a.html#load-dataframe",
    "href": "DANL210_lab2a.html#load-dataframe",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbeer_mkt = pd.read_csv('https://bcdanl.github.io/data/beer_markets.csv')\nbeer_mkt.head(10)\n\n\n\n\n\n  \n    \n      \n      hh\n      _purchase_desc\n      quantity\n      brand\n      dollar_spent\n      beer_floz\n      price_per_floz\n      container\n      promo\n      market\n      ...\n      age\n      employment\n      degree\n      cow\n      race\n      microwave\n      dishwasher\n      tvcable\n      singlefamilyhome\n      npeople\n    \n  \n  \n    \n      0\n      2000946\n      BUD LT BR CN 12P\n      1\n      BUD LIGHT\n      8.14\n      144.0\n      0.056528\n      CAN\n      False\n      RURAL ILLINOIS\n      ...\n      50+\n      none\n      Grad\n      none/retired/student\n      white\n      True\n      True\n      premium\n      False\n      1\n    \n    \n      1\n      2003036\n      BUD LT BR CN 24P\n      1\n      BUD LIGHT\n      17.48\n      288.0\n      0.060694\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      2\n      2003036\n      BUD LT BR CN 24P\n      2\n      BUD LIGHT\n      33.92\n      576.0\n      0.058889\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      3\n      2003036\n      BUD LT BR CN 30P\n      2\n      BUD LIGHT\n      34.74\n      720.0\n      0.048250\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      4\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      40.48\n      864.0\n      0.046852\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      5\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      42.96\n      864.0\n      0.049722\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      6\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      40.96\n      864.0\n      0.047407\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      7\n      2001521\n      BUD LT BR CN 6P\n      5\n      BUD LIGHT\n      30.60\n      480.0\n      0.063750\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n    \n      8\n      2001521\n      BUD LT BR CN 6P\n      1\n      BUD LIGHT\n      9.99\n      96.0\n      0.104063\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n    \n      9\n      2001521\n      BUD LT BR CN 6P\n      5\n      BUD LIGHT\n      30.70\n      480.0\n      0.063958\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n  \n\n10 rows × 24 columns\n\n\n\n\nVariable Description\n\nhh: An identifier of the purchasing household;\n_purchase_desc: Details on the purchased item;\nquantity: Number of items purchased;\nbrand: BUD LIGHT, BUSCH LIGHT, COORS LIGHT, MILLER LITE, or NATURAL LIGHT;\nspent: Total dollar value of purchase;\nbeer_floz: Total volume of beer, in fluid ounces;\nprice_per_floz: Price per fl.oz. (i.e., spent/beer_floz);\ncontainer: Type of container;\npromo: Whether the item was promoted (coupon or something else);\nmarket: Scan-track market (or state if rural);\nvarious demographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher.\n\nSummarize DataFrame beer_mkt.\n\n\nskim(beer_mkt)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 73115  │ │ string      │ 13    │                                                          │\n│ │ Number of columns │ 24     │ │ bool        │ 6     │                                                          │\n│ └───────────────────┴────────┘ │ float64     │ 3     │                                                          │\n│                                │ int64       │ 2     │                                                          │\n│                                └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name      ┃ NA  ┃ NA %  ┃ mean      ┃ sd        ┃ p0       ┃ p25     ┃ p75      ┃ p100     ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━┩  │\n│ │ hh               │   0 │     0 │  17000000 │  12000000 │  2000000 │ 8200000 │ 30000000 │ 30000000 │ ▂█   █ │  │\n│ │ quantity         │   0 │     0 │       1.3 │       1.1 │        1 │       1 │        1 │       48 │   █    │  │\n│ │ dollar_spent     │   0 │     0 │        14 │       8.7 │     0.51 │       9 │       16 │      160 │   █▁   │  │\n│ │ beer_floz        │   0 │     0 │       270 │       200 │       12 │     140 │      360 │     9200 │   █    │  │\n│ │ price_per_floz   │   0 │     0 │     0.056 │     0.013 │   0.0013 │   0.046 │    0.064 │     0.23 │   ▁█   │  │\n│ └──────────────────┴─────┴───────┴───────────┴───────────┴──────────┴─────────┴──────────┴──────────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                   ┃ NA     ┃ NA %       ┃ words per row               ┃ total words            ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ _purchase_desc                │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ brand                         │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ container                     │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ market                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ buyertype                     │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ income                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ age                           │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ employment                    │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ degree                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ cow                           │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ race                          │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ tvcable                       │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ npeople                       │      0 │          0 │                         5.3 │                 390000 │  │\n│ └───────────────────────────────┴────────┴────────────┴─────────────────────────────┴────────────────────────┘  │\n│                                                      bool                                                       │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                               ┃ true            ┃ true rate                 ┃ hist             ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩  │\n│ │ promo                                     │           15000 │                       0.2 │      █    ▂      │  │\n│ │ childrenUnder6                            │            5000 │                     0.068 │      █    ▁      │  │\n│ │ children6to17                             │           15000 │                       0.2 │      █    ▂      │  │\n│ │ microwave                                 │           73000 │                      0.99 │           █      │  │\n│ │ dishwasher                                │           53000 │                      0.73 │      ▃    █      │  │\n│ │ singlefamilyhome                          │           59000 │                      0.81 │      ▂    █      │  │\n│ └───────────────────────────────────────────┴─────────────────┴───────────────────────────┴──────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nbeer_mkt.describe()\n\n\n\n\n\n  \n    \n      \n      hh\n      quantity\n      dollar_spent\n      beer_floz\n      price_per_floz\n    \n  \n  \n    \n      count\n      7.311500e+04\n      73115.000000\n      73115.000000\n      73115.000000\n      73115.000000\n    \n    \n      mean\n      1.740772e+07\n      1.317527\n      13.777683\n      265.926853\n      0.055951\n    \n    \n      std\n      1.158215e+07\n      1.149649\n      8.722942\n      199.522488\n      0.013417\n    \n    \n      min\n      2.000235e+06\n      1.000000\n      0.510000\n      12.000000\n      0.001315\n    \n    \n      25%\n      8.223438e+06\n      1.000000\n      8.970000\n      144.000000\n      0.046306\n    \n    \n      50%\n      8.413624e+06\n      1.000000\n      12.990000\n      216.000000\n      0.055509\n    \n    \n      75%\n      3.017132e+07\n      1.000000\n      16.380000\n      360.000000\n      0.063750\n    \n    \n      max\n      3.044072e+07\n      48.000000\n      159.130000\n      9216.000000\n      0.234063\n    \n  \n\n\n\n\n\nbeer_mkt.groupby('brand').describe()\n\n\n\n\n\n  \n    \n      \n      hh\n      quantity\n      ...\n      beer_floz\n      price_per_floz\n    \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n      count\n      mean\n      ...\n      75%\n      max\n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      brand\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      BUD LIGHT\n      21592.0\n      1.728292e+07\n      1.151146e+07\n      2000235.0\n      8212165.0\n      8403502.0\n      30173350.0\n      30439375.0\n      21592.0\n      1.326834\n      ...\n      288.0\n      2880.0\n      21592.0\n      0.061601\n      0.012090\n      0.002593\n      0.055417\n      0.060708\n      0.067604\n      0.187500\n    \n    \n      BUSCH LIGHT\n      8674.0\n      1.950770e+07\n      1.154444e+07\n      2001531.0\n      8274694.0\n      30022041.0\n      30199810.0\n      30440718.0\n      8674.0\n      1.326954\n      ...\n      360.0\n      2304.0\n      8674.0\n      0.044922\n      0.009065\n      0.003734\n      0.039972\n      0.044340\n      0.048542\n      0.234063\n    \n    \n      COORS LIGHT\n      13074.0\n      1.777439e+07\n      1.167094e+07\n      2000417.0\n      8207890.0\n      9000465.0\n      30173465.0\n      30440718.0\n      13074.0\n      1.267171\n      ...\n      360.0\n      1984.0\n      13074.0\n      0.060638\n      0.012364\n      0.004583\n      0.052833\n      0.060000\n      0.067361\n      0.180417\n    \n    \n      MILLER LITE\n      17159.0\n      1.743478e+07\n      1.160809e+07\n      2000946.0\n      8239785.0\n      8425198.0\n      30181493.0\n      30440718.0\n      17159.0\n      1.266915\n      ...\n      288.0\n      9216.0\n      17159.0\n      0.059676\n      0.011804\n      0.001315\n      0.052500\n      0.059028\n      0.065903\n      0.173333\n    \n    \n      NATURAL LIGHT\n      12616.0\n      1.576071e+07\n      1.134515e+07\n      2001715.0\n      8199800.0\n      8354289.0\n      30130924.0\n      30440718.0\n      12616.0\n      1.416138\n      ...\n      360.0\n      2160.0\n      12616.0\n      0.043939\n      0.008032\n      0.003467\n      0.039444\n      0.043368\n      0.048542\n      0.123750\n    \n  \n\n5 rows × 40 columns"
  },
  {
    "objectID": "DANL210_lab2a.html#q1a",
    "href": "DANL210_lab2a.html#q1a",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1a",
    "text": "Q1a\n\nSort the DataFrame beer_mkt by hh in ascending order.\n\n\nbeer_mkt = beer_mkt.sort_values('hh')"
  },
  {
    "objectID": "DANL210_lab2a.html#q1b",
    "href": "DANL210_lab2a.html#q1b",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1b",
    "text": "Q1b\n\nFind the top 5 beer markets in terms of the number of households that purchased beer.\n\n\n# value_counts()\nq1b = (\n       beer_mkt[['market', 'hh']]\n       .value_counts()\n       .sort_index()\n       .groupby('market')\n       .count()\n       .sort_values(ascending = False)\n       )\n       \n# size()\nq1b = (\n       beer_mkt.groupby(['market', 'hh'])\n       .size()\n       .count(level='market')\n       .sort_values(ascending = False)\n       )\n\n\n# adding a ranking variable to the DataFrame\nq1b = pd.DataFrame(q1b, columns=['n_hh'])\nq1b['ranking'] = q1b['n_hh'].rank(method = 'dense', ascending = False)\nq1b = q1b.query('ranking <= 5')"
  },
  {
    "objectID": "DANL210_lab2a.html#q1c",
    "href": "DANL210_lab2a.html#q1c",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1c",
    "text": "Q1c\n\nFind the top 5 beer markets in terms of the amount of total beer consumption.\n\n\nq1c = (\n       beer_mkt\n       .groupby('market')[['beer_floz']]\n       .sum()\n       .sort_values(by = 'beer_floz', ascending = False)\n       )\n\n# adding a ranking variable to the DataFrame\nq1c['ranking'] = q1c['beer_floz'].rank(method = 'dense', ascending = False)\nq1c = q1c.query('ranking <= 5')"
  },
  {
    "objectID": "DANL210_lab2a.html#q1d",
    "href": "DANL210_lab2a.html#q1d",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1d",
    "text": "Q1d\n\nProvide (1) seaborn code and (2) a simple comment to describe how the distribution of price_per_floz varies by brand.\n\n\nsns.displot(data = beer_mkt,\n            x = 'price_per_floz', bins = 200,\n            hue = 'brand', # for colorful histogram\n            row = 'brand')\n\n<seaborn.axisgrid.FacetGrid at 0x7ff096eee6d0>"
  },
  {
    "objectID": "DANL210_lab2a.html#q1e",
    "href": "DANL210_lab2a.html#q1e",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1e",
    "text": "Q1e\n\nProvide (1) seaborn code and (2) a simple comment to describe how the relationship between price_per_floz and beer_floz varies by brand.\n\n\nsns.lmplot(data = beer_mkt,\n           x = \"beer_floz\",\n           y = \"price_per_floz\",\n           scatter_kws = {'alpha' : 0.1},\n           hue = 'brand',\n           row = 'brand')\n\n<seaborn.axisgrid.FacetGrid at 0x7ff0947cb0d0>\n\n\n\n\n\n\n# adding log transformed variables\nbeer_mkt['log_beer_floz'] = np.log(beer_mkt['beer_floz'])\nbeer_mkt['log_price_per_floz'] = np.log(beer_mkt['price_per_floz'])\n\n\nsns.lmplot(data = beer_mkt,\n           x = \"log_beer_floz\",\n           y = \"log_price_per_floz\",\n           scatter_kws = {'alpha' : 0.1},\n           hue = 'brand',\n           row = 'brand')\n\n<seaborn.axisgrid.FacetGrid at 0x7ff09a4008e0>"
  },
  {
    "objectID": "mba-ch1-logit.html#logistic-regression-model",
    "href": "mba-ch1-logit.html#logistic-regression-model",
    "title": "Logistic Regression",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\nIn a regression model, we model the conditional mean for \\(y\\) given \\(x\\) as\n\n\\[\n\\begin{align}\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] &= \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}\\\\\n{ }\\\\\ny_{i} &=   \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1, i} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p, i} + \\epsilon_{i} \\quad \\text{for } i = 1, 2, ..., n\n\\end{align}\n\\] - Logistic regression is used to model a binary response:\n\n\\(y\\) is either 1 or 0 (e.g., True or False).\n\\(\\epsilon_{i}\\) is the random noise from logistic distribution whose cumulative density function is as follows:\n\n\nknitr::include_graphics('lec_figs/mba-1-11.png')\n\n\n\n\n\n\n\n\nwhere \\(z\\) is a linear combination of explanatory variables \\(\\mathbf{X}\\).\n\\[\nf(z) = \\frac{e^{z}}{1 + e^{z}}\n\\] - Since \\(y\\) is either 0 or 1, the conditional mean value of \\(y\\) is the probability:\n\\[\n\\begin{align}\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] &= \\text{Pr}(y = 1 | \\mathbf{X}) \\times 1 \\,+\\,\\text{Pr}(y = 0 | \\mathbf{X}) \\times 0\\\\ &= \\text{Pr}(y = 1 | \\mathbf{X}) \\\\\n{ }\\\\\n\\text{Pr}(y = 1 | \\mathbf{X}) &= f(\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p})\\\\\n&= \\frac{e^{\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}}}{1 + e^{\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}}} \\\\\n\\end{align}\n\\]\n\nThe logistic regression finds the beta coefficients, \\(b_{0}, b_{1}, \\cdots\\), such that the logistic function ranging from 0 to 1\n\n\\[\n\\begin{align}\nf( b_{0} + b_{1}*x_{i,1} + b_{2}*x_{i,2} + \\cdots )\\notag\n\\end{align}\n\\]\nis the best possible estimate of the binary outcome \\(y_{i}\\).\n\n\nInterpretation of Beta Estimates\n\nIn logistic regression, the effect of \\(x_{1}\\) on \\(Pr(y_{i} = 1 | \\mathbf{X})\\) is different across observations \\(i = 1, 2, \\cdots\\):\n\n\nknitr::include_graphics('lec_figs/effect-linear-logit.png')\n\n\n\n\n\n\n\n\n\n\n\nThe Goals of Logistic Regression:\n\nModeling for prediction (\\(\\text{Pr}({y} | \\mathbf{X})\\)): When we want to predict an outcome variable \\(y = 0 ,1\\) based on the information contained in a set of predictor variables \\(\\mathbf{X}\\).\n\n\nWe are estimating the conditional expectation (mean) for \\(y\\): \\[\n\\text{Pr}(\\, y \\,|\\, \\mathbf{X} \\,) = f(\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}).\n\\]\nwhich is the probability that \\(y = 1\\) given the value for \\(X\\) from the logistic function.\nPrediction from the logistic regression with a threshold on the probabilities can be used as a classifier.\n\nIf the probability that the newborn baby i is at risk is greater than the threshold \\(\\theta\\in (0, 1)\\) (\\(\\text{Pr}(y_{i} = 1 | \\mathbf{X}) > \\theta\\)), the baby i is classified as at-risk.\n\nWe can discuss the performance of classifiers later.\n\nAccuracy: When the classifier says this newborn baby is at risk or is not at risk, what is the probability that the model is correct?\nPrecision: If the classifier says this newborn baby is at risk, what’s the probability that the baby is really at risk?\nRecall: Of all the babies at risk, what fraction did the classifier detect?\nThere is a trade-off between recall and precision.\n\n\n\n\nModeling for explanation (\\(\\hat{\\beta}\\)): When we want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(\\mathbf{X}\\).\n\n\nWe can average the marginal effects across the training data (average marginal effect, or AME).\nWe can obtain the marginal effect at an average observation or representative observations in the training data (marginal effect at the mean or at representative values).\nWe can also consider:\nThe AME for subgroup of the data\nThe AME at the mean value of VARIABLE.\n\n\n\n\nExample: Newbron Babies at risk\n\nWe’ll use a sample dataset from the 2010 CDC natality public-use data.\n\nThe data set records information about all US births, including risk factors about the mother and father, and about the delivery.\nNewborn babies are assessed at one and five minutes after birth to determine if a baby needs immediate emergency care or extra medical attention.\n\n\nTask 1. Identify the effects of several risk factors on the probability of atRisk == TRUE. Task 2. Classify ahead of time babies with a higher probability of atRisk == TRUE.\n\n\nLoad Packages and Data\n\n# install.packages(\"margins\")\nlibrary(tidyverse)\nlibrary(margins) # for AME\nlibrary(hrbrthemes) # for ggplot theme, theme_ipsum()\nlibrary(stargazer)\n\ntheme_set(theme_ipsum()) # setting theme_ipsum() default\nload(url(\"https://bcdanl.github.io/data/NatalRiskData.rData\"))\n\n# 50:50 split between training and testing data\ntrain <- filter(sdata,\n                ORIGRANDGROUP <= 5)\ntest <- filter(sdata,\n                ORIGRANDGROUP > 5)\nskim(train)\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n14212\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nlogical\n9\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGESTREC3\n0\n1\nFALSE\n2\n>= : 12651, < 3: 1561\n\n\nDPLURAL\n0\n1\nFALSE\n3\nsin: 13761, twi: 424, tri: 27\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nCIG_REC\n0\n1\n0.09\nFAL: 12913, TRU: 1299\n\n\nULD_MECO\n0\n1\n0.05\nFAL: 13542, TRU: 670\n\n\nULD_PRECIP\n0\n1\n0.03\nFAL: 13854, TRU: 358\n\n\nULD_BREECH\n0\n1\n0.06\nFAL: 13316, TRU: 896\n\n\nURF_DIAB\n0\n1\n0.05\nFAL: 13450, TRU: 762\n\n\nURF_CHYPER\n0\n1\n0.01\nFAL: 14046, TRU: 166\n\n\nURF_PHYPER\n0\n1\n0.04\nFAL: 13595, TRU: 617\n\n\nURF_ECLAM\n0\n1\n0.00\nFAL: 14181, TRU: 31\n\n\natRisk\n0\n1\n0.02\nFAL: 13939, TRU: 273\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPWGT\n0\n1\n153.28\n38.87\n74\n125\n145\n172\n375\n▆▇▂▁▁\n\n\nUPREVIS\n0\n1\n11.17\n4.02\n0\n9\n11\n13\n49\n▃▇▁▁▁\n\n\nDBWT\n0\n1\n3276.02\n582.91\n265\n2985\n3317\n3632\n6165\n▁▁▇▂▁\n\n\nORIGRANDGROUP\n0\n1\n2.53\n1.70\n0\n1\n3\n4\n5\n▇▅▅▅▅\n\n\n\n\n\n\n\n\nLinear Probability Model (LPM)\n\n# linear probability model\nlpm <- lm(atRisk ~ CIG_REC + GESTREC3 + DPLURAL + \n               ULD_MECO + ULD_PRECIP + ULD_BREECH + \n               URF_DIAB + URF_CHYPER + URF_PHYPER + URF_ECLAM, \n             data = train)\n\n\nLPM often works well when it comes to identifying AME.\nCaveats\n\nProbability of \\(y = 1\\) can be beyond [0, 1].\nThe error can’t be distributed Normal with \\(y_{i} \\in \\{0 , 1\\}\\).\nWhen using LPM, we should make standard errors of beta estimates robust to heteroskedasticity—variances of errors are non-constant across observations.\n\n\n\n\n\nLogistic Regression via glm( family = binomial(link = \"logit\") )\nmodel <- glm(atRisk ~ PWGT + UPREVIS + CIG_REC + GESTREC3 + DPLURAL + \n               ULD_MECO + ULD_PRECIP + ULD_BREECH + \n               URF_DIAB + URF_CHYPER + URF_PHYPER + URF_ECLAM, \n             data = train, \n             family = binomial(link = \"logit\") )\nstargazer(model, type = 'html')\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\natRisk\n\n\n\n\n\n\n\n\nPWGT\n\n\n0.004**\n\n\n\n\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\nUPREVIS\n\n\n-0.063***\n\n\n\n\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\nCIG_REC\n\n\n0.313*\n\n\n\n\n\n\n(0.187)\n\n\n\n\n\n\n\n\n\n\nGESTREC3< 37 weeks\n\n\n1.545***\n\n\n\n\n\n\n(0.141)\n\n\n\n\n\n\n\n\n\n\nDPLURALtriplet or higher\n\n\n1.394***\n\n\n\n\n\n\n(0.499)\n\n\n\n\n\n\n\n\n\n\nDPLURALtwin\n\n\n0.312\n\n\n\n\n\n\n(0.241)\n\n\n\n\n\n\n\n\n\n\nULD_MECO\n\n\n0.818***\n\n\n\n\n\n\n(0.236)\n\n\n\n\n\n\n\n\n\n\nULD_PRECIP\n\n\n0.192\n\n\n\n\n\n\n(0.358)\n\n\n\n\n\n\n\n\n\n\nULD_BREECH\n\n\n0.749***\n\n\n\n\n\n\n(0.178)\n\n\n\n\n\n\n\n\n\n\nURF_DIAB\n\n\n-0.346\n\n\n\n\n\n\n(0.288)\n\n\n\n\n\n\n\n\n\n\nURF_CHYPER\n\n\n0.560\n\n\n\n\n\n\n(0.390)\n\n\n\n\n\n\n\n\n\n\nURF_PHYPER\n\n\n0.162\n\n\n\n\n\n\n(0.250)\n\n\n\n\n\n\n\n\n\n\nURF_ECLAM\n\n\n0.498\n\n\n\n\n\n\n(0.777)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-4.412***\n\n\n\n\n\n\n(0.289)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n14,212\n\n\n\n\nLog Likelihood\n\n\n-1,231.496\n\n\n\n\nAkaike Inf. Crit.\n\n\n2,490.992\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nsummary(model)\n\n\nCall:\nglm(formula = atRisk ~ PWGT + UPREVIS + CIG_REC + GESTREC3 + \n    DPLURAL + ULD_MECO + ULD_PRECIP + ULD_BREECH + URF_DIAB + \n    URF_CHYPER + URF_PHYPER + URF_ECLAM, family = binomial(link = \"logit\"), \n    data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.9732  -0.1818  -0.1511  -0.1358   3.2641  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -4.412189   0.289352 -15.249  < 2e-16 ***\nPWGT                      0.003762   0.001487   2.530 0.011417 *  \nUPREVIS                  -0.063289   0.015252  -4.150 3.33e-05 ***\nCIG_RECTRUE               0.313169   0.187230   1.673 0.094398 .  \nGESTREC3< 37 weeks        1.545183   0.140795  10.975  < 2e-16 ***\nDPLURALtriplet or higher  1.394193   0.498866   2.795 0.005194 ** \nDPLURALtwin               0.312319   0.241088   1.295 0.195163    \nULD_MECOTRUE              0.818426   0.235798   3.471 0.000519 ***\nULD_PRECIPTRUE            0.191720   0.357680   0.536 0.591951    \nULD_BREECHTRUE            0.749237   0.178129   4.206 2.60e-05 ***\nURF_DIABTRUE             -0.346467   0.287514  -1.205 0.228187    \nURF_CHYPERTRUE            0.560025   0.389678   1.437 0.150676    \nURF_PHYPERTRUE            0.161599   0.250003   0.646 0.518029    \nURF_ECLAMTRUE             0.498064   0.776948   0.641 0.521489    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2698.7  on 14211  degrees of freedom\nResidual deviance: 2463.0  on 14198  degrees of freedom\nAIC: 2491\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nLogistic regression finds the beta parameters that maximize the log likelihood of the data, given the model, which is equivalent to minimizing the sum of the residual deviance.\n\nWe want to make the likelihood as big as possible.\nWe want to make the deviance as small as possible.\n\n\n\n\n\nLikelihood Function\n\nLikelihood is the probability of our data given the model.\n\n\nknitr::include_graphics('lec_figs/logistic_likelihood.png')\n\n\n\n\n\n\n\n\n\nThe probability that the seven data points would be observed: \\(L = (1-P1)*(1-P2)* P3*(1-P4)*P5*P6*P7\\).\n\nThe log of the likelihood: \\(\\log(L) = \\log(1-P1) + \\log(1-P2) + \\log(P3) + \\log(1-P4) + \\log(P5) + \\log(P6) + \\log(P7)\\)\n\nIn statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data.\n\nThis is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable.\n\n\n\n\n\nDeviance\n\nDeviance measures to the distance between data and fit\nThe null deviance is similar to the variance of the data around the average rate of positive examples.\n\n\n# likelihood function for logistic regression\n## y: the outcome in numeric form, either 1 or 0\n## py: the predicted probability that y == 1\n\nloglikelihood <- function(y, py) {\n  sum( y * log(py) + (1-y)*log(1 - py) )\n}\n\n# the rate of positive example in the dataset\npnull <- mean( as.numeric(train$atRisk) )\n\n# the null deviance\nnull.dev <- -2 *loglikelihood(as.numeric(train$atRisk), pnull)\n\n# the null deviance from summary(model)\nmodel$null.deviance\n\n[1] 2698.716\n\n# the predicted probability for the training data\npred <- predict(model, newdata=train, type = \"response\")\n\n# the residual deviance\nresid.dev <- -2 * loglikelihood(as.numeric(train$atRisk), pred)\n\n# the residual deviance from summary(model)\nmodel$deviance\n\n[1] 2462.992\n\n\n\n\n\nAIC, AICc, BIC and pseudo R-squared\n\nThe AIC, or the Akaike information criterion, is the log likelihood adjusted for the number of coefficients.\n\nThe corrected AIC, AICc, is the AIC corrected by the sample size and the degree of freedom, which is superior to the AIC.\n\nThe BIC, or Bayesian information criterion, attempts to approximate the posterior probability that each model is best.\n\nThe BIC can be useful only when in small sample settings.\n\nThe pseudo R-squared is a goodness-of-fit measure of how much of the deviance is “explained” by the model.\n\n\n# the AIC\nAIC <- 2 * ( length( model$coefficients ) -\n             loglikelihood( as.numeric(train$atRisk), pred) )\nAIC\n\n[1] 2490.992\n\nmodel$aic\n\n[1] 2490.992\n\n# the pseudo R-squared\npseudo_r2 <- 1 - (resid.dev / null.dev)\n\n\nRegression preserves the probabilities:\n\n\ntrain$pred <- predict(model, newdata=train, type = \"response\")\ntest$pred <- predict(model, newdata=test, type=\"response\")\n\nsum(train$atRisk == TRUE)\n\n[1] 273\n\nsum(train$pred)\n\n[1] 273\n\npremature <- subset(train, GESTREC3 == \"< 37 weeks\")\nsum(premature$atRisk == TRUE)\n\n[1] 112\n\nsum(premature$pred)\n\n[1] 112\n\n\n\n\n\nAverage Marginal Effects\n\nm <- margins(model)\name_result <- summary(m)\name_result\n\n                   factor     AME     SE       z      p   lower   upper\n                  CIG_REC  0.0064 0.0043  1.5000 0.1336 -0.0020  0.0148\n DPLURALtriplet or higher  0.0484 0.0290  1.6677 0.0954 -0.0085  0.1052\n              DPLURALtwin  0.0064 0.0056  1.1480 0.2510 -0.0045  0.0173\n       GESTREC3< 37 weeks  0.0450 0.0062  7.2235 0.0000  0.0328  0.0571\n                     PWGT  0.0001 0.0000  2.5096 0.0121  0.0000  0.0001\n               ULD_BREECH  0.0181 0.0056  3.2510 0.0012  0.0072  0.0290\n                 ULD_MECO  0.0211 0.0082  2.5718 0.0101  0.0050  0.0371\n               ULD_PRECIP  0.0038 0.0077  0.4946 0.6209 -0.0113  0.0189\n                  UPREVIS -0.0012 0.0003 -4.0631 0.0000 -0.0017 -0.0006\n               URF_CHYPER  0.0131 0.0115  1.1437 0.2528 -0.0094  0.0356\n                 URF_DIAB -0.0055 0.0040 -1.3887 0.1649 -0.0133  0.0023\n                URF_ECLAM  0.0114 0.0220  0.5196 0.6033 -0.0317  0.0545\n               URF_PHYPER  0.0031 0.0052  0.6066 0.5441 -0.0070  0.0133\n\nggplot(data = ame_result) +\n  geom_point( aes(factor, AME) ) +\n  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper), \n                width = .5) +\n  geom_hline(yintercept = 0) +\n  coord_flip()"
  },
  {
    "objectID": "mba-ch1-logit.html#references",
    "href": "mba-ch1-logit.html#references",
    "title": "Logistic Regression",
    "section": "References",
    "text": "References\n\n\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\nPractical Data Science with R by Nina Zumel and John Mount."
  },
  {
    "objectID": "DANL210_lab3q.html",
    "href": "DANL210_lab3q.html",
    "title": "Python Lab 3 - Tidy Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab3q.html#load-dataframe",
    "href": "DANL210_lab3q.html#load-dataframe",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbillboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')\nny_pincp = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_wide.csv')\ncovid = pd.read_csv('https://bcdanl.github.io/data/covid19_cases.csv')"
  },
  {
    "objectID": "DANL210_lab3q.html#q1a",
    "href": "DANL210_lab3q.html#q1a",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1a",
    "text": "Q1a\n\nDescribe how the distribution of rating varies across week 1, week 2, and week 3 using the faceted histogram."
  },
  {
    "objectID": "DANL210_lab3q.html#q1b",
    "href": "DANL210_lab3q.html#q1b",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1b",
    "text": "Q1b\n\nWhich artist(s) have the most number of tracks in billboard DataFrame?"
  },
  {
    "objectID": "DANL210_lab3q.html#q1c",
    "href": "DANL210_lab3q.html#q1c",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1c",
    "text": "Q1c\n\nMake ny_pincp longer."
  },
  {
    "objectID": "DANL210_lab3q.html#q1d",
    "href": "DANL210_lab3q.html#q1d",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1d",
    "text": "Q1d\n\nMake a wide-form DataFrame of covid whose variable names are from countriesAndTerritories and values are from cases."
  },
  {
    "objectID": "DANL210_lab3q.html#q1e",
    "href": "DANL210_lab3q.html#q1e",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1e",
    "text": "Q1e\n\nUse the wide-form DataFrame of covid to find the top 10 countries for which their cases are highly correlated with USA’s cases using DataFrame.corr()"
  },
  {
    "objectID": "DANL210_hw1a.html",
    "href": "DANL210_hw1a.html",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files."
  },
  {
    "objectID": "DANL210_hw1a.html#q1a",
    "href": "DANL210_hw1a.html#q1a",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q1a",
    "text": "Q1a\n\nCalculate the simple difference between the probability of survival when passengers are first-class and the probability of survival when they are not.\n\n\n# Count the number of passengers in each class and return the count in descending order\ntitanic_1[['pclass']].value_counts()\n\n# Count the number of passengers who survived and who didn't and return the count in descending order\ntitanic_1[['survived']].value_counts()\n\n# Add a new column 'd' to the titanic_1 dataframe and set all values to 0\ntitanic_1['d'] = 0\n\n# For rows where the 'pclass' column is '1st class', set the value of the 'd' column to 1\ntitanic_1.loc[titanic_1['pclass']=='1st class', 'd'] = 1\n\n# Add a new column 'survived_d' to the titanic_1 DataFrame and set all values to 0\ntitanic_1['survived_d'] = 0\n\n# For rows where the 'survived' column is 'yes', set the value of the 'survived_d' column to 1\ntitanic_1.loc[titanic_1['survived']=='yes', 'survived_d'] = 1\n\n# Compute the mean of 'survived_d' for rows where 'd' is 0\ne_y0 = titanic_1.loc[titanic_1['d']==0, 'survived_d'].mean()\n\n# Compute the mean of 'survived_d' for rows where 'd' is 1\ne_y1 = titanic_1.loc[titanic_1['d']==1, 'survived_d'].mean()\n\n# Compute the SDP (so called treatment effect) by subtracting the mean of 'survived_d' where 'd' is 0 from the mean where 'd' is 1\nSDP = e_y1 - e_y0\n\n# Return the value of SDP\nSDP\n\n0.3152436786584735"
  },
  {
    "objectID": "DANL210_hw1a.html#q1b",
    "href": "DANL210_hw1a.html#q1b",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q1b",
    "text": "Q1b\n\nHow much does the probability of survival increase for first-class passengers relative to those who are not first-class passengers?\nSDP tells us what would happen to the probability of survival if non-first-class passengers were first-class.\n\nIn other words, SDP means the effect of being the first-class on the probability of survival from the Titanic Disaster."
  },
  {
    "objectID": "DANL210_hw1a.html#q1c",
    "href": "DANL210_hw1a.html#q1c",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q1c",
    "text": "Q1c\n\nConsider the probability of survival in titanic_2.csv.\n\n\ntitanic_2 = pd.read_csv('https://bcdanl.github.io/data/titanic_2.csv')\n\n\ntitanic_2.head()\n\n\n\n\n\n  \n    \n      \n      pclass\n      survived\n      sex\n      age\n    \n  \n  \n    \n      0\n      1st class\n      yes\n      female\n      29.0000\n    \n    \n      1\n      1st class\n      yes\n      male\n      0.9167\n    \n    \n      2\n      1st class\n      no\n      female\n      2.0000\n    \n    \n      3\n      1st class\n      no\n      male\n      30.0000\n    \n    \n      4\n      1st class\n      no\n      female\n      25.0000\n    \n  \n\n\n\n\n\ntitanic_2.describe()\n\n\n\n\n\n  \n    \n      \n      age\n    \n  \n  \n    \n      count\n      1046.000000\n    \n    \n      mean\n      29.881135\n    \n    \n      std\n      14.413500\n    \n    \n      min\n      0.166700\n    \n    \n      25%\n      21.000000\n    \n    \n      50%\n      28.000000\n    \n    \n      75%\n      39.000000\n    \n    \n      max\n      80.000000\n    \n  \n\n\n\n\n\nAfter stratifying on gender and age, what happens to the difference in the probabilities of survival between first-class passengers and non-first-class passengers.\nExplain in your own words what stratifying on gender and age did for this difference in probabilities of survival between first-class passengers and non-first-class passengers.\n\n\n# Get count of passengers by pclass\ntitanic_2[['pclass']].value_counts()\n\n# Get count of passengers who survived or not\ntitanic_2[['survived']].value_counts()\n\n# Get count of passengers by gender\ntitanic_2[['sex']].value_counts()\n\n# Get count of passengers by age\ntitanic_2[['age']].value_counts()\n\nage    \n24.0000    47\n22.0000    43\n21.0000    41\n30.0000    40\n18.0000    39\n           ..\n20.5000     1\n11.5000     1\n0.6667      1\n0.4167      1\n80.0000     1\nLength: 98, dtype: int64\n\n\n\ntitanic_2.groupby('sex').describe()\n\n\n\n\n\n  \n    \n      \n      age\n    \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      sex\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      female\n      388.0\n      28.687071\n      14.576995\n      0.1667\n      19.0\n      27.0\n      38.0\n      76.0\n    \n    \n      male\n      658.0\n      30.585233\n      14.280571\n      0.3333\n      21.0\n      28.0\n      39.0\n      80.0\n    \n  \n\n\n\n\n\n# Create a new column 'd' and set its value to 0 for all rows\ntitanic_2['d'] = 0\n\n# Set the value of column 'd' to 1 for rows where pclass is '1st class'\ntitanic_2.loc[titanic_2['pclass']=='1st class', 'd'] = 1\n\n# Create a new column 'survived_d' and set its value to 0 for all rows\ntitanic_2['survived_d'] = 0\n\n# Set the value of column 'survived_d' to 1 for rows where survived is 'yes'\ntitanic_2.loc[titanic_2['survived']=='yes', 'survived_d'] = 1\n\n# Create a new column 'sex_d' and set its value to 0 for all rows\ntitanic_2['sex_d'] = 0\n\n# Set the value of column 'sex_d' to 1 for rows where sex is 'male'\ntitanic_2.loc[titanic_2['sex']=='male', 'sex_d'] = 1\n\n# Create a new column 'AgeGroup' and set its value to 0 for all rows\ntitanic_2['AgeGroup'] = 0\n\n# Set the value of column 'AgeGroup' to 1 for rows where age is greater than or equal to 18\ntitanic_2.loc[titanic_2['age'] >= 18, 'AgeGroup'] = 1\n\n\ntitanic_2.groupby('AgeGroup').describe()\n\n\n\n\n\n  \n    \n      \n      age\n      d\n      ...\n      survived_d\n      sex_d\n    \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n      count\n      mean\n      ...\n      75%\n      max\n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      AgeGroup\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      154.0\n      9.101732\n      6.015429\n      0.1667\n      3.0\n      9.0\n      15.75\n      17.0\n      418.0\n      0.129187\n      ...\n      1.0\n      1.0\n      418.0\n      0.638756\n      0.480937\n      0.0\n      0.0\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      892.0\n      33.468610\n      12.244544\n      18.0000\n      24.0\n      30.0\n      41.00\n      80.0\n      892.0\n      0.301570\n      ...\n      1.0\n      1.0\n      892.0\n      0.645740\n      0.478557\n      0.0\n      0.0\n      1.0\n      1.0\n      1.0\n    \n  \n\n2 rows × 32 columns\n\n\n\n\ntitanic_2.groupby(['sex', 'AgeGroup']).describe()\n\n\n\n\n\n  \n    \n      \n      \n      age\n      d\n      ...\n      survived_d\n      sex_d\n    \n    \n      \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n      count\n      mean\n      ...\n      75%\n      max\n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      sex\n      AgeGroup\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      female\n      0\n      72.0\n      9.015047\n      5.974978\n      0.1667\n      3.00\n      9.0\n      15.0\n      17.0\n      150.0\n      0.126667\n      ...\n      1.0\n      1.0\n      150.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      316.0\n      33.169304\n      12.016747\n      18.0000\n      23.75\n      30.0\n      40.0\n      76.0\n      316.0\n      0.395570\n      ...\n      1.0\n      1.0\n      316.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      male\n      0\n      82.0\n      9.177845\n      6.086437\n      0.3333\n      3.00\n      9.0\n      16.0\n      17.0\n      267.0\n      0.131086\n      ...\n      0.0\n      1.0\n      267.0\n      1.0\n      0.0\n      1.0\n      1.0\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      576.0\n      33.632812\n      12.375016\n      18.0000\n      24.00\n      30.0\n      41.0\n      80.0\n      576.0\n      0.250000\n      ...\n      0.0\n      1.0\n      576.0\n      1.0\n      0.0\n      1.0\n      1.0\n      1.0\n      1.0\n      1.0\n    \n  \n\n4 rows × 32 columns\n\n\n\n\n# Create a new column 's' and set its value to 0 for all rows\ntitanic_2['s'] = 0 \n\n# Set the value of column 's' based on gender and age group\ntitanic_2.loc[(titanic_2.sex_d == 0) & (titanic_2.AgeGroup == 1), 's'] = 1\ntitanic_2.loc[(titanic_2.sex_d == 0) & (titanic_2.AgeGroup == 0), 's'] = 2\ntitanic_2.loc[(titanic_2.sex_d == 1) & (titanic_2.AgeGroup == 1), 's'] = 3\ntitanic_2.loc[(titanic_2.sex_d == 1) & (titanic_2.AgeGroup == 0), 's'] = 4\n\n# Get the number of observations where d is 0\nobs = titanic_2.loc[titanic_2.d == 0].shape[0]\n\n# Define a function to calculate weighted average effect\ndef weighted_avg_effect(df):\n  \n  # Calculate the difference in survival rates between the treatment and control groups\n    diff = ( df[ df.d == 1 ].survived_d.mean() - \n             df[ df.d == 0 ].survived_d.mean() )\n  \n  # Calculate the weight assigned to the treatment group\n    weight = df[ df.d == 0 ].shape[0] / obs\n    \n  # Calculate the weighted average effect\n    return diff * weight\n\n\n# Apply the weighted_avg_effect function to each group in the data frame grouped by the s variable\nSDP2 = titanic_2.groupby('s').apply( weighted_avg_effect )\n\n# Calculate the weighted average effect of treatment for the entire population\nWSDP = SDP2.sum()\n\nWSDP\n\n0.24695099450754543\n\n\n\nThe probability of survival for the first-class passengers can be different across gender and age groups.\n\nIn other words, the effect of being the first-class on the probability of survival from the Titanic Disaster can be different across genders and age groups.\n\nWSDP takes into account the difference in the effect of being first-class across gender and age groups by weighting.\n\n\nWSDP - SDP\n\n-0.0682926841509281\n\n\n\nThe probability of survival for first-class after taking into account gender and age (WSDP) is less than that (SDP) which does presumably assume that characteristics of passengers such as gender and ages is related with the probability of survival."
  },
  {
    "objectID": "DANL210_hw1a.html#q2a",
    "href": "DANL210_hw1a.html#q2a",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2a",
    "text": "Q2a\n\nHow many players have been recorded?\n\n\n# the number of unique players\nq2a = nhl1617['id_player'].nunique()  \n\nq2a\n\n888"
  },
  {
    "objectID": "DANL210_hw1a.html#q2b.",
    "href": "DANL210_hw1a.html#q2b.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2b.",
    "text": "Q2b.\n\nA column points (“P”) is missing in the data. The number of points of a player is defined as the sum of his goals (“G”) and assists (“A”).\nAdd the point column “P” to your DataFrame.\n\n\n# create a new column called 'P' in the nhl1617 dataframe that is the sum of the 'G' and 'A' columns\nq2b = nhl1617.assign(P = nhl1617['G'] + nhl1617['A'])\n\nq2b\n\n\n\n\n\n  \n    \n      \n      id_player\n      Born\n      City\n      Cntry\n      Nat\n      Ht\n      Wt\n      Last_Name\n      First_Name\n      Position\n      Team\n      GP\n      G\n      A\n      TOI\n      TOI_GP\n      P\n    \n  \n  \n    \n      0\n      1\n      30.04.1988\n      Hamilton\n      CAN\n      CAN\n      69\n      170\n      Abbott\n      Spencer\n      LW\n      CHI\n      1\n      0\n      0\n      514\n      8.57\n      0\n    \n    \n      1\n      2\n      25.02.1987\n      Muskegon\n      USA\n      USA\n      74\n      218\n      Abdelkader\n      Justin\n      LW/RW\n      DET\n      64\n      7\n      14\n      63969\n      16.65\n      21\n    \n    \n      2\n      3\n      23.09.1993\n      Stockholm\n      SWE\n      SWE\n      71\n      196\n      Aberg\n      Pontus\n      LW\n      NSH\n      15\n      1\n      1\n      11102\n      12.33\n      2\n    \n    \n      3\n      4\n      01.12.1991\n      Johnston\n      USA\n      USA\n      70\n      208\n      Acciari\n      Noel\n      C\n      BOS\n      29\n      2\n      3\n      18047\n      10.23\n      5\n    \n    \n      4\n      5\n      30.04.1992\n      Morristown\n      USA\n      USA\n      72\n      202\n      Agostino\n      Kenny\n      LW\n      STL\n      7\n      1\n      2\n      5366\n      12.78\n      3\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      883\n      884\n      18.04.1993\n      Huddinge\n      SWE\n      SWE\n      74\n      215\n      Zibanejad\n      Mika\n      C/RW\n      NYR\n      56\n      14\n      23\n      57362\n      17.07\n      37\n    \n    \n      884\n      885\n      01.09.1987\n      Toronto\n      CAN\n      CAN\n      71\n      180\n      Zolnierczyk\n      Harry\n      LW\n      NSH\n      24\n      2\n      2\n      12776\n      8.87\n      4\n    \n    \n      885\n      886\n      01.09.1987\n      Oslo\n      NOR\n      NOR\n      67\n      179\n      Zuccarello\n      Mats\n      RW/C/LW\n      NYR\n      80\n      15\n      44\n      90378\n      18.83\n      59\n    \n    \n      886\n      887\n      16.01.1992\n      Newport Beach\n      USA\n      USA\n      71\n      187\n      Zucker\n      Jason\n      LW/RW\n      MIN\n      79\n      22\n      25\n      72455\n      15.28\n      47\n    \n    \n      887\n      888\n      15.05.1995\n      St. Petersburg\n      RUS\n      RUS\n      73\n      224\n      Zykov\n      Valentin\n      LW\n      CAR\n      2\n      1\n      0\n      750\n      6.25\n      1\n    \n  \n\n888 rows × 17 columns"
  },
  {
    "objectID": "DANL210_hw1a.html#q2c.",
    "href": "DANL210_hw1a.html#q2c.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2c.",
    "text": "Q2c.\n\nWho is the top scorer in terms of points?\n\n\n# Sort the DataFrame q2b by the column 'P' in descending order\nq2c = q2b.sort_values(by= 'P', ascending=False)\n\nq2c\n\n\n\n\n\n  \n    \n      \n      id_player\n      Born\n      City\n      Cntry\n      Nat\n      Ht\n      Wt\n      Last_Name\n      First_Name\n      Position\n      Team\n      GP\n      G\n      A\n      TOI\n      TOI_GP\n      P\n    \n  \n  \n    \n      509\n      510\n      13.01.1997\n      Richmond Hill\n      CAN\n      CAN\n      73\n      200\n      McDavid\n      Connor\n      C\n      EDM\n      82\n      30\n      70\n      103967\n      21.13\n      100\n    \n    \n      149\n      150\n      07.08.1987\n      Cole Harbour\n      CAN\n      CAN\n      71\n      200\n      Crosby\n      Sidney\n      C\n      PIT\n      75\n      44\n      45\n      89450\n      19.88\n      89\n    \n    \n      389\n      390\n      19.11.1988\n      Buffalo\n      USA\n      USA\n      71\n      177\n      Kane\n      Patrick\n      RW/C\n      CHI\n      82\n      34\n      55\n      105263\n      21.40\n      89\n    \n    \n      21\n      22\n      23.11.1987\n      Gävle\n      SWE\n      SWE\n      73\n      213\n      Backstrom\n      Nicklas\n      C\n      WSH\n      82\n      23\n      63\n      89839\n      18.27\n      86\n    \n    \n      426\n      427\n      17.06.1993\n      Maykop\n      RUS\n      RUS\n      71\n      178\n      Kucherov\n      Nikita\n      RW\n      T.B\n      74\n      40\n      45\n      86320\n      19.43\n      85\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      512\n      513\n      22.05.1994\n      Hamilton\n      CAN\n      CAN\n      74\n      203\n      McEneny\n      Evan\n      D\n      VAN\n      1\n      0\n      0\n      908\n      15.13\n      0\n    \n    \n      520\n      521\n      22.02.1993\n      Langley\n      CAN\n      CAN\n      74\n      214\n      McNeill\n      Mark\n      C/RW\n      DAL\n      1\n      0\n      0\n      829\n      13.82\n      0\n    \n    \n      781\n      782\n      22.04.1994\n      Saskatoon\n      CAN\n      CAN\n      72\n      204\n      Stephenson\n      Chandler\n      C\n      WSH\n      4\n      0\n      0\n      2131\n      8.88\n      0\n    \n    \n      523\n      524\n      10.12.1992\n      Plantation\n      USA\n      USA\n      78\n      225\n      Megna\n      Jaycob\n      D\n      ANA\n      1\n      0\n      0\n      920\n      15.33\n      0\n    \n    \n      0\n      1\n      30.04.1988\n      Hamilton\n      CAN\n      CAN\n      69\n      170\n      Abbott\n      Spencer\n      LW\n      CHI\n      1\n      0\n      0\n      514\n      8.57\n      0\n    \n  \n\n888 rows × 17 columns"
  },
  {
    "objectID": "DANL210_hw1a.html#q2d.",
    "href": "DANL210_hw1a.html#q2d.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2d.",
    "text": "Q2d.\n\nHow many Russian (non-goalie) players had some ice time in there 2016/2017 regular season?\nHint: Nationality of a player can be found in “Nat”. Russians are indicated by “RUS”.\n\n\n# the number of rows in the 'nhl1617' dataframe where the value in the 'Nat' column is equal to 'RUS', and returns the count as output.\nq2d = len( nhl1617[nhl1617['Nat'] == 'RUS'] )\n\nq2d\n\n38"
  },
  {
    "objectID": "DANL210_hw1a.html#q2e.",
    "href": "DANL210_hw1a.html#q2e.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2e.",
    "text": "Q2e.\n\nWhat are their names?\n\n\n# Select rows where the 'Nat' column equals 'RUS', then select only the 'Last_Name' and 'First_Name' columns\nq2e = nhl1617[ nhl1617['Nat'] == 'RUS' ][ ['Last_Name', 'First_Name'] ]\n\nq2e\n\n\n\n\n\n  \n    \n      \n      Last_Name\n      First_Name\n    \n  \n  \n    \n      11\n      Anisimov\n      Artem\n    \n    \n      27\n      Barbashev\n      Ivan\n    \n    \n      87\n      Buchnevich\n      Pavel\n    \n    \n      90\n      Burmistrov\n      Alex\n    \n    \n      207\n      Emelin\n      Alexei\n    \n    \n      272\n      Goldobin\n      Nikolay\n    \n    \n      293\n      Grigorenko\n      Mikhail\n    \n    \n      303\n      Gurianov\n      Denis\n    \n    \n      385\n      Kalinin\n      Sergey\n    \n    \n      386\n      Kamenev\n      Vladislav\n    \n    \n      426\n      Kucherov\n      Nikita\n    \n    \n      429\n      Kulemin\n      Nikolay\n    \n    \n      430\n      Kulikov\n      Dmitry\n    \n    \n      433\n      Kuznetsov\n      Evgeny\n    \n    \n      473\n      Lyubimov\n      Roman\n    \n    \n      480\n      Malkin\n      Evgeni\n    \n    \n      486\n      Marchenko\n      Alexey\n    \n    \n      489\n      Markov\n      Andrei\n    \n    \n      557\n      Namestnikov\n      Vladislav\n    \n    \n      566\n      Nesterov\n      Nikita\n    \n    \n      595\n      Orlov\n      Dmitry\n    \n    \n      600\n      Ovechkin\n      Alex\n    \n    \n      607\n      Panarin\n      Artemi\n    \n    \n      646\n      Provorov\n      Ivan\n    \n    \n      657\n      Radulov\n      Alex\n    \n    \n      707\n      Scherbak\n      Nikita\n    \n    \n      724\n      Sergachev\n      Mikhail\n    \n    \n      749\n      Slepyshev\n      Anton\n    \n    \n      763\n      Soshnikov\n      Nikita\n    \n    \n      799\n      Svechnikov\n      Evgeny\n    \n    \n      802\n      Tarasenko\n      Vladimir\n    \n    \n      819\n      Tolchinsky\n      Sergey\n    \n    \n      824\n      Tryamkin\n      Nikita\n    \n    \n      828\n      Tyutin\n      Fedor\n    \n    \n      875\n      Yakupov\n      Nail\n    \n    \n      878\n      Zadorov\n      Nikita\n    \n    \n      879\n      Zaitsev\n      Nikita\n    \n    \n      887\n      Zykov\n      Valentin"
  },
  {
    "objectID": "DANL210_hw1a.html#q2f.",
    "href": "DANL210_hw1a.html#q2f.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2f.",
    "text": "Q2f.\n\nWho performed best among the Russian players in terms of points (“P”)?\n\n\n# Select only the rows where 'Nat' column is 'RUS'\n# Then sort these rows in descending order based on the values in the 'P' column\nq2f = q2b[q2b['Nat'] == 'RUS'].sort_values(by='P', ascending = False)\n\nq2f\n\n\n\n\n\n  \n    \n      \n      id_player\n      Born\n      City\n      Cntry\n      Nat\n      Ht\n      Wt\n      Last_Name\n      First_Name\n      Position\n      Team\n      GP\n      G\n      A\n      TOI\n      TOI_GP\n      P\n    \n  \n  \n    \n      426\n      427\n      17.06.1993\n      Maykop\n      RUS\n      RUS\n      71\n      178\n      Kucherov\n      Nikita\n      RW\n      T.B\n      74\n      40\n      45\n      86320\n      19.43\n      85\n    \n    \n      802\n      803\n      13.12.1991\n      Yaroslavl\n      RUS\n      RUS\n      72\n      219\n      Tarasenko\n      Vladimir\n      RW\n      STL\n      82\n      39\n      36\n      90872\n      18.47\n      75\n    \n    \n      607\n      608\n      30.10.1991\n      Korkino\n      RUS\n      RUS\n      71\n      170\n      Panarin\n      Artemi\n      LW/C\n      CHI\n      82\n      31\n      43\n      95798\n      19.47\n      74\n    \n    \n      480\n      481\n      31.07.1986\n      Magnitogorsk\n      RUS\n      RUS\n      75\n      195\n      Malkin\n      Evgeni\n      C/RW\n      PIT\n      62\n      33\n      39\n      69263\n      18.62\n      72\n    \n    \n      600\n      601\n      17.09.1985\n      Moscow\n      RUS\n      RUS\n      75\n      239\n      Ovechkin\n      Alex\n      LW/RW\n      WSH\n      82\n      33\n      36\n      90361\n      18.37\n      69\n    \n    \n      433\n      434\n      19.05.1992\n      Chelyabinsk\n      RUS\n      RUS\n      74\n      192\n      Kuznetsov\n      Evgeny\n      C/LW\n      WSH\n      82\n      19\n      40\n      83410\n      16.95\n      59\n    \n    \n      657\n      658\n      05.07.1986\n      Nizhny Tagil\n      RUS\n      RUS\n      74\n      205\n      Radulov\n      Alex\n      RW\n      MTL\n      76\n      18\n      36\n      83405\n      18.30\n      54\n    \n    \n      11\n      12\n      24.05.1988\n      Yaroslavl\n      RUS\n      RUS\n      76\n      198\n      Anisimov\n      Artem\n      C/LW\n      CHI\n      64\n      22\n      23\n      68529\n      17.85\n      45\n    \n    \n      489\n      490\n      20.12.1978\n      Voskresensk\n      RUS\n      RUS\n      72\n      200\n      Markov\n      Andrei\n      D\n      MTL\n      62\n      6\n      30\n      81230\n      21.83\n      36\n    \n    \n      879\n      880\n      29.10.1991\n      Moscow\n      RUS\n      RUS\n      74\n      195\n      Zaitsev\n      Nikita\n      D\n      TOR\n      82\n      4\n      32\n      108332\n      21.85\n      36\n    \n    \n      595\n      596\n      23.07.1991\n      Novokuznetsk\n      RUS\n      RUS\n      71\n      212\n      Orlov\n      Dmitry\n      D\n      WSH\n      82\n      6\n      27\n      96107\n      19.53\n      33\n    \n    \n      646\n      647\n      13.01.1997\n      Yaroslavl\n      RUS\n      RUS\n      73\n      201\n      Provorov\n      Ivan\n      D\n      PHI\n      82\n      6\n      24\n      108132\n      21.98\n      30\n    \n    \n      557\n      558\n      22.11.1992\n      Zhukovskiy\n      RUS\n      RUS\n      71\n      180\n      Namestnikov\n      Vladislav\n      C/LW\n      T.B\n      74\n      10\n      18\n      65645\n      14.78\n      28\n    \n    \n      293\n      294\n      16.05.1994\n      Khabarovsk\n      RUS\n      RUS\n      75\n      209\n      Grigorenko\n      Mikhail\n      C\n      COL\n      75\n      10\n      13\n      63401\n      14.08\n      23\n    \n    \n      429\n      430\n      14.07.1986\n      Magnitogorsk\n      RUS\n      RUS\n      73\n      225\n      Kulemin\n      Nikolay\n      LW/RW\n      NYI\n      72\n      12\n      11\n      59691\n      13.82\n      23\n    \n    \n      87\n      88\n      17.04.1995\n      Cherepovets\n      RUS\n      RUS\n      74\n      193\n      Buchnevich\n      Pavel\n      RW/LW\n      NYR\n      41\n      8\n      12\n      32616\n      13.25\n      20\n    \n    \n      566\n      567\n      28.03.1993\n      Chelyabinsk\n      RUS\n      RUS\n      71\n      191\n      Nesterov\n      Nikita\n      D\n      MTL/T.B\n      48\n      4\n      13\n      46901\n      16.28\n      17\n    \n    \n      90\n      91\n      21.10.1991\n      Kazan\n      RUS\n      RUS\n      73\n      180\n      Burmistrov\n      Alex\n      C/RW\n      ARI/WPG\n      49\n      5\n      11\n      39266\n      13.35\n      16\n    \n    \n      828\n      829\n      19.07.1983\n      Izhevsk\n      RUS\n      RUS\n      74\n      221\n      Tyutin\n      Fedor\n      D\n      COL\n      69\n      1\n      12\n      78405\n      18.93\n      13\n    \n    \n      27\n      28\n      14.12.1995\n      Moscow\n      RUS\n      RUS\n      72\n      180\n      Barbashev\n      Ivan\n      C\n      STL\n      30\n      5\n      7\n      21224\n      11.78\n      12\n    \n    \n      749\n      750\n      13.05.1994\n      Penza\n      RUS\n      RUS\n      74\n      218\n      Slepyshev\n      Anton\n      LW\n      EDM\n      41\n      4\n      6\n      27342\n      11.12\n      10\n    \n    \n      878\n      879\n      16.04.1995\n      Moscow\n      RUS\n      RUS\n      77\n      230\n      Zadorov\n      Nikita\n      D\n      COL\n      56\n      0\n      10\n      63960\n      19.03\n      10\n    \n    \n      207\n      208\n      25.04.1986\n      Togliatti\n      RUS\n      RUS\n      74\n      218\n      Emelin\n      Alexei\n      D\n      MTL\n      76\n      2\n      8\n      97227\n      21.32\n      10\n    \n    \n      763\n      764\n      14.10.1993\n      Nizhny Tagil\n      RUS\n      RUS\n      71\n      190\n      Soshnikov\n      Nikita\n      RW\n      TOR\n      56\n      5\n      4\n      36450\n      10.70\n      9\n    \n    \n      875\n      876\n      06.10.1993\n      Nizhnekamsk\n      RUS\n      RUS\n      71\n      195\n      Yakupov\n      Nail\n      RW/LW\n      STL\n      40\n      3\n      6\n      25553\n      10.67\n      9\n    \n    \n      824\n      825\n      30.08.1994\n      Yekaterinburg\n      RUS\n      RUS\n      79\n      265\n      Tryamkin\n      Nikita\n      D\n      VAN\n      66\n      2\n      7\n      66291\n      16.73\n      9\n    \n    \n      486\n      487\n      02.01.1992\n      Moscow\n      RUS\n      RUS\n      75\n      210\n      Marchenko\n      Alexey\n      D\n      DET/TOR\n      41\n      1\n      7\n      41700\n      16.95\n      8\n    \n    \n      473\n      474\n      06.01.1992\n      Tver\n      RUS\n      RUS\n      74\n      207\n      Lyubimov\n      Roman\n      LW\n      PHI\n      47\n      4\n      2\n      27011\n      9.58\n      6\n    \n    \n      430\n      431\n      29.10.1990\n      Lipetsk\n      RUS\n      RUS\n      73\n      204\n      Kulikov\n      Dmitry\n      D\n      BUF\n      47\n      2\n      3\n      61766\n      21.90\n      5\n    \n    \n      385\n      386\n      17.03.1991\n      Omsk\n      RUS\n      RUS\n      75\n      200\n      Kalinin\n      Sergey\n      C/RW\n      N.J\n      43\n      2\n      2\n      33008\n      12.63\n      4\n    \n    \n      272\n      273\n      07.10.1995\n      Moscow\n      RUS\n      RUS\n      71\n      185\n      Goldobin\n      Nikolay\n      RW/LW\n      S.J/VAN\n      14\n      3\n      0\n      9551\n      11.37\n      3\n    \n    \n      707\n      708\n      30.12.1995\n      Moscow\n      RUS\n      RUS\n      74\n      190\n      Scherbak\n      Nikita\n      RW\n      MTL\n      3\n      1\n      0\n      2048\n      11.38\n      1\n    \n    \n      819\n      820\n      03.02.1995\n      Moscow\n      RUS\n      RUS\n      68\n      170\n      Tolchinsky\n      Sergey\n      LW\n      CAR\n      2\n      0\n      1\n      1377\n      11.48\n      1\n    \n    \n      887\n      888\n      15.05.1995\n      St. Petersburg\n      RUS\n      RUS\n      73\n      224\n      Zykov\n      Valentin\n      LW\n      CAR\n      2\n      1\n      0\n      750\n      6.25\n      1\n    \n    \n      724\n      725\n      25.06.1998\n      Nizhnekamsk\n      RUS\n      RUS\n      75\n      215\n      Sergachev\n      Mikhail\n      D\n      MTL\n      4\n      0\n      0\n      2910\n      12.13\n      0\n    \n    \n      799\n      800\n      31.10.1996\n      Yuzhno-Sakhalinsk\n      RUS\n      RUS\n      75\n      205\n      Svechnikov\n      Evgeny\n      RW/LW\n      DET\n      2\n      0\n      0\n      1577\n      13.15\n      0\n    \n    \n      303\n      304\n      07.06.1997\n      Togliatti\n      RUS\n      RUS\n      75\n      200\n      Gurianov\n      Denis\n      RW\n      DAL\n      1\n      0\n      0\n      786\n      13.10\n      0\n    \n    \n      386\n      387\n      12.08.1996\n      Orsk\n      RUS\n      RUS\n      74\n      194\n      Kamenev\n      Vladislav\n      LW/C\n      NSH\n      2\n      0\n      0\n      1207\n      10.07\n      0"
  },
  {
    "objectID": "DANL210_hw1a.html#q2g.",
    "href": "DANL210_hw1a.html#q2g.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2g.",
    "text": "Q2g.\n\nHow many points (“P”) did he have?\n\n\nq2g = (\n  q2b[q2b['Nat'] == 'RUS'] \n  .sort_values(by='P', ascending=False)\n  .head(1)[['Last_Name', 'First_Name', 'P']]\n  )\n  \nq2g\n\n\n\n\n\n  \n    \n      \n      Last_Name\n      First_Name\n      P\n    \n  \n  \n    \n      426\n      Kucherov\n      Nikita\n      85\n    \n  \n\n\n\n\n\nThe above code creates a new DataFrame called q2g, which:\n\nFilters q2b for players whose nationality is ‘RUS’\nSorts the filtered DataFrame by the ‘P’ column in descending order\nSelects the top row of the sorted DataFrame using the .head(1) method\nFilters the columns ‘Last_Name’, ‘First_Name’, and ‘P’ from the selected row"
  },
  {
    "objectID": "DANL210_hw1a.html#q2h.",
    "href": "DANL210_hw1a.html#q2h.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2h.",
    "text": "Q2h.\n\nHow well did he perform in the entire league? Put differently, what was his rank in terms of points?\n\n\nq2h = (\n     q2b.assign(ranking = q2b['P'].rank(method = 'min', ascending=False))  # Create a new column 'ranking' based on the values in column 'P'\n        .sort_values(by='P', ascending=False)  # Sort the DataFrame by 'P' column in descending order\n        .loc[q2b['Nat'] == 'RUS']  # Filter the rows where the 'Nat' column is 'RUS'\n        .head(1)[['ranking', 'Last_Name', 'First_Name', 'P']]  # Select the top row and specific columns\n        )\n\nq2h\n\n\n\n\n\n  \n    \n      \n      ranking\n      Last_Name\n      First_Name\n      P\n    \n  \n  \n    \n      426\n      5.0\n      Kucherov\n      Nikita\n      85"
  },
  {
    "objectID": "DANL210_hw1a.html#q2i.",
    "href": "DANL210_hw1a.html#q2i.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2i.",
    "text": "Q2i.\n\nFind the top ten scorers (in terms of points) and print them including their number of point and their respective team.\n\n\nq2i = ( q2b.assign( ranking = q2b['P'].rank(ascending=False) )   # add a new column 'ranking' based on the 'P' column's rank\n           .sort_values(by='P', ascending=False)                  # sort by 'P' column in descending order\n           )\n\n( q2i.loc[q2i['ranking'] <= 10]                                    # select rows where 'ranking' is less than or equal to 10\n            [['ranking', 'Last_Name', 'First_Name', 'P', 'Team']]  # select specific columns\n            )\n\n\n\n\n\n  \n    \n      \n      ranking\n      Last_Name\n      First_Name\n      P\n      Team\n    \n  \n  \n    \n      509\n      1.0\n      McDavid\n      Connor\n      100\n      EDM\n    \n    \n      149\n      2.5\n      Crosby\n      Sidney\n      89\n      PIT\n    \n    \n      389\n      2.5\n      Kane\n      Patrick\n      89\n      CHI\n    \n    \n      21\n      4.0\n      Backstrom\n      Nicklas\n      86\n      WSH\n    \n    \n      426\n      5.5\n      Kucherov\n      Nikita\n      85\n      T.B\n    \n    \n      485\n      5.5\n      Marchand\n      Brad\n      85\n      BOS\n    \n    \n      704\n      7.0\n      Scheifele\n      Mark\n      82\n      WPG\n    \n    \n      184\n      8.0\n      Draisaitl\n      Leon\n      77\n      EDM\n    \n    \n      91\n      9.0\n      Burns\n      Brent\n      76\n      S.J\n    \n    \n      802\n      10.0\n      Tarasenko\n      Vladimir\n      75\n      STL"
  },
  {
    "objectID": "DANL210_hw1a.html#q2j.",
    "href": "DANL210_hw1a.html#q2j.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2j.",
    "text": "Q2j.\n\nWhat are the three countries with the most players originating from?\n\n\n# Create a dataframe of count of players by nationality\nq2j = q2b[['Nat']].value_counts().reset_index()\n\n# Rename the columns of the dataframe\nq2j.columns = ['Nat', 'counts']\n\n# Create a new column 'ranking' with the ranking of each nationality by count\nq2j = ( q2j.assign( ranking = q2j['counts']\n                             .rank(method = 'dense', ascending=False) )\n           \n           # Filter the dataframe to only include the top 3 nationalities by count\n           .query('ranking <= 3' ) \n      )"
  },
  {
    "objectID": "DANL210_hw1a.html#q3a.",
    "href": "DANL210_hw1a.html#q3a.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q3a.",
    "text": "Q3a.\n\nFor each type of mine, calculate the total coal production for each pair of state-year.\n\n\n# Create a new column 'production' which is the sum of two other columns\ncoal['production'] = coal['production_underground'] + coal['production_surface']\n\n# Group the data by 'state' and 'year' columns and calculate the sum of 'production', 'production_underground', and 'production_surface'\nq3a = (\n       coal[ ['state', 'year', 'production',\n             'production_underground',\n             'production_surface'] ]\n       .groupby(['state', 'year'])\n       .sum()\n       )\n\nq3a\n\n\n\n\n\n  \n    \n      \n      \n      production\n      production_underground\n      production_surface\n    \n    \n      state\n      year\n      \n      \n      \n    \n  \n  \n    \n      Alabama\n      2011\n      19071\n      10878\n      8193\n    \n    \n      2012\n      19320\n      12569\n      6751\n    \n    \n      2013\n      18620\n      13515\n      5105\n    \n    \n      2014\n      16363\n      12517\n      3846\n    \n    \n      2015\n      13191\n      9897\n      3294\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Wyoming\n      2014\n      395665\n      3370\n      392295\n    \n    \n      2015\n      375773\n      3090\n      372683\n    \n    \n      2016\n      297218\n      1167\n      296051\n    \n    \n      2017\n      316454\n      1716\n      314738\n    \n    \n      2018\n      304188\n      2210\n      301978\n    \n  \n\n197 rows × 3 columns"
  },
  {
    "objectID": "DANL210_hw1a.html#q3b.",
    "href": "DANL210_hw1a.html#q3b.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q3b.",
    "text": "Q3b.\n\nFind the top 5 coal-producing states for each year.\n\n\n# Creates a new dataframe q3b\nq3b = (\n       # Reset the index of q3a\n       q3a.reset_index()\n       # Sort the values of q3a by the production column in descending order\n       .sort_values(['production'], ascending = False)\n       # Group q3a by year and get the head with the largest production for each year\n       .groupby('year')\n       .head()\n       # Sort q3b first by year in ascending order and then by production in descending order\n       .sort_values(['year','production'], ascending = [True, False])\n       )\n\nq3b\n\n\n\n\n\n  \n    \n      \n      state\n      year\n      production\n      production_underground\n      production_surface\n    \n  \n  \n    \n      189\n      Wyoming\n      2011\n      438673\n      3043\n      435630\n    \n    \n      181\n      West Virginia\n      2011\n      130186\n      80403\n      49783\n    \n    \n      61\n      Kentucky\n      2011\n      108768\n      65250\n      43518\n    \n    \n      141\n      Pennsylvania\n      2011\n      59183\n      47318\n      11865\n    \n    \n      157\n      Texas\n      2011\n      45903\n      0\n      45903\n    \n    \n      190\n      Wyoming\n      2012\n      401442\n      4637\n      396805\n    \n    \n      182\n      West Virginia\n      2012\n      115247\n      76281\n      38966\n    \n    \n      62\n      Kentucky\n      2012\n      90865\n      58200\n      32665\n    \n    \n      142\n      Pennsylvania\n      2012\n      54719\n      45042\n      9677\n    \n    \n      40\n      Illinois\n      2012\n      48485\n      42835\n      5650\n    \n    \n      191\n      Wyoming\n      2013\n      387924\n      4443\n      383481\n    \n    \n      183\n      West Virginia\n      2013\n      106868\n      74105\n      32763\n    \n    \n      63\n      Kentucky\n      2013\n      80379\n      54621\n      25758\n    \n    \n      143\n      Pennsylvania\n      2013\n      54007\n      45165\n      8842\n    \n    \n      41\n      Illinois\n      2013\n      52148\n      46447\n      5701\n    \n    \n      192\n      Wyoming\n      2014\n      395665\n      3370\n      392295\n    \n    \n      184\n      West Virginia\n      2014\n      107566\n      77074\n      30492\n    \n    \n      64\n      Kentucky\n      2014\n      77337\n      52809\n      24528\n    \n    \n      144\n      Pennsylvania\n      2014\n      60913\n      52913\n      8000\n    \n    \n      42\n      Illinois\n      2014\n      57969\n      52714\n      5255\n    \n    \n      193\n      Wyoming\n      2015\n      375773\n      3090\n      372683\n    \n    \n      185\n      West Virginia\n      2015\n      91174\n      71098\n      20076\n    \n    \n      65\n      Kentucky\n      2015\n      61426\n      43379\n      18047\n    \n    \n      43\n      Illinois\n      2015\n      56100\n      51973\n      4127\n    \n    \n      145\n      Pennsylvania\n      2015\n      50031\n      43894\n      6137\n    \n    \n      194\n      Wyoming\n      2016\n      297218\n      1167\n      296051\n    \n    \n      186\n      West Virginia\n      2016\n      76116\n      61316\n      14800\n    \n    \n      146\n      Pennsylvania\n      2016\n      45718\n      41385\n      4333\n    \n    \n      44\n      Illinois\n      2016\n      43423\n      41258\n      2165\n    \n    \n      66\n      Kentucky\n      2016\n      42868\n      32713\n      10155\n    \n    \n      195\n      Wyoming\n      2017\n      316454\n      1716\n      314738\n    \n    \n      187\n      West Virginia\n      2017\n      87800\n      69175\n      18625\n    \n    \n      147\n      Pennsylvania\n      2017\n      49084\n      43586\n      5498\n    \n    \n      45\n      Illinois\n      2017\n      48204\n      44905\n      3299\n    \n    \n      67\n      Kentucky\n      2017\n      41786\n      31512\n      10274\n    \n    \n      196\n      Wyoming\n      2018\n      304188\n      2210\n      301978\n    \n    \n      188\n      West Virginia\n      2018\n      89876\n      69366\n      20510\n    \n    \n      148\n      Pennsylvania\n      2018\n      49882\n      44633\n      5249\n    \n    \n      46\n      Illinois\n      2018\n      49564\n      46073\n      3491\n    \n    \n      68\n      Kentucky\n      2018\n      39569\n      30964\n      8605"
  },
  {
    "objectID": "DANL210_hw1a.html#q3c.",
    "href": "DANL210_hw1a.html#q3c.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q3c.",
    "text": "Q3c.\n\nVisualize the yearly trend of the total coal production from each type of mine.\n\n\n# selecting the columns 'year', 'production_underground', and 'production_surface' from the original DataFrame coal, grouping them by year, and summing the values of each group.\nq3c = (\n  coal[['year','production_underground', 'production_surface']]\n    .groupby(['year']).sum()\n    )\n\n\n# create a line plot for the 'production_underground' column of the q3c DataFrame.\nq3c['production_underground'].plot()\n\n<AxesSubplot:xlabel='year'>\n\n\n\n\n\n\n# create a line plot for the 'production_surface' column of the q3c DataFrame.\nq3c['production_surface'].plot()\n\n<AxesSubplot:xlabel='year'>\n\n\n\n\n\n\n# create a line plot for the 'production_underground' and 'production_surface' columns of the q3c DataFrame on the same graph.\nq3c.plot()\n\n<AxesSubplot:xlabel='year'>"
  },
  {
    "objectID": "mba-ch1-lm.html",
    "href": "mba-ch1-lm.html",
    "title": "Linear Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr) \nlibrary(ggfortify) # to create regression-related plots\nlibrary(ggcorrplot) # to create correlation heatmaps\nlibrary(fastDummies) # to create dummy variables\nlibrary(stargazer) # to create regression tables\n\noj <- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')"
  },
  {
    "objectID": "mba-ch1-lm.html#exploratory-data-analysis",
    "href": "mba-ch1-lm.html#exploratory-data-analysis",
    "title": "Linear Regression",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDescriptive Statistics\n\nskim(oj)\n\n\nData summary\n\n\nName\noj\n\n\nNumber of rows\n28947\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nbrand\n0\n1\n9\n11\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales\n0\n1\n17312.21\n27477.66\n64.00\n4864.00\n8384.00\n17408.00\n716416.00\n▇▁▁▁▁\n\n\nprice\n0\n1\n2.28\n0.65\n0.52\n1.79\n2.17\n2.73\n3.87\n▁▆▇▅▂\n\n\nad\n0\n1\n0.24\n0.43\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\n\n\noj %>% group_by(brand) %>% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n28947\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nbrand\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nbrand\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales\ndominicks\n0\n1\n19834.56\n32245.47\n64.00\n4416.00\n9152.00\n21056.00\n716416.00\n▇▁▁▁▁\n\n\nsales\nminute.maid\n0\n1\n18238.46\n29992.21\n320.00\n4800.00\n8320.00\n18560.00\n591360.00\n▇▁▁▁▁\n\n\nsales\ntropicana\n0\n1\n13863.62\n17515.82\n192.00\n5248.00\n8000.00\n13824.00\n288384.00\n▇▁▁▁▁\n\n\nprice\ndominicks\n0\n1\n1.74\n0.39\n0.52\n1.58\n1.59\n1.99\n2.69\n▁▂▇▃▂\n\n\nprice\nminute.maid\n0\n1\n2.24\n0.40\n0.88\n1.99\n2.17\n2.49\n3.17\n▁▂▇▆▂\n\n\nprice\ntropicana\n0\n1\n2.87\n0.55\n1.29\n2.49\n2.99\n3.19\n3.87\n▁▃▅▇▅\n\n\nad\ndominicks\n0\n1\n0.26\n0.44\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nad\nminute.maid\n0\n1\n0.29\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nad\ntropicana\n0\n1\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\n\n\noj %>% group_by(brand, ad) %>% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n28947\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nbrand, ad\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nbrand\nad\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales\ndominicks\n0\n0\n1\n11733.91\n15718.77\n64.00\n3584.00\n7040.00\n13312.00\n248000.00\n▇▁▁▁▁\n\n\nsales\ndominicks\n1\n0\n1\n43251.33\n50930.45\n64.00\n11888.00\n27744.00\n54160.00\n716416.00\n▇▁▁▁▁\n\n\nsales\nminute.maid\n0\n0\n1\n8372.65\n7701.69\n320.00\n4160.00\n6272.00\n9984.00\n231808.00\n▇▁▁▁▁\n\n\nsales\nminute.maid\n1\n0\n1\n42566.32\n46260.27\n1728.00\n16576.00\n29440.00\n49808.00\n591360.00\n▇▁▁▁▁\n\n\nsales\ntropicana\n0\n0\n1\n10038.86\n10246.02\n192.00\n4864.00\n7168.00\n11008.00\n175872.00\n▇▁▁▁▁\n\n\nsales\ntropicana\n1\n0\n1\n33047.02\n29632.94\n1408.00\n10816.00\n23808.00\n46896.00\n288384.00\n▇▂▁▁▁\n\n\nprice\ndominicks\n0\n0\n1\n1.80\n0.38\n0.52\n1.58\n1.69\n1.99\n2.69\n▁▂▇▅▂\n\n\nprice\ndominicks\n1\n0\n1\n1.56\n0.34\n0.89\n1.39\n1.58\n1.59\n2.69\n▂▇▂▁▁\n\n\nprice\nminute.maid\n0\n0\n1\n2.33\n0.41\n0.88\n1.99\n2.26\n2.62\n3.17\n▁▂▇▇▃\n\n\nprice\nminute.maid\n1\n0\n1\n2.02\n0.30\n0.99\n1.99\n1.99\n2.19\n2.81\n▁▂▇▂▂\n\n\nprice\ntropicana\n0\n0\n1\n2.97\n0.51\n1.32\n2.59\n2.99\n3.39\n3.87\n▁▂▃▇▅\n\n\nprice\ntropicana\n1\n0\n1\n2.39\n0.46\n1.29\n1.99\n2.39\n2.79\n3.59\n▁▇▆▅▂\n\n\n\n\n\n\n\n\nData Visualization\n\nCorrelation heatmap is a great tool to start identifying which input variables are strongly correlated with an outcome variable.\n\n\n# to convert a factor variable into indicators\noj_dummies <- dummy_cols(oj, select_columns = 'brand' ) %>% \n  select(-brand)\n\n# the matrix of the correlation test p-values\np.mat <- cor_pmat(oj_dummies) \n\n# correlation heatmap with correlation values\nggcorrplot( cor(oj_dummies), lab = T,\n            type = 'lower',\n            colors = c(\"#2E74C0\", \"white\", \"#CB454A\"),\n            p.mat = p.mat) # p.values\n\n\n\n# variation in log price\nggplot(oj, aes(x = log(price), fill = brand )) +\n  geom_histogram() +\n  facet_wrap(brand ~., ncol = 1)\n\n\n\n# variation in log sales\nggplot(oj, aes(x = log(sales), fill = brand )) +\n  geom_histogram() +\n  facet_wrap(brand ~., ncol = 1)\n\n\n\n# law of demand\np <- ggplot(oj, aes(x = log(sales), y = log(price),\n                    color = brand ))\n\np + geom_point( alpha = .025 ) +\n  geom_smooth(method = lm, se = F)\n\n\n\n# mosaic plot\nggplot(data = oj) +\n  geom_bar(aes(x = as.factor(ad), y = after_stat(prop),\n               group = brand, fill = brand), \n           position = \"fill\") +\n  labs(x = 'ad') +\n  theme(plot.title = element_text(size = rel(1.5)),\n        axis.title = element_text(size = 25),\n        axis.text.x = element_text(size = rel(1.5)),\n        axis.text.y = element_text(size = rel(1.5)))"
  },
  {
    "objectID": "mba-ch1-lm.html#linear-regression-model",
    "href": "mba-ch1-lm.html#linear-regression-model",
    "title": "Linear Regression",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\nA basic but powerful regression strategy is to deal in averages and lines.\n\nWe model the conditional mean for \\(y\\) given \\(x\\) as\n\n\n\\[\n\\begin{align}\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] &= \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}\\\\\n{ }\\\\\ny_{i} &=   \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1, i} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p, i} + \\epsilon_{i} \\quad \\text{for } i = 1, 2, ..., n\n\\end{align}\n\\] - Linear regression is used to model linear relationship between an outcome variable, \\(y\\), and a set of predictor variables \\(x_{1}, \\,\\cdots\\,, x_{p}\\).\n\nknitr::include_graphics('lec_figs/mba-1-2.png')\n\n\n\n\n\n\n\n\n\n\\(\\beta_{0}\\) is an intercept when \\(\\mathbf{X} = \\mathbf{0}\\).\n\\(\\beta_{1}\\) is a slope that describes a change in average value for \\(y\\) for each one-unit increase in \\(x_{1}\\).\n\\(\\epsilon_{i}\\) is the random noise.\n\nFor inference, we need to assume that \\(\\epsilon_{i}\\) is independent, identically distributed (iid) from Normal distribution.\n\n\n\\[\n\\epsilon_i \\overset{iid}{\\sim}N(0, \\sigma^2) \\quad \\text{ with constant variance } \\sigma^2\n\\]\n\nknitr::include_graphics('lec_figs/mba-1-3.png')\n\n\n\n\n\n\n\n\n\n\nFitted Line and Beta Estimates\n\nWe estimatede the best fitting line by ordinary least squares (OLS) - by minimizing the sum of squared errors (SSE)\n\n\\[\nS S E\\left(\\beta_{0}, \\beta_{1}\\right)=\\sum_{i=1}^{n}\\left[Y_{i}-\\left({\\beta}_{0}+{\\beta}_{1} X_{i}\\right)\\right]^{2}\n\\]\n\nknitr::include_graphics('lec_figs/mba-1-9.png')\n\n\n\n\n\n\n\n\nTherefore, the beta estimate has the following solution:\n\\[\n\\widehat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}} \\quad \\text{ and } \\quad \\widehat{\\beta}_{0}=\\bar{Y}-\\widehat{\\beta}_{1} \\bar{X}\n\\]\nwhere \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) and \\(\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\)\n\n\n\nConnection to covariance and correlation\n\nCovariance describes the joint variability of two variables\n\n\\[\n\\text{Cov}(X, Y) = \\sigma_{X,Y} = \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n\\]\n\nCorrelation is a normalized form of covariance, ranges from -1 to 1\n\n\\[\n\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}\n\\] - So, the beta coefficent can be represented by:\n$$ \\[\\begin{align}\n\\widehat{\\beta}_{1} &= \\, \\frac{\\widehat{\\text{Cov}}(X,Y)}{\\widehat{\\text{Var}}(X)} \\,=\\,  \\hat{\\rho}_{x,y} \\cdot \\frac{\\hat{\\sigma}_{Y}}{\\hat{\\sigma}_{X}}\n\n\\end{align}\\] $$\n\n\n\nInference with OLS\n\n\\(t\\)-statistics are coefficients Estimates / Std. Error, i.e., number of standard deviations from 0\n\np-values (i.e., Pr(>|t|)): estimated probability observing value as extreme as |t value| given the null hypothesis \\(\\beta = 0\\)\np-value \\(<\\) conventional threshold of \\(\\alpha = 0.05\\), sufficient evidence to reject the null hypothesis that the coefficient is zero,\nTypically |t values| \\(> 2\\) indicate significant relationship at \\(\\alpha = 0.05\\)\ni.e., there is a significant association between log(sales) and log(price)\n\nControlling the Type 1 error rate at \\(\\alpha = 0.05\\), i.e., the probability of a false positive mistake:\n\n5% chance that you’ll conclude there’s a significant association between \\(x\\) and \\(y\\) even when there is none\n\n\n\n\n\nR-squared\n\n\\(R^2\\) estimates the proportion of the variance of \\(Y\\) explained by \\(X\\).\n\n\n\n\nMean Squared Errors\n\nMean squared error (MSE) is a commonly used metric to evaluate the performance of a regression model.\n\nIt measures the average squared difference between the predicted values and the actual values in the dataset.\n\n\n\\[\nM S E \\,=\\, \\frac{\\sum_{i = 1}^{ n}\\,(\\, y_{i} - \\hat{y}_{i} \\,)^2}{n}\n\\] - Root mean squared error (RMSE) is the square root of the mean squared error (MSE).\n\\[\nR M S E \\,=\\, \\sqrt{M S E}\n\\] - RMSE shows how far predictions fall from true values.\n\n\n\nThe Goals of Linear Regression\n\nModeling for prediction (\\(\\hat{y}\\)): When we want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(\\mathbf{X}\\).\n\n\nWe are estimating the conditional expection (mean) for \\(y\\): \\[\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] = \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}.\n\\]\nwhich is the average value for \\(y\\) given the value for \\(X\\).\n\n\n\nModeling for explanation (\\(\\hat{\\beta}\\)): When we want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(\\mathbf{X}\\).\n\n\nCorrelation does not imply causation.\nWithout proper identification strategies, \\(\\beta_{1}\\) just means a correlation between \\(x_{1}\\) and \\(y\\).\nHowever, we can possibly identify a causal relationship between the explanatory variable and the outcome variable."
  },
  {
    "objectID": "mba-ch1-lm.html#linear-regression-and-controls",
    "href": "mba-ch1-lm.html#linear-regression-and-controls",
    "title": "Linear Regression",
    "section": "Linear Regression and Controls",
    "text": "Linear Regression and Controls\n\nSimple Linear Regression\nTo start, we can fit a simple model that regresses log price on log sales.\n\\[\n\\mathbb{E}[\\, \\log(\\, \\texttt{sales} \\,) \\,|\\, \\texttt{price}\\,] = \\alpha \\,+\\, \\beta\\, \\log(\\,\\texttt{price}\\,)\n\\] - The following model incorporates both brand and price:\n\\[\n\\mathbb{E}[\\, \\log(\\, \\texttt{sales} \\,) \\,|\\, \\texttt{price}\\,] = \\alpha_{\\texttt{brand}} \\,+\\, \\beta\\, \\log(\\,\\texttt{price}\\,)\n\\]\nformula_0 <- log(sales) ~ log(price)\nformula_1 <- log(sales) ~ brand + log(price)\n\nfit_0 <- lm( formula_0, data = oj )\nfit_1 <- lm( formula_1, data = oj )\n\nstargazer(fit_0, fit_1, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlog(sales)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nbrandminute.maid\n\n\n\n\n0.870***\n\n\n\n\n\n\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana\n\n\n\n\n1.530***\n\n\n\n\n\n\n\n\n(0.016)\n\n\n\n\n\n\n\n\n\n\n\n\nlog(price)\n\n\n-1.601***\n\n\n-3.139***\n\n\n\n\n\n\n(0.018)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n10.423***\n\n\n10.829***\n\n\n\n\n\n\n(0.015)\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n28,947\n\n\n28,947\n\n\n\n\nR2\n\n\n0.208\n\n\n0.394\n\n\n\n\nAdjusted R2\n\n\n0.208\n\n\n0.394\n\n\n\n\nResidual Std. Error\n\n\n0.907 (df = 28945)\n\n\n0.794 (df = 28943)\n\n\n\n\nF Statistic\n\n\n7,608.212*** (df = 1; 28945)\n\n\n6,275.074*** (df = 3; 28943)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nWe know that there are different brands of OJ here and some are more valuable than others.\n\nWhen we control for brand effect, the elasticity estimate nearly doubles to −3.14.\n\nThe premium brands, Minute Maid and Tropicana, had equivalent sales to Dominick’s at higher price points.\nSo if we don’t control for brand, it looks as though prices can rise without affecting sales for those observations.\n\nThis dampens the observable relationship between prices and sales and results in the (artificially) low elasticity estimate of −1.6.\n\nMore mechanically, how does this happen in regression?\n\n\nprice_fit <- lm(log(price) ~ brand, data = oj)\np_hat <- predict(price_fit, newdata = oj)\np_resid <- log(oj$price) - p_hat\n\n# regress log sales on p_resid\nresid_fit <- lm(log(sales) ~ p_resid, data = oj)\n\n# What is the beta coefficient for p_resid?!\nround( coef(resid_fit)[2], digit = 3 )\n\np_resid \n -3.139 \n\n\n\nThe coefficient on p_resid, the residuals from regression of log price on brand, is exactly the same as what we get on log(price) in the multiple linear regression for log sales onto this and brand!\nThis is one way that you can understand what OLS is doing:\n\nIt is finding the coefficients on the part of each input that is independent from the other inputs.\n\n\n\n\n\nControls in Linear Regression\n\nOmitted variable bias is a type of bias that can occur in linear regression when an important variable that is related to both the outcome variable and the input variable(s) is not included in the model.\n\nThis omission can lead to biased estimates of the beta coefficients of the included input variables.\n\nIn linear regression, a confounding variable is a variable that is related to both treatment and outcome variables, and that affects the relationship between them.\n\nA treatment variable is an input variable that the researcher believes has a causal effect on the outcome variable.\nWhen a confounding variable is not controlled in the regression model, it can lead to biased estimates of the relationship between the independent and treatment variables.\n\nBad controls in linear regression refer to the inclusion of variables in the model that do not actually control for the confounding factors they are intended to control for.\n\nThis can lead to biased estimates of the relationship between the independent and dependent variables.\n\n\n# simulation data\ntb <- tibble( \n  female = ifelse(runif(10000)>=0.5,1,0), # female indicator variable\n  ability = rnorm(10000), # e.g., talent, usually unobserved.\n  discrimination = female, # gender discrimination variable\n  occupation = 1 + 2*ability + 0*female - 2*discrimination + rnorm(10000), # true data generating process for occupation variable\n  wage = 1 - 1*discrimination + 1*occupation + 2*ability + rnorm(10000) # true data generating process for wage variable\n)\n\nlm_1 <- lm(wage ~ female, tb)\nlm_2 <- lm(wage ~ female + occupation, tb)\nlm_3 <- lm(wage ~ female + occupation + ability, tb)\n\nstargazer(lm_1,lm_2,lm_3, \n          column.labels = c(\"Biased Unconditional\", \n                            \"Biased\",\n                            \"Unbiased Conditional\"),\n          type = 'html')\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nwage\n\n\n\n\n\n\nBiased Unconditional\n\n\nBiased\n\n\nUnbiased Conditional\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nfemale\n\n\n-2.906***\n\n\n0.622***\n\n\n-0.983***\n\n\n\n\n\n\n(0.085)\n\n\n(0.029)\n\n\n(0.028)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noccupation\n\n\n\n\n1.793***\n\n\n1.000***\n\n\n\n\n\n\n\n\n(0.006)\n\n\n(0.010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nability\n\n\n\n\n\n\n1.981***\n\n\n\n\n\n\n\n\n\n\n(0.022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n1.884***\n\n\n0.174***\n\n\n0.983***\n\n\n\n\n\n\n(0.059)\n\n\n(0.019)\n\n\n(0.017)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n10,000\n\n\n10,000\n\n\n10,000\n\n\n\n\nR2\n\n\n0.105\n\n\n0.913\n\n\n0.951\n\n\n\n\nAdjusted R2\n\n\n0.105\n\n\n0.913\n\n\n0.951\n\n\n\n\nResidual Std. Error\n\n\n4.232 (df = 9998)\n\n\n1.321 (df = 9997)\n\n\n0.988 (df = 9996)\n\n\n\n\nF Statistic\n\n\n1,178.157*** (df = 1; 9998)\n\n\n52,328.200*** (df = 2; 9997)\n\n\n65,047.100*** (df = 3; 9996)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\noccupation is a bad control.\n\n\n\n\nResidual plot, QQ plot, and Residual vs Leverage plot\n\nResiduals should NOT display any systematic pattern.\nCheck the assumptions about normality with a QQ plot using ggfortify.\n\n\nlibrary(ggfortify)\nautoplot(fit_1, ncol = 1)\n\n\n\n\n\nStandardized residuals = residuals / sd(residuals)\nA QQ (Quantile-Quantile) plot is a graphical tool used to assess the normality of a distribution.\n\nIn the context of linear regression, a QQ plot can be used to assess whether the residuals are normally distributed.\nA QQ plot visualizes the relationship between the quantiles of the residuals and the quantiles of a theoretical normal distribution.\nQQ plots can be useful in identifying potential outliers or influential observations in a linear regression model, as well as in deciding whether to transform the dependent variable or use a different type of regression model altogether.\n\nA residual vs leverage plot is a graphical tool used to detect influential observations in linear regression.\n\nLeverage refers to how much an observation’s independent variables differ from the mean of the independent variables, and is a measure of how much influence that observation has on the regression line.\nIn a residual vs leverage plot, influential observations will typically appear as points that are far away from the center of the plot.\nIf an observation has high leverage but a small residual, it may not be influential.\nConversely, an observation with a large residual but low leverage may also not be influential."
  },
  {
    "objectID": "mba-ch1-lm.html#exercise",
    "href": "mba-ch1-lm.html#exercise",
    "title": "Linear Regression",
    "section": "Exercise",
    "text": "Exercise\nConsider the orange juice models:\n\nformula_0 <- log(sales) ~ log(price)\nformula_1 <- log(sales) ~ brand + log(price)\n\nfit_0 <- lm( formula_0, data = oj )\nfit_1 <- lm( formula_1, data = oj )\n\n\nDraw a residual plot for each model of fit_0 and fit_1.\nCalculate the RMSE for each model of fit_0 and fit_1.\nReview the interaction models:\n\nformula_0 <- log(sales) ~ log(price)\nformula_1 <- log(sales) ~ brand + log(price)\nformula_2 <- log(sales) ~ brand * log(price)\nformula_3 <- log(sales) ~ brand * ad * log(price)\n\nfit_0 <- lm( formula_0, data = oj )\nfit_1 <- lm( formula_1, data = oj )\nfit_2 <- lm( formula_2, data = oj )\nfit_3 <- lm( formula_3, data = oj )\n\nstargazer(fit_1, fit_2, fit_3, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlog(sales)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nbrandminute.maid\n\n\n0.870***\n\n\n0.888***\n\n\n0.047\n\n\n\n\n\n\n(0.013)\n\n\n(0.042)\n\n\n(0.047)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana\n\n\n1.530***\n\n\n0.962***\n\n\n0.708***\n\n\n\n\n\n\n(0.016)\n\n\n(0.046)\n\n\n(0.051)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nad\n\n\n\n\n\n\n1.094***\n\n\n\n\n\n\n\n\n\n\n(0.038)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(price)\n\n\n-3.139***\n\n\n-3.378***\n\n\n-2.774***\n\n\n\n\n\n\n(0.023)\n\n\n(0.036)\n\n\n(0.039)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandminute.maid:ad\n\n\n\n\n\n\n1.173***\n\n\n\n\n\n\n\n\n\n\n(0.082)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana:ad\n\n\n\n\n\n\n0.785***\n\n\n\n\n\n\n\n\n\n\n(0.099)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandminute.maid:log(price)\n\n\n\n\n0.057\n\n\n0.783***\n\n\n\n\n\n\n\n\n(0.057)\n\n\n(0.061)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana:log(price)\n\n\n\n\n0.666***\n\n\n0.736***\n\n\n\n\n\n\n\n\n(0.054)\n\n\n(0.057)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nad:log(price)\n\n\n\n\n\n\n-0.471***\n\n\n\n\n\n\n\n\n\n\n(0.074)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandminute.maid:ad:log(price)\n\n\n\n\n\n\n-1.109***\n\n\n\n\n\n\n\n\n\n\n(0.122)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana:ad:log(price)\n\n\n\n\n\n\n-0.986***\n\n\n\n\n\n\n\n\n\n\n(0.124)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n10.829***\n\n\n10.955***\n\n\n10.407***\n\n\n\n\n\n\n(0.015)\n\n\n(0.021)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n28,947\n\n\n28,947\n\n\n28,947\n\n\n\n\nR2\n\n\n0.394\n\n\n0.398\n\n\n0.535\n\n\n\n\nAdjusted R2\n\n\n0.394\n\n\n0.398\n\n\n0.535\n\n\n\n\nResidual Std. Error\n\n\n0.794 (df = 28943)\n\n\n0.791 (df = 28941)\n\n\n0.695 (df = 28935)\n\n\n\n\nF Statistic\n\n\n6,275.074*** (df = 3; 28943)\n\n\n3,823.404*** (df = 5; 28941)\n\n\n3,031.232*** (df = 11; 28935)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01"
  },
  {
    "objectID": "mba-ch1-lm.html#references",
    "href": "mba-ch1-lm.html#references",
    "title": "Linear Regression",
    "section": "References",
    "text": "References\n\n\nCausal Inference: The Mixtape by Scott Cunningham.\nStatistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim.\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\n\n\n\nSummer Undergraduate Research Experience (SURE) 2022 in Statistics at Carnegie Mellon University by Ron Yurko."
  },
  {
    "objectID": "DANL210_lab2q.html",
    "href": "DANL210_lab2q.html",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "",
    "text": "import pandas as pd\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab2q.html#load-dataframe",
    "href": "DANL210_lab2q.html#load-dataframe",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbeer_mkt = pd.read_csv('https://bcdanl.github.io/data/beer_markets.csv')\nbeer_mkt.head(10)\n\n\n\n\n\n  \n    \n      \n      hh\n      _purchase_desc\n      quantity\n      brand\n      dollar_spent\n      beer_floz\n      price_per_floz\n      container\n      promo\n      market\n      ...\n      age\n      employment\n      degree\n      cow\n      race\n      microwave\n      dishwasher\n      tvcable\n      singlefamilyhome\n      npeople\n    \n  \n  \n    \n      0\n      2000946\n      BUD LT BR CN 12P\n      1\n      BUD LIGHT\n      8.14\n      144.0\n      0.056528\n      CAN\n      False\n      RURAL ILLINOIS\n      ...\n      50+\n      none\n      Grad\n      none/retired/student\n      white\n      True\n      True\n      premium\n      False\n      1\n    \n    \n      1\n      2003036\n      BUD LT BR CN 24P\n      1\n      BUD LIGHT\n      17.48\n      288.0\n      0.060694\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      2\n      2003036\n      BUD LT BR CN 24P\n      2\n      BUD LIGHT\n      33.92\n      576.0\n      0.058889\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      3\n      2003036\n      BUD LT BR CN 30P\n      2\n      BUD LIGHT\n      34.74\n      720.0\n      0.048250\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      4\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      40.48\n      864.0\n      0.046852\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      5\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      42.96\n      864.0\n      0.049722\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      6\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      40.96\n      864.0\n      0.047407\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      7\n      2001521\n      BUD LT BR CN 6P\n      5\n      BUD LIGHT\n      30.60\n      480.0\n      0.063750\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n    \n      8\n      2001521\n      BUD LT BR CN 6P\n      1\n      BUD LIGHT\n      9.99\n      96.0\n      0.104063\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n    \n      9\n      2001521\n      BUD LT BR CN 6P\n      5\n      BUD LIGHT\n      30.70\n      480.0\n      0.063958\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n  \n\n10 rows × 24 columns\n\n\n\n\nVariable Description\n\nhh: An identifier of the purchasing household;\n_purchase_desc: Details on the purchased item;\nquantity: Number of items purchased;\nbrand: BUD LIGHT, BUSCH LIGHT, COORS LIGHT, MILLER LITE, or NATURAL LIGHT;\nspent: Total dollar value of purchase;\nbeer_floz: Total volume of beer, in fluid ounces;\nprice_per_floz: Price per fl.oz. (i.e., spent/beer_floz);\ncontainer: Type of container;\npromo: Whether the item was promoted (coupon or something else);\nmarket: Scan-track market (or state if rural);\nvarious demographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher.\n\nSummarize DataFrame beer_mkt.\n\n\nskim(beer_mkt)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 73115  │ │ string      │ 13    │                                                          │\n│ │ Number of columns │ 24     │ │ bool        │ 6     │                                                          │\n│ └───────────────────┴────────┘ │ float64     │ 3     │                                                          │\n│                                │ int64       │ 2     │                                                          │\n│                                └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name      ┃ NA  ┃ NA %  ┃ mean      ┃ sd        ┃ p0       ┃ p25     ┃ p75      ┃ p100     ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━┩  │\n│ │ hh               │   0 │     0 │  17000000 │  12000000 │  2000000 │ 8200000 │ 30000000 │ 30000000 │ ▂█   █ │  │\n│ │ quantity         │   0 │     0 │       1.3 │       1.1 │        1 │       1 │        1 │       48 │   █    │  │\n│ │ dollar_spent     │   0 │     0 │        14 │       8.7 │     0.51 │       9 │       16 │      160 │   █▁   │  │\n│ │ beer_floz        │   0 │     0 │       270 │       200 │       12 │     140 │      360 │     9200 │   █    │  │\n│ │ price_per_floz   │   0 │     0 │     0.056 │     0.013 │   0.0013 │   0.046 │    0.064 │     0.23 │   ▁█   │  │\n│ └──────────────────┴─────┴───────┴───────────┴───────────┴──────────┴─────────┴──────────┴──────────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                   ┃ NA     ┃ NA %       ┃ words per row               ┃ total words            ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ _purchase_desc                │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ brand                         │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ container                     │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ market                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ buyertype                     │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ income                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ age                           │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ employment                    │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ degree                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ cow                           │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ race                          │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ tvcable                       │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ npeople                       │      0 │          0 │                         5.3 │                 390000 │  │\n│ └───────────────────────────────┴────────┴────────────┴─────────────────────────────┴────────────────────────┘  │\n│                                                      bool                                                       │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                               ┃ true            ┃ true rate                 ┃ hist             ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩  │\n│ │ promo                                     │           15000 │                       0.2 │      █    ▂      │  │\n│ │ childrenUnder6                            │            5000 │                     0.068 │      █    ▁      │  │\n│ │ children6to17                             │           15000 │                       0.2 │      █    ▂      │  │\n│ │ microwave                                 │           73000 │                      0.99 │           █      │  │\n│ │ dishwasher                                │           53000 │                      0.73 │      ▃    █      │  │\n│ │ singlefamilyhome                          │           59000 │                      0.81 │      ▂    █      │  │\n│ └───────────────────────────────────────────┴─────────────────┴───────────────────────────┴──────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nbeer_mkt.describe()\n\n\n\n\n\n  \n    \n      \n      hh\n      quantity\n      dollar_spent\n      beer_floz\n      price_per_floz\n    \n  \n  \n    \n      count\n      7.311500e+04\n      73115.000000\n      73115.000000\n      73115.000000\n      73115.000000\n    \n    \n      mean\n      1.740772e+07\n      1.317527\n      13.777683\n      265.926853\n      0.055951\n    \n    \n      std\n      1.158215e+07\n      1.149649\n      8.722942\n      199.522488\n      0.013417\n    \n    \n      min\n      2.000235e+06\n      1.000000\n      0.510000\n      12.000000\n      0.001315\n    \n    \n      25%\n      8.223438e+06\n      1.000000\n      8.970000\n      144.000000\n      0.046306\n    \n    \n      50%\n      8.413624e+06\n      1.000000\n      12.990000\n      216.000000\n      0.055509\n    \n    \n      75%\n      3.017132e+07\n      1.000000\n      16.380000\n      360.000000\n      0.063750\n    \n    \n      max\n      3.044072e+07\n      48.000000\n      159.130000\n      9216.000000\n      0.234063"
  },
  {
    "objectID": "DANL210_lab2q.html#q1a",
    "href": "DANL210_lab2q.html#q1a",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1a",
    "text": "Q1a\n\nSort the DataFrame beer_mkt by hh in ascending order."
  },
  {
    "objectID": "DANL210_lab2q.html#q1b",
    "href": "DANL210_lab2q.html#q1b",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1b",
    "text": "Q1b\n\nFind the top 5 beer markets in terms of the number of households that purchased beer."
  },
  {
    "objectID": "DANL210_lab2q.html#q1c",
    "href": "DANL210_lab2q.html#q1c",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1c",
    "text": "Q1c\n\nFind the top 5 beer markets in terms of the amount of total beer consumption."
  },
  {
    "objectID": "DANL210_lab2q.html#q1d",
    "href": "DANL210_lab2q.html#q1d",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1d",
    "text": "Q1d\n\nProvide (1) seaborn code and (2) a simple comment to describe how the distribution of price_per_floz varies by brand."
  },
  {
    "objectID": "DANL210_lab2q.html#q1e",
    "href": "DANL210_lab2q.html#q1e",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1e",
    "text": "Q1e\n\nProvide (1) seaborn code and (2) a simple comment to describe how the relationship between price_per_floz and beer_floz varies by brand."
  },
  {
    "objectID": "DANL210_hw1q.html",
    "href": "DANL210_hw1q.html",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files.\nImport all the Python libraries you need here.\n\n\nimport pandas as pd"
  },
  {
    "objectID": "DANL210_hw1q.html#q1a",
    "href": "DANL210_hw1q.html#q1a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q1a",
    "text": "Q1a\nCalculate the simple difference between the probability of survival when passengers are first-class and the probability of survival when they are not."
  },
  {
    "objectID": "DANL210_hw1q.html#q1b",
    "href": "DANL210_hw1q.html#q1b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q1b",
    "text": "Q1b\nHow much does the probability of survival increase for first-class passengers relative to those who are not first-class passengers?"
  },
  {
    "objectID": "DANL210_hw1q.html#q1c",
    "href": "DANL210_hw1q.html#q1c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q1c",
    "text": "Q1c\nConsider the probability of survival in titanic_2.csv.\n\ntitanic_2 = pd.read_csv(\"data/titanic_2.csv\")\n\nAfter stratifying on gender and age, what happens to the difference in the probabilities of survival between first-class passengers and non-first-class passengers.\nExplain in your own words what stratifying on gender and age did for this difference in probabilities of survival between first-class passengers and non-first-class passengers."
  },
  {
    "objectID": "DANL210_hw1q.html#q2a",
    "href": "DANL210_hw1q.html#q2a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2a",
    "text": "Q2a\nHow many players have been recorded?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2b",
    "href": "DANL210_hw1q.html#q2b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2b",
    "text": "Q2b\nA column points (P) is missing in the data. The number of points of a player is defined as the sum of his goals (G) and assists (A). Add the point column P to your DataFrame."
  },
  {
    "objectID": "DANL210_hw1q.html#q2c",
    "href": "DANL210_hw1q.html#q2c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2c",
    "text": "Q2c\nWho is the top scorer in terms of points?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2d",
    "href": "DANL210_hw1q.html#q2d",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2d",
    "text": "Q2d\nHow many Russian (non-goalie) players had some ice time in there 2016/2017 regular season? Hint: Nationality of a player can be found in “Nat”. Russians are indicated by “RUS”."
  },
  {
    "objectID": "DANL210_hw1q.html#q2e",
    "href": "DANL210_hw1q.html#q2e",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2e",
    "text": "Q2e\nWhat are their names?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2f",
    "href": "DANL210_hw1q.html#q2f",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2f",
    "text": "Q2f\nWho performed best among the Russian players in terms of points (P)?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2g",
    "href": "DANL210_hw1q.html#q2g",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2g",
    "text": "Q2g\nHow many points (P) did he have?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2h",
    "href": "DANL210_hw1q.html#q2h",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2h",
    "text": "Q2h\nHow well did he perform in the entire league? Put differently, what was his rank in terms of points?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2i",
    "href": "DANL210_hw1q.html#q2i",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2i",
    "text": "Q2i\nFind the top ten scorers (in terms of points) and print them including their number of point and their respective team."
  },
  {
    "objectID": "DANL210_hw1q.html#q2j",
    "href": "DANL210_hw1q.html#q2j",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2j",
    "text": "Q2j\nWhat are the three countries with the most players originating from?"
  },
  {
    "objectID": "DANL210_hw1q.html#q3a",
    "href": "DANL210_hw1q.html#q3a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q3a",
    "text": "Q3a\nFor each type of mine, calculate the total coal production for each pair of state-year."
  },
  {
    "objectID": "DANL210_hw1q.html#q3b",
    "href": "DANL210_hw1q.html#q3b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q3b",
    "text": "Q3b\nFind the top 5 coal-producing states for each year."
  },
  {
    "objectID": "DANL210_hw1q.html#q3c",
    "href": "DANL210_hw1q.html#q3c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q3c",
    "text": "Q3c\nVisualize the yearly trend of the total coal production from each type of mine."
  },
  {
    "objectID": "DANL210_lab3a.html",
    "href": "DANL210_lab3a.html",
    "title": "Python Lab 3 - Tidy Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab3a.html#load-dataframe",
    "href": "DANL210_lab3a.html#load-dataframe",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbillboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')\nny_pincp = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_wide.csv')\ncovid = pd.read_csv('https://bcdanl.github.io/data/covid19_cases.csv')"
  },
  {
    "objectID": "DANL210_lab3a.html#q1a",
    "href": "DANL210_lab3a.html#q1a",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1a",
    "text": "Q1a\n\nDescribe how the distribution of rating varies across week 1, week 2, and week 3 using the faceted histogram.\n\n\nbillboard_long = billboard.melt(\n  id_vars = [\"year\", \"artist\", \"track\", \"time\", \"date.entered\"],\n  var_name = \"week\",\n  value_name = \"rating\",\n)\n\n\n# The .isin() method is used to check whether each value in the \"week\" column is present in the given list of values. \nbillboard_wk1_2_3 = billboard_long.loc[\n    billboard_long['week'].isin(['wk1', 'wk2', 'wk3'])\n    ]\n\nsns.displot(billboard_wk1_2_3,\n             x = 'rating',\n             row = 'week'\n             )\n\n<seaborn.axisgrid.FacetGrid at 0x7f9d40845cd0>"
  },
  {
    "objectID": "DANL210_lab3a.html#q1b",
    "href": "DANL210_lab3a.html#q1b",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1b",
    "text": "Q1b\n\nWhich artist(s) have the most number of tracks in billboard DataFrame?\n\n\nbillboard_songs = (\n    billboard[[\"artist\", \"track\"]]\n    .drop_duplicates()        # drops duplicate observations.\n    .drop(\"track\", axis = 1)  # drops variable, track.\n    .value_counts()\n    )"
  },
  {
    "objectID": "DANL210_lab3a.html#q1c",
    "href": "DANL210_lab3a.html#q1c",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1c",
    "text": "Q1c\n\nMake ny_pincp longer.\n\n\nny_pincp_long = ny_pincp.melt(\n        id_vars = ['fips', 'geoname'],\n        var_name = 'year',\n        value_name = 'pincp'\n        )"
  },
  {
    "objectID": "DANL210_lab3a.html#q1d",
    "href": "DANL210_lab3a.html#q1d",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1d",
    "text": "Q1d\n\nMake a wide-form DataFrame of covid whose variable names are from countriesAndTerritories and values are from cases.\n\n\ncovid_wide = (\n    covid\n    .pivot_table(index = 'date', \n           columns = 'countriesAndTerritories', \n           values = 'cases')\n    )"
  },
  {
    "objectID": "DANL210_lab3a.html#q1e",
    "href": "DANL210_lab3a.html#q1e",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1e",
    "text": "Q1e\n\nUse the wide-form DataFrame of covid to find the top 10 countries for which their cases are highly correlated with USA’s cases using DataFrame.corr()\n\n\ncorr_usa = (\n    covid_wide.corr()\n    .sort_values(by = 'USA', ascending = False)\n    .USA\n    )"
  },
  {
    "objectID": "DANL310_lab1a.html",
    "href": "DANL310_lab1a.html",
    "title": "R Lab - Map Visualization",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(socviz)"
  },
  {
    "objectID": "DANL310_lab1a.html#variable-description",
    "href": "DANL310_lab1a.html#variable-description",
    "title": "R Lab - Map Visualization",
    "section": "Variable Description",
    "text": "Variable Description\n\nbelief:\n\nhuman: Estimated percentage who think that global warming is caused mostly by human activities.\nhappening: Estimated percentage who think that global warming is happening.\n\n\n\nclimate_opinion_long <- read_csv(\n  'https://bcdanl.github.io/data/climate_opinion_2021.csv')\n\nRows: 6284 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): GeoName, belief\ndbl (2): id, perc\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nclimate_opinion_long\n\n# A tibble: 6,284 × 4\n      id GeoName                 belief     perc\n   <dbl> <chr>                   <chr>     <dbl>\n 1  1001 Autauga County, Alabama happening  59.2\n 2  1001 Autauga County, Alabama human      45.5\n 3  1003 Baldwin County, Alabama happening  60.5\n 4  1003 Baldwin County, Alabama human      44.7\n 5  1005 Barbour County, Alabama happening  68.1\n 6  1005 Barbour County, Alabama human      50.5\n 7  1007 Bibb County, Alabama    happening  57.6\n 8  1007 Bibb County, Alabama    human      42.6\n 9  1009 Blount County, Alabama  happening  52.5\n10  1009 Blount County, Alabama  human      41.5\n# … with 6,274 more rows"
  },
  {
    "objectID": "DANL310_lab1a.html#q1a.",
    "href": "DANL310_lab1a.html#q1a.",
    "title": "R Lab - Map Visualization",
    "section": "Q1a.",
    "text": "Q1a.\n\nFilter climate_opinion_long, so that climate_opinion_long has only estimated percentage who think that global warming is caused mostly by human activities.\nThen join the two data.frames socviz::county_map and the resulting data.frame above.\n\n\nclimate_opinion_long <- climate_opinion_long %>% \n  filter(belief == 'human')\n\ncounty_map <- county_map\ncounty_map$id <- as.integer(county_map$id)\ncounty_full <- left_join(county_map, climate_opinion_long)\n\nJoining, by = \"id\""
  },
  {
    "objectID": "DANL310_lab1a.html#q1b.",
    "href": "DANL310_lab1a.html#q1b.",
    "title": "R Lab - Map Visualization",
    "section": "Q1b.",
    "text": "Q1b.\n\nReplicate the following map.\n\nDo not use coord_map(projection = \"albers\", lat0 = 39, lat1 = 45).\n\n\n\np1 <- ggplot(data = county_full) + \n  geom_polygon(mapping = aes(x = long, y = lat,\n                             group = group, \n                             fill = perc),\n               color = \"grey60\", \n               linewidth = 0.1) \n\n\np2 <- p1 + scale_fill_gradient2( \n  low = '#2E74C0',  \n  high = '#CB454A',  \n  mid = 'white', # transparent white\n  na.value = \"grey80\",\n  midpoint = 50,\n  breaks = c(quantile(climate_opinion_long$perc, 0, na.rm = T),\n             quantile(climate_opinion_long$perc, .25, na.rm = T),\n             quantile(climate_opinion_long$perc, .5, na.rm = T),\n             quantile(climate_opinion_long$perc, .75, na.rm = T),\n             quantile(climate_opinion_long$perc, 1, na.rm = T)),\n  labels = c(paste(round(quantile(climate_opinion_long$perc, 0, na.rm = T), 1),\"\\n(Min)\"),\n             paste(round(quantile(climate_opinion_long$perc, .25, na.rm = T), 1),\"\\n(25th)\"),\n             paste(round(quantile(climate_opinion_long$perc, .5, na.rm = T), 1),\"\\n(50th)\"),\n             paste(round(quantile(climate_opinion_long$perc, .75, na.rm = T), 1),\"\\n(75th)\"),\n             paste(round(quantile(climate_opinion_long$perc, 1, na.rm = T), 1),\"\\n(Max)\")\n  ),\n  guide = guide_colorbar( direction = \"horizontal\",\n                          barwidth = 25,\n                          title.vjust = 1 )\n) \n\np <- p2 + labs(fill = \"Percent\\nBelief\", title = \"U.S. Climate Opinion, 2021\",\n               caption = \"Sources: Yale Program on Climate Change Communication\\n(https://climatecommunication.yale.edu/visualizations-data/ycom-us/)\") +\n  theme_map() +\n  theme(plot.margin = unit( c(1, 1, 3.85, 0.5), \"cm\"),\n        legend.position = c(0.5, -.15),\n        legend.justification = c(.5,.5),\n        strip.background = element_rect( colour = \"black\",\n                                         fill = \"white\",\n                                         color = \"grey80\" )\n  ) +\n  guides(fill = guide_colourbar(direction = \"horizontal\", barwidth = 25,\n                                title.vjust = 1))\np"
  },
  {
    "objectID": "DANL200_lab1a.html",
    "href": "DANL200_lab1a.html",
    "title": "R Lab 1 - ggplot visualization",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n# install.packages(\"hexbin\")   # if you do not have the \"hexbin\" package\nlibrary(hexbin)"
  },
  {
    "objectID": "DANL200_lab1a.html#q1a",
    "href": "DANL200_lab1a.html#q1a",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1a",
    "text": "Q1a\nRead the data file, bikeshare_cleaned.csv, as the data.frame object with the name, bikeshare, using (1) the read_csv() function and (2) its URL, https://bcdanl.github.io/data/bikeshare_cleaned.csv.\n\nurl <- 'https://bcdanl.github.io/data/bikeshare_cleaned.csv'\nbikeshare <- read_csv(url)\n\nRows: 17376 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): month, date, wkday, seasons, weather_cond\ndbl (7): cnt, year, hr, holiday, temp, hum, windspeed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbikeshare\n\n# A tibble: 17,376 × 12\n     cnt  year month date     hr wkday    holiday seasons weather…¹   temp   hum\n   <dbl> <dbl> <chr> <chr> <dbl> <chr>      <dbl> <chr>   <chr>      <dbl> <dbl>\n 1    16  2011 01    01        0 saturday       0 spring  Clear or… -1.33  0.947\n 2    40  2011 01    01        1 saturday       0 spring  Clear or… -1.44  0.896\n 3    32  2011 01    01        2 saturday       0 spring  Clear or… -1.44  0.896\n 4    13  2011 01    01        3 saturday       0 spring  Clear or… -1.33  0.636\n 5     1  2011 01    01        4 saturday       0 spring  Clear or… -1.33  0.636\n 6     1  2011 01    01        5 saturday       0 spring  Mist or … -1.33  0.636\n 7     2  2011 01    01        6 saturday       0 spring  Clear or… -1.44  0.896\n 8     3  2011 01    01        7 saturday       0 spring  Clear or… -1.54  1.21 \n 9     8  2011 01    01        8 saturday       0 spring  Clear or… -1.33  0.636\n10    14  2011 01    01        9 saturday       0 spring  Clear or… -0.919 0.688\n# … with 17,366 more rows, 1 more variable: windspeed <dbl>, and abbreviated\n#   variable name ¹​weather_cond\n\n\n\nskim(bikeshare)\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nbikeshare\n\n\n\n\nNumber of rows\n\n\n17376\n\n\n\n\nNumber of columns\n\n\n12\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\ncharacter\n\n\n5\n\n\n\n\nnumeric\n\n\n7\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nempty\n\n\nn_unique\n\n\nwhitespace\n\n\n\n\n\n\nmonth\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n12\n\n\n0\n\n\n\n\ndate\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n31\n\n\n0\n\n\n\n\nwkday\n\n\n0\n\n\n1\n\n\n6\n\n\n9\n\n\n0\n\n\n7\n\n\n0\n\n\n\n\nseasons\n\n\n0\n\n\n1\n\n\n4\n\n\n6\n\n\n0\n\n\n4\n\n\n0\n\n\n\n\nweather_cond\n\n\n0\n\n\n1\n\n\n14\n\n\n24\n\n\n0\n\n\n3\n\n\n0\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\ncnt\n\n\n0\n\n\n1\n\n\n189.48\n\n\n181.40\n\n\n1.00\n\n\n40.00\n\n\n142.00\n\n\n281.00\n\n\n977.00\n\n\n▇▃▁▁▁\n\n\n\n\nyear\n\n\n0\n\n\n1\n\n\n2011.50\n\n\n0.50\n\n\n2011.00\n\n\n2011.00\n\n\n2012.00\n\n\n2012.00\n\n\n2012.00\n\n\n▇▁▁▁▇\n\n\n\n\nhr\n\n\n0\n\n\n1\n\n\n11.55\n\n\n6.91\n\n\n0.00\n\n\n6.00\n\n\n12.00\n\n\n18.00\n\n\n23.00\n\n\n▇▇▆▇▇\n\n\n\n\nholiday\n\n\n0\n\n\n1\n\n\n0.03\n\n\n0.17\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n▇▁▁▁▁\n\n\n\n\ntemp\n\n\n0\n\n\n1\n\n\n0.00\n\n\n1.00\n\n\n-2.48\n\n\n-0.82\n\n\n0.02\n\n\n0.85\n\n\n2.61\n\n\n▂▇▇▇▁\n\n\n\n\nhum\n\n\n0\n\n\n1\n\n\n0.00\n\n\n1.00\n\n\n-3.25\n\n\n-0.76\n\n\n0.01\n\n\n0.79\n\n\n1.93\n\n\n▁▃▇▇▆\n\n\n\n\nwindspeed\n\n\n0\n\n\n1\n\n\n0.00\n\n\n1.00\n\n\n-1.55\n\n\n-0.70\n\n\n0.03\n\n\n0.52\n\n\n5.40\n\n\n▇▆▂▁▁\n\n\n\n\n\n\ntable(bikeshare$wkday)\n\n\n   friday    monday  saturday    sunday  thursday   tuesday wednesday \n     2487      2478      2511      2502      2471      2453      2474 \n\ntable(bikeshare$month)\n\n\n  01   02   03   04   05   06   07   08   09   10   11   12 \n1426 1341 1473 1437 1488 1440 1488 1475 1437 1451 1437 1483 \n\ntable(bikeshare$seasons)\n\n\n  fall spring summer winter \n  4496   4239   4409   4232 \n\ntable(bikeshare$weather_cond)\n\n\n     Clear or Few Cloudy Light Snow or Light Rain           Mist or Cloudy \n                   11413                     1419                     4544 \n\nprop.table( table( bikeshare$wkday ) )\n\n\n   friday    monday  saturday    sunday  thursday   tuesday wednesday \n0.1431285 0.1426105 0.1445097 0.1439917 0.1422076 0.1411717 0.1423803 \n\nprop.table( table( bikeshare$month ) )\n\n\n        01         02         03         04         05         06         07 \n0.08206722 0.07717541 0.08477210 0.08270028 0.08563536 0.08287293 0.08563536 \n        08         09         10         11         12 \n0.08488720 0.08270028 0.08350599 0.08270028 0.08534761 \n\nprop.table( table( bikeshare$seasons ) )\n\n\n     fall    spring    summer    winter \n0.2587477 0.2439572 0.2537408 0.2435543 \n\nprop.table( table( bikeshare$weather_cond ) )\n\n\n     Clear or Few Cloudy Light Snow or Light Rain           Mist or Cloudy \n              0.65682551               0.08166436               0.26151013 \n\n\nUse the data.frame bikeshare for the rest of questions in Question 1.\n\nDescription of variables in the data file, bikeshare_cleaned.csv\nThe data set, bikeshare_cleaned.csv, includes 17376 observations of hourly counts from 2011 to 2012 for bike rides (rentals) in Washington D.C.\n\ncnt: count of total bikes rented out\nyear: year\nmonth: month\ndate: date\nhr: hours\nwkday: week day\nholiday: holiday if holiday == 1; non-holiday otherwise\nseasons: season\nweather_cond: weather condition\ntemp: temperature, measured in standard deviations from average.\nhum: humidity, measured in standard deviations from average.\nwindspeed: wind speed, measured in standard deviations from average."
  },
  {
    "objectID": "DANL200_lab1a.html#q1b",
    "href": "DANL200_lab1a.html#q1b",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1b",
    "text": "Q1b\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of cnt.\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = cnt),\n                 binwidth = 5) \n\n\n\n\n\n\n\nThe distribution of cnt is right-skewed.\nThe most common values for cnt range from 0 to 50.\nNote: When visualizing a histogram, we should experiment on either bins (the number of bins) or binwidth (the width of a bin)."
  },
  {
    "objectID": "DANL200_lab1a.html#q1c",
    "href": "DANL200_lab1a.html#q1c",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1c",
    "text": "Q1c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of cnt by year and month.\n\n# density plot\nggplot(bikeshare) +\n  geom_density( aes(x = cnt, fill = month),\n                color = NA,\n                show.legend = F) +\n  facet_grid(month~year) \n\n# histogram\nggplot(bikeshare) +\n  geom_histogram( aes(x = cnt, fill = month),\n                  binwidth = 10,\n                  color = NA,\n                  show.legend = F) +\n  facet_grid(month~year) \n\n# boxplot\nggplot(bikeshare) +\n  geom_boxplot( aes(x = cnt, y = month, \n                    fill = month),\n                  show.legend = F) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverall, the demand for bike rentals tends to be higher in 2012 than in 2011.\nOverall, the demand for bike rentals tends to be lower in winter."
  },
  {
    "objectID": "DANL200_lab1a.html#q1d",
    "href": "DANL200_lab1a.html#q1d",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1d",
    "text": "Q1d\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of temp by year and month.\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = temp, fill = month),\n                  binwidth = .1,\n                 show.legend = F) +\n  geom_vline(xintercept = 0, color = 'red') + \n  facet_grid(month ~ year) \n\n\n\n\n\n\n\nWe observe the four seasons when it comes to temperature.\nJanuary tends to be the coldest and July is the hottest.\nThe distribution of temperature across months looks similar across years 2011-2012."
  },
  {
    "objectID": "DANL200_lab1a.html#q1e",
    "href": "DANL200_lab1a.html#q1e",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1e",
    "text": "Q1e\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of hum by year and month.\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = hum, fill = month),\n                binwidth = .1,\n                show.legend = F) +\n  geom_vline(xintercept = 0, color = 'red') + \n  facet_grid(month ~ year) \n\n\n\n\n\n\n\nIn years 2011-2012 in Washington D.C., May, August, and September tend to be more humid than other months.\nIn years 2011-2012 in Washington D.C., January and February tend to be less humid than other months.\nOverall, the distribution of humidity across months looks similar across years 2011-2012 in Washington D.C."
  },
  {
    "objectID": "DANL200_lab1a.html#q1f",
    "href": "DANL200_lab1a.html#q1f",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1f",
    "text": "Q1f\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of windspeed by year and month.\n\nggplot(bikeshare) +\n  geom_density(aes(x = windspeed)) +\n  geom_vline(xintercept = 0, color = 'red') + \n  facet_grid(month ~ year) \n\n\n\n\n\n\n\nOverall, the monthly distribution of wind speed looks similar across years 2011-2012 in Washington D.C.\nIn years 2011-2012 in Washington D.C., noticeably slow wind speed, -1.5, which is a deviation from the standardized mean of wind speed 0, is observed throughout all months."
  },
  {
    "objectID": "DANL200_lab1a.html#q1g",
    "href": "DANL200_lab1a.html#q1g",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1g",
    "text": "Q1g\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between temp and cnt.\n\nggplot(bikeshare,\n       aes(x = temp, y = cnt)) +\n  geom_hex() +\n  geom_smooth(color = 'red') +\n  geom_smooth(method = lm) \n\n\n\n\n\n\n\ntemp and cnt are positively associated with each other.\nToo high temp (above 1.5) may lead to lower cnt."
  },
  {
    "objectID": "DANL200_lab1a.html#q1h",
    "href": "DANL200_lab1a.html#q1h",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1h",
    "text": "Q1h\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between temp and cnt by year and month.\n\nggplot(bikeshare, aes(x = temp, y = cnt )) +\n    geom_point(alpha = .1)  +\n    geom_smooth(color = \"red3\",\n                fill = \"orchid\",\n                method = lm)  +\n    geom_smooth(color = \"royalblue\",\n                fill = \"orchid\")  +\n  facet_grid(month~year)\n\n\n\n\n\n\n\nOverall, temp and cnt are positively associated with each other.\nIn June, July, and August, the association between temp and cnt switches from positive to negative at which temp is around 1.5."
  },
  {
    "objectID": "DANL200_lab1a.html#q1i",
    "href": "DANL200_lab1a.html#q1i",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1i",
    "text": "Q1i\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between weather_cond and cnt.\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = cnt),\n                 binwidth = 5) + \n  facet_grid(. ~ weather_cond) \n\nggplot(bikeshare) +\n  geom_density(aes(x = cnt)) + \n  facet_grid(. ~ weather_cond) \n\n\n\n\n\n\n\n\n\n\nIn 2012-2013 in Washington D.C., people rented out bikes more often when weather_cond is Clear or Few Cloudy.\nThe most common values for cnt are around 50 across all values of weather_cond.\nWhen weather_cond is Light Snow or Light Rain, people are more likely to rent less number of bikes."
  },
  {
    "objectID": "DANL200_lab1a.html#q1j",
    "href": "DANL200_lab1a.html#q1j",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1j",
    "text": "Q1j\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between weather_cond and cnt by hr.\n\nggplot(bikeshare) +\n  geom_density(aes(x = cnt, fill = as.factor(hr)),\n               color = NA,\n               show.legend = F) + \n  facet_grid(hr ~ weather_cond, scale = \"free_y\") \n\n\n\n\n\n\n\nThe values of cnt during commuting hours (hr 7:00 A.M.-8:59 A.M. and 5:00 P.M.-7:59 P.M.) are often larger than other commuting hours.\n\nIt implies that a shortage of rental bikes is more likely to happen during these hours."
  },
  {
    "objectID": "DANL200_lab1a.html#q1k",
    "href": "DANL200_lab1a.html#q1k",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1k",
    "text": "Q1k\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between wkday and cnt.\n\nggplot(bikeshare) +\n  geom_density(aes(x = cnt,\n                   fill = as.factor(wkday)),\n               color = NA,\n               show.legend = F) + \n  facet_grid(. ~ wkday)\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = cnt,\n                   fill = as.factor(wkday)),\n                 binwidth = 10,\n               color = NA,\n               show.legend = F) + \n  facet_grid(. ~ wkday)\n\n\nggplot(bikeshare,\n       aes(x = wkday, y = cnt)) +\n  geom_boxplot( aes(fill = wkday),\n                show.legend = F ) +\n  stat_summary(fun = mean)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of cnt is right-skewed, and looks similar across all values of wkday."
  },
  {
    "objectID": "DANL200_lab1a.html#q1l",
    "href": "DANL200_lab1a.html#q1l",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1l",
    "text": "Q1l\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between wkday and cnt by hr.\n\nggplot(bikeshare) +\n  geom_density(aes(x = cnt,\n                     fill = as.factor(hr)),\n               color = NA, \n               show.legend = F) + \n  facet_grid(hr~wkday, scale = \"free_y\") \n\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = cnt,\n                     fill = as.factor(hr)),\n                 binwidth = 10,\n               color = NA, \n               show.legend = F) + \n  facet_grid(hr~wkday, scale = \"free_y\") \n\n\nggplot(bikeshare) +\n  geom_boxplot(aes(x = cnt,\n                   y = as.factor(hr),\n                   fill = as.factor(hr)),\n               show.legend = F) + \n  facet_grid(.~wkday, scale = \"free_y\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuring hours from 10 to 15, people tend to rent out bikes more on Saturday and Sunday than on other week days.\nDuring the morning commuting hours, people tend to rent out bikes less on Saturday and Sunday than on other week days."
  },
  {
    "objectID": "DANL200_lab1a.html#q2a",
    "href": "DANL200_lab1a.html#q2a",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2a",
    "text": "Q2a\nRead the data file, NY_school_enrollment_socioecon.csv, as the data.frame object with the name, NY_school_enrollment_socioecon, using (1) the read_csv() function and (2) its URL, https://bcdanl.github.io/data/NY_school_enrollment_socioecon.csv.\n\nurl2 <- 'https://bcdanl.github.io/data/NY_school_enrollment_socioecon.csv'\nNY_school_enrollment_socioecon <- read_csv(url2)\nNY_school_enrollment_socioecon\n\n\nWe can view the data.frame NY_school_enrollment_socioecon:\n\n\n\n# A tibble: 372 × 310\n    fips  year county_name pincp pover…¹ pover…² pover…³ pover…⁴ pover…⁵ pover…⁶\n   <dbl> <dbl> <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 36001  2015 Albany      55120    30.9    13.9     8.2     4.6    30.3    13.2\n 2 36001  2016 Albany      55126    28      14.4     8.3     4.1    28.2    13.9\n 3 36001  2017 Albany      58814    28      14       8.1     3.7    26.5    13.7\n 4 36001  2018 Albany      59547    26.4    12.8     8.5     3.8    23.6    12.9\n 5 36001  2019 Albany      61876    27.1    11.9     7.6     4.1    23.6    12  \n 6 36001  2020 Albany      66632    27.4    12.9     8.3     4.1    24.1    11.9\n 7 36003  2015 Allegany    32205    26.5    14.9    10.4     3.9    23      12.9\n 8 36003  2016 Allegany    32417    25.9    14.4    11.2     4      23.2    11.3\n 9 36003  2017 Allegany    34001    25.8    13.9    11.4     4.1    23.9    11.1\n10 36003  2018 Allegany    34553    26.9    14.3    12       4.9    24      11.2\n# … with 362 more rows, 300 more variables: poverty_male_assc <dbl>,\n#   poverty_male_bachelor_or_higher <dbl>, poverty_female_less_than_hs <dbl>,\n#   poverty_female_hs <dbl>, poverty_female_assc <dbl>,\n#   poverty_female_bachelor_or_higher <dbl>, c01_001 <dbl>, c01_002 <dbl>,\n#   c01_003 <dbl>, c01_004 <dbl>, c01_005 <dbl>, c01_006 <dbl>, c01_007 <dbl>,\n#   c01_008 <dbl>, c01_009 <dbl>, c01_010 <dbl>, c01_011 <dbl>, c01_012 <dbl>,\n#   c01_013 <dbl>, c01_014 <dbl>, c01_015 <dbl>, c01_016 <dbl>, …\n\n\nFor description of variables in NY_school_enrollment_socioecon, refer to the file, ny_school_enrollment_socioecon_description.zip, which is in the Files section in our Canvas web-page. (I recommend you to extract the zip file, and then read the file, ny_school_enrollment_socioecon_description.csv, using Excel or Numbers.)\n\nHere are some details about the data.frame, NY_school_enrollment_socioecon:\nThe geographic and time units of observation (row) in the data.frame, NY_school_enrollment_socioecon, are New York county and year.\n\n\n\n\n\n \n  \n    FIPS \n    year \n    county_name \n    pincp \n    c01_001 \n    c02_002 \n  \n \n\n  \n    36001 \n    2015 \n    Albany \n    55793 \n    84463 \n    4.7 \n  \n\n\n\n\n\n\nFor example, the observation above means that in Albany county in year 2015 …\n\nAverage personal income of people is $55,793.\nPopulation 3 years and over enrolled in school is 84,463.\nPercent of population 3 years and over enrolled in nursery school and preschool is 4.7%.\n\nThe following is sample observations from Bronx and Livingston counties:\n\n\n\n\n\n\n\n\n\n\n\nThe following describes the variables:\n\nc01_010: Total!!Population enrolled in college or graduate school\nSo, c01_010 is total population enrolled in college or graduate school;\nc02_010: Percent!!Population enrolled in college or graduate school\nSo, c02_010 is a percent of total population enrolled in college or graduate school;\nIn which county is more likely for a person to be enrolled in a college or graduate school?\n\nA county’s college enrollment level can be represented by an overall tendency of that county’s residents to be enrolled in college (as long as we are interested in analyzing how a human behaves overall!).\nThe size of a county’s population enrolled in college or graduate school (c01_010) may not be appropriate to represent a county’s college enrollment level.\n\nA county’s larger size of population enrolled in college does not necessarily mean people in people in that county are likely to be enrolled in college.\n\nConsider the following example:\n\n\n\n\n\n \n  \n    County \n    Total.Population \n    Bachelor.s.Degree \n    High.School \n    Percent.of.Bachelor.s.Degree \n    Percent.of.High.School \n  \n \n\n  \n    A \n    100,000 \n    1,000 \n    99,000 \n    1.0% \n    99.0% \n  \n  \n    B \n    1,000 \n    999 \n    1 \n    99.9% \n    0.1% \n  \n\n\n\n\n\n\nAlthough County A has the larger number of people that have bachelor’s degrees than County B, it is more appropriate to say that people in County B have a higher college enrollment than people in County A.\nThis is because the overall tendency of County B’s people to attend college is stronger than that of County A’s people.\nSimilarly, to represent a standard of living of people in a country, we do not use a country’s gross domestic product (GDP) but its GDP per capita (GDP per capita is GDP devided by population).\n\nFor example, China records the second largest GDP in the world as of now. However, World Bank still considers China a middle-income country, because of its relatively low level of GDP per capita."
  },
  {
    "objectID": "DANL200_lab1a.html#q2b",
    "href": "DANL200_lab1a.html#q2b",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2b",
    "text": "Q2b\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between college enrollment and educational attainment of population 45 to 64 years, and how such relationship varies by the type (public or private) of colleges.\n\nTo represent a level of educational attainment of population 45 to 64 years in a county, I choose variable, 100 - d01_024, a percent of population 45 to 64 years without bachelor’s degree.\nTo represent a level of college enrollment of population in a county, I choose variable, c02_010, a percent of population enrolled in college or graduate school.\nTo represent a level of public college’s enrollment of population in a county, I choose variable, c04_010, a percent of population enrolled in public college or graduate school.\nTo represent a level of private college’s enrollment of population in a county, I choose variable, c06_010, a percent of population enrolled in private college or graduate school.\nThe following ggplot describes the relationship between college enrollment and educational attainment of population 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon,\n       aes(x = 100 - d01_024, y = c02_010)) +\n  geom_hex()  +\n  geom_smooth(method = lm)  +\n  geom_smooth(color = 'red') +\n  coord_fixed()\n\n\n\n\n\n\n\nThe percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in college or graduate school.\nThe following ggplot describes the relationship between college enrollment in public schools and educational attainment of population 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon,\n       aes(x = 100 - d01_024, y = c04_010)) +\n  geom_hex()  +\n  geom_smooth(method = lm)  +\n  geom_smooth(color = 'red') +\n  coord_fixed()\n\n\n\n\n\n\n\nThe following ggplot describes the relationship between college enrollment in private schools and educational attainment of population 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon,\n       aes(x = 100 - d01_024, y = c06_010)) +\n  geom_hex()  +\n  geom_smooth(method = lm)  +\n  geom_smooth(color = 'red') +\n  coord_fixed()\n\n\n\n\n\n\n\nThe percentage of population 45 to 64 years without Bachelor’s degree is positively associated with the percentage of population enrolled in public college or graduate school.\nThe percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in private college or graduate school."
  },
  {
    "objectID": "DANL200_lab1a.html#q2c",
    "href": "DANL200_lab1a.html#q2c",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2c",
    "text": "Q2c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe how the relationships described in Q3b vary by gender of population 45 to 64 years.\n\nTo represent a level of educational attainment of male population 45 to 64 years in a county, I choose variable, 100 - d03_024, a percent of male population 45 to 64 years without bachelor’s degree.\nTo represent a level of educational attainment of female population 45 to 64 years in a county, I choose variable, 100 - d05_024, a percent of female population 45 to 64 years without bachelor’s degree.\nThe following ggplot describes the relationship between college enrollment and educational attainment of male/female populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100 - d03_024, y = c02_010),\n             color = 'blue')  +\n  geom_point(aes(x = 100 - d05_024, y = c02_010),\n             color = 'red')  +\n  geom_smooth(aes(x = 100 - d03_024, y = c02_010),\n              color = 'blue',  \n              method = lm)  +\n  geom_smooth(aes(x = 100 - d05_024, y = c02_010),\n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of population 45 to 64 years, the percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in college or graduate school.\nGiven the same level of the percentage of population enrolled in college or graduate school, male population 45 to 64 years without bachelor’s degree tends to be higher than female population 45 to 64 years without bachelor’s degree.\nThe following ggplot describes the relationship between public college enrollment and educational attainment of male/female populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100 - d03_024, y = c04_010),\n             color = 'blue')  +\n  geom_point(aes(x = 100 - d05_024, y = c04_010),\n             color = 'red')  +\n  geom_smooth(aes(x = 100 - d03_024, y = c04_010),\n              color = 'blue',  \n              method = lm)  +\n  geom_smooth(aes(x = 100 - d05_024, y = c04_010),\n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of population 45 to 64 years, the percentage of population 45 to 64 years without Bachelor’s degree is positively associated with the percentage of population enrolled in public college or graduate school.\nFor the same level of the percentage of population 45 to 64 years without Bachelor’s degree, a level of educational attainment of female population 45 to 64 years may be associated with a higher level of public college enrollment than that of male population 45 to 64 years.\nGiven the same level of the percentage of population enrolled in public college or graduate school, male population 45 to 64 years without bachelor’s degree tends to be higher than female population 45 to 64 years without bachelor’s degree.\nThe following ggplot describes the relationship between private college enrollment and educational attainment of male/female populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100 - d03_024, y = c06_010),\n             color = 'blue')  +\n  geom_point(aes(x = 100 - d05_024, y = c06_010),\n             color = 'red')  +\n  geom_smooth(aes(x = 100 - d03_024, y = c06_010),\n              color = 'blue',  \n              method = lm)  +\n  geom_smooth(aes(x = 100 - d05_024, y = c06_010),\n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of population 45 to 64 years, the percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in private college or graduate school.\nGiven the same level of the percentage of population enrolled in private college or graduate school, male population 45 to 64 years without bachelor’s degree tends to be higher than female population 45 to 64 years without bachelor’s degree."
  },
  {
    "objectID": "DANL200_lab1a.html#q2d",
    "href": "DANL200_lab1a.html#q2d",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2d",
    "text": "Q2d\nProvide both (1) ggplot codes and (2) a couple of sentences to describe how the relationships described in Q3b vary by gender of college enrollment.\n\nTo represent a level of male college enrollment of population in a county, I choose variable, c02_011, a percent of male population enrolled in college or graduate school.\nTo represent a level of female college enrollment of population in a county, I choose variable, c02_012, a percent of female population enrolled in college or graduate school.\nThe following ggplot describes the relationship between male/female college enrollment and educational attainment of populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100-d01_024, y = c02_011), \n             color = 'blue')  +\n  geom_point(aes(x = 100-d01_024, y = c02_012), \n             color = 'red')  +\n  geom_smooth(aes(x = 100-d01_024, y = c02_011), \n              color = 'blue',\n              method = lm) +\n  geom_smooth(aes(x = 100-d01_024, y = c02_012), \n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of college enrollment, the percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in college or graduate school.\nFor the same level of the percentage of population 45 to 64 years without Bachelor’s degree, a level of educational attainment of population 45 to 64 years may be associated with a higher level of female college enrollment than that of male college enrollment.\nGiven the same level of the percentage of population 45 to 64 years without bachelor’s degree, male population enrolled in college or graduate schools tends to be lower than female population enrolled in college or graduate schools.\nTo represent a level of male population in a county that are enrolled in public colleges, I choose variable, c04_011, a percent of male population enrolled in public college or graduate school.\nTo represent a level of female population in a county that are enrolled in public colleges, I choose variable, c04_012, a percent of female population enrolled in public college or graduate school.\nThe following ggplot describes the relationship between male/female public college enrollment and educational attainment of populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100-d01_024, y = c04_011), \n             color = 'blue')  +\n  geom_point(aes(x = 100-d01_024, y = c04_012), \n             color = 'red')  +\n  geom_smooth(aes(x = 100-d01_024, y = c04_011), \n              color = 'blue',\n              method = lm) +\n  geom_smooth(aes(x = 100-d01_024, y = c04_012), \n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nTo represent a level of male population in a county that are enrolled in private colleges, I choose variable, c06_011, a percent of male population enrolled in private college or graduate school.\nTo represent a level of male population in a county that are enrolled in private colleges, I choose variable, c06_012, a percent of female population enrolled in private college or graduate school.\nThe following ggplot describes the relationship between male/female public college enrollment and educational attainment of populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100-d01_024, y = c06_011), \n             color = 'blue')  +\n  geom_point(aes(x = 100-d01_024, y = c06_012), \n             color = 'red')  +\n  geom_smooth(aes(x = 100-d01_024, y = c06_011), \n              color = 'blue',\n              method = lm) +\n  geom_smooth(aes(x = 100-d01_024, y = c06_012), \n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of college enrollment, the percentage of population 45 to 64 years without Bachelor’s degree is positively associated with the percentage of population enrolled in public college or graduate school.\nRegardless of a gender of college enrollment, the percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in private college or graduate school."
  },
  {
    "objectID": "DANL200_hw1a.html",
    "href": "DANL200_hw1a.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "",
    "text": "Step 1. Download the compressed ZIP file, ny_colleges.zip, from the Homework Assignment 1 in our Canvas.\nStep 2. Extract the file, ny_colleges.zip, so that you can access the file, NY_colleges.csv, for Homework Assignment 1.\n\n\n\n\n\n\n\n(1) median debt of college students,\n\n\nnet price of college\n\n\nnumber of college students,\n\n\ncollege majors\n\n\n\nwith a variety of segmentation.\n\nA description of each variable in NY_colleges.csv is provided in ny_colleges.yaml, which we can open in RStudio or other text editors.\nA description of values of each variable in NY_colleges.csv is provided in columns, VARIABLE_NAME, VALUE, and LABEL, in ny_colleges_vars.xlsx, which we can open in Microsoft Excel.\nUsing Ctrl + F (cmd + F for mac users) would be useful to find the variable description.\nI recommend you to copy and paste the description of the variable you use in your R code to your R script with a comment (# …).\nFrom the file, ny_colleges_vars.xlsx, we can find that\n\nCIP**BACHL, whose value is either 0, 1, or 2, indicates whether a university offers a bachelor’s degree in SOME MAJOR.\n\n0: Program not offered\n1: Program offered\n2: Program offered through an exclusively distance-education program"
  },
  {
    "objectID": "DANL200_hw1a.html#q1a",
    "href": "DANL200_hw1a.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1a",
    "text": "Q1a\nRead the data file, NY_colleges.csv, as the data.frame object with the name, NY_colleges, using (1) the read_csv() function and (2) the absolute path name of the file NY_colleges.csv from your local hard disk drive in your laptop.\n\n# The following path is from Byeong-Hak's local hard disk drive.\npath <- \"/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/ny_colleges/NY_colleges.csv\"  \n\nNY_colleges <- read_csv(path)"
  },
  {
    "objectID": "DANL200_hw1a.html#q1b",
    "href": "DANL200_hw1a.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1b",
    "text": "Q1b\nWhat are the mean, median1, minimum, maximum, and standard deviation for each of the following variables?\n\n\naverage net price of public institution;\n\n\naverage net price of private institution;\n\n\nmedian debt for students who have completed;\n\n\nmedian debt for students who have not completed.\n\n\n\nlibrary(skimr)\nskim(NY_colleges$NPT4_PUB) # it does not give us a meaningful summary.\n\n\nData summary\n\n\nName\nNY_colleges$NPT4_PUB\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndata\n0\n1\n4\n5\n0\n422\n0\n\n\n\n\n                           # this is because NPT4_PUB is character type.\n\n# as.numeric() can convert any variable type into a numeric type!\nNY_colleges$NPT4_PUB <- as.numeric( NY_colleges$NPT4_PUB ) \nNY_colleges$NPT4_PRIV <- as.numeric( NY_colleges$NPT4_PRIV )\n\n\nAfter converting the variables into numeric types, we can get the summary statistics for NPT4_PUB and NPT4_PRIV.\n\n\nskim(NY_colleges$NPT4_PUB)\n\n\nData summary\n\n\nName\nNY_colleges$NPT4_PUB\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n4347\n0.09\n11809.64\n5126.4\n1814\n6476\n13412\n15926\n20616\n▅▃▃▇▃\n\n\n\n\nskim(NY_colleges$NPT4_PRIV)\n\n\nData summary\n\n\nName\nNY_colleges$NPT4_PRIV\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n3209\n0.33\n19859\n9714.22\n1053\n12001\n20085\n26628.5\n54902\n▆▇▇▂▁\n\n\n\n\n\n\nThe following is the summary statistics for GRAD_DEBT_MDN and WDRAW_DEBT_MDN.\n\n\nskim(NY_colleges$GRAD_DEBT_MDN)\n\n\nData summary\n\n\nName\nNY_colleges$GRAD_DEBT_MDN\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n1545\n0.68\n17174.19\n6073.82\n2500\n13625\n17125\n21600\n39467\n▂▇▅▂▁\n\n\n\n\nskim(NY_colleges$WDRAW_DEBT_MDN)\n\n\nData summary\n\n\nName\nNY_colleges$WDRAW_DEBT_MD…\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n1555\n0.67\n6929.58\n2872.37\n1050\n5216\n6500\n8750\n25979\n▇▇▁▁▁"
  },
  {
    "objectID": "DANL200_hw1a.html#instruction-for-q1c-q1i",
    "href": "DANL200_hw1a.html#instruction-for-q1c-q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Instruction for Q1c-Q1i",
    "text": "Instruction for Q1c-Q1i\nFrom Q1c to Q1g, provide both (1) ggplot codes and (2) a couple of sentences to answer the questions."
  },
  {
    "objectID": "DANL200_hw1a.html#q1c",
    "href": "DANL200_hw1a.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1c",
    "text": "Q1c\nCompare between public institutions and private institutions in terms of the distribution of average net price.\n\nggplot( data = NY_colleges ) + \n  geom_freqpoly( mapping = aes( x = NPT4_PUB),\n                 color = \"blue\",\n                 bins = 50 ) +   # blue for public type\n  geom_freqpoly( mapping = aes( x = NPT4_PRIV),\n                 color = \"red\",\n                 bins = 50  ) +  # red for private type\n\n\n\n\n\n\n\nPrivate institutions tend to have a higher average net price than public ones."
  },
  {
    "objectID": "DANL200_hw1a.html#q1d",
    "href": "DANL200_hw1a.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1d",
    "text": "Q1d\nCompare between public institutions and private institutions in terms of the relationship between (1) average net price and (2) median debt for students who have completed.\n\nggplot( data = NY_colleges,\n        mapping = aes( y = GRAD_DEBT_MDN ) ) + \n  geom_point( mapping = aes( x = NPT4_PUB ),\n              color = \"blue\",\n              alpha = .2 ) +   # blue for public type+\n  geom_smooth( mapping = aes( x = NPT4_PUB ),\n               color = \"blue\", \n               method = lm ) +   # blue for public type\n  geom_point( mapping = aes( x = NPT4_PRIV),\n              color = \"red\",\n              alpha = .2  ) +  # red for private type\n  geom_smooth( mapping = aes( x = NPT4_PRIV),\n               color = \"red\", \n               method = lm ) \n\n\n\n\n\n\n\nThe association between average net price and median debt is stronger in public than in private.\nThe levels of average net price and median debt tend to be lower in public than in private."
  },
  {
    "objectID": "DANL200_hw1a.html#q1e",
    "href": "DANL200_hw1a.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1e",
    "text": "Q1e\nHow does the relationship between (1) average net price and (2) median debt for students who have completed vary by levels of family income?\n\nggplot( data = NY_colleges,\n        mapping = aes( y = LO_INC_DEBT_MDN ) ) + \n  geom_point( mapping = aes( x = NPT4_PUB ),\n              color = \"blue\",\n              alpha = .2 ) +   # blue for public type+\n  geom_smooth( mapping = aes( x = NPT4_PUB ),\n               color = \"blue\", \n               method = lm ) +   # blue for public type\n  geom_point( mapping = aes( x = NPT4_PRIV),\n              color = \"red\",\n              alpha = .2  ) +  # red for private type\n  geom_smooth( mapping = aes( x = NPT4_PRIV),\n               color = \"red\", \n               method = lm )\n\n\n\n\n\n\n\nggplot( data = NY_colleges,\n        mapping = aes( y = MD_INC_DEBT_MDN ) ) + \n  geom_point( mapping = aes( x = NPT4_PUB ),\n              color = \"blue\",\n              alpha = .2 ) +   # blue for public type+\n  geom_smooth( mapping = aes( x = NPT4_PUB ),\n               color = \"blue\", \n               method = lm ) +   # blue for public type\n  geom_point( mapping = aes( x = NPT4_PRIV),\n              color = \"red\",\n              alpha = .2  ) +  # red for private type\n  geom_smooth( mapping = aes( x = NPT4_PRIV),\n               color = \"red\", \n               method = lm )\n\n\n\n\n\n\n\nggplot( data = NY_colleges,\n        mapping = aes( y = HI_INC_DEBT_MDN ) ) + \n  geom_point( mapping = aes( x = NPT4_PUB ),\n              color = \"blue\",\n              alpha = .2 ) +   # blue for public type+\n  geom_smooth( mapping = aes( x = NPT4_PUB ),\n               color = \"blue\", \n               method = lm ) +   # blue for public type\n  geom_point( mapping = aes( x = NPT4_PRIV),\n              color = \"red\",\n              alpha = .2  ) +  # red for private type\n  geom_smooth( mapping = aes( x = NPT4_PRIV),\n               color = \"red\", \n               method = lm )\n\n\n\n\n\n\n\nRegardless of the levels of family income, the association between average net price and median debt is stronger in public than in private.\nRegardless of the levels of family income, the levels of average net price and median debt tend to be lower in public than in private."
  },
  {
    "objectID": "DANL200_hw1a.html#q1f",
    "href": "DANL200_hw1a.html#q1f",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1f",
    "text": "Q1f\nCompare between public institutions and private institutions in terms of the proportion of high-income students.\n\nggplot( data = NY_colleges ) + \n  geom_histogram( mapping = aes( x = HI_INC_YR4_N / OVERALL_YR4_N ),\n                  binwidth = .015 ) +\n  facet_wrap( .~ as.factor(CONTROL) )\n\n\n\n\n\n\n\nggplot( data = NY_colleges ) + \n  geom_density( mapping = aes( x = HI_INC_YR4_N / OVERALL_YR4_N )\n                   ) +\n  facet_wrap( .~ as.factor(CONTROL) )\n\n\n\n\n\n\n\nPrivate nonprofit tends to have the larger proportion of high-income students than public and private for-profit."
  },
  {
    "objectID": "DANL200_hw1a.html#q1g",
    "href": "DANL200_hw1a.html#q1g",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1g",
    "text": "Q1g\nDescribe how the number of students in a college varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services.\n\nggplot( data = NY_colleges) +\n  geom_density( mapping = aes(x = OVERALL_YR4_N ) ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\nggplot( data = NY_colleges) +\n  geom_histogram( mapping = aes(x = OVERALL_YR4_N ),\n                  binwidth = 200 ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\n\nggplot( data = NY_colleges ) +\n  geom_boxplot(mapping = aes(y = factor(CIP52BACHL),\n                             x = OVERALL_YR4_N,\n                             fill = factor(CIP52BACHL) ),\n                ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nLog transformation can be useful when a distribution of a variable of interest is highly skewed.\n\nIn such case, log transformation make the distribution more normal-ish.\n\n\n\nggplot( data = NY_colleges) +\n  geom_density( mapping = aes(x = log(OVERALL_YR4_N) ) ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\nggplot( data = NY_colleges) +\n  geom_histogram( mapping = aes(x = log(OVERALL_YR4_N) ) ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\n\nggplot( data = NY_colleges ) +\n  geom_boxplot(mapping = aes(y = factor(CIP52BACHL),\n                             x = log(OVERALL_YR4_N),\n                             fill = factor(CIP52BACHL) ),\n                ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nSchools that offer a business major tend to have more students than schools that do not offer one."
  },
  {
    "objectID": "DANL200_hw1a.html#q1h",
    "href": "DANL200_hw1a.html#q1h",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1h",
    "text": "Q1h\nDescribe how the median debt for students who have completed varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services.\n\nggplot( data = NY_colleges ) +\n  geom_density( mapping = aes(x = GRAD_DEBT_MDN ) ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\nggplot( data = NY_colleges) +\n  geom_histogram( mapping = aes(x = GRAD_DEBT_MDN ),\n                  binwidth = 200 ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\n\nggplot( data = NY_colleges ) +\n  geom_boxplot(mapping = aes(y = factor(CIP52BACHL),\n                             x = GRAD_DEBT_MDN,\n                             fill = factor(CIP52BACHL) ),\n               ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nThe levels of median debt tends to be larger for schools that offers a business major online exclusively."
  },
  {
    "objectID": "DANL200_hw1a.html#q1i",
    "href": "DANL200_hw1a.html#q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1i",
    "text": "Q1i\nDescribe how the yearly trend of the median debt for students varies by levels of family income.\n\nggplot( NY_colleges,\n              aes(x = as.factor(YEAR) ) ) +\n  geom_boxplot(fill = 'red',\n               aes( y = LO_INC_DEBT_MDN) ) +\n  ylim(c(0,30000))\n\n\n\n\n\n\n\nggplot( NY_colleges,\n              aes(x = as.factor(YEAR) ) ) +\n  geom_boxplot(fill = 'green',\n               aes( y = MD_INC_DEBT_MDN) ) +\n  ylim(c(0,30000)) \n\n\n\n\n\n\n\nggplot( NY_colleges,\n              aes(x = as.factor(YEAR) ) ) +\n  geom_boxplot(fill = 'blue',\n               aes( y = HI_INC_DEBT_MDN) ) +\n  ylim(c(0,30000)) \n\n\n\n\n\n\n\nThe median debt from low-income students tends to increase less than that from median-income and high-income students."
  },
  {
    "objectID": "DANL310_lab1q.html",
    "href": "DANL310_lab1q.html",
    "title": "R Lab - Map Visualization",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(socviz)"
  },
  {
    "objectID": "DANL310_lab1q.html#variable-description",
    "href": "DANL310_lab1q.html#variable-description",
    "title": "R Lab - Map Visualization",
    "section": "Variable Description",
    "text": "Variable Description\n\nbelief:\n\nhuman: Estimated percentage who think that global warming is caused mostly by human activities.\nhappening: Estimated percentage who think that global warming is happening.\n\n\n\nclimate_opinion_long <- read_csv(\n  'https://bcdanl.github.io/data/climate_opinion_2021.csv')\n\nRows: 6284 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): GeoName, belief\ndbl (2): id, perc\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nclimate_opinion_long\n\n# A tibble: 6,284 × 4\n      id GeoName                 belief     perc\n   <dbl> <chr>                   <chr>     <dbl>\n 1  1001 Autauga County, Alabama happening  59.2\n 2  1001 Autauga County, Alabama human      45.5\n 3  1003 Baldwin County, Alabama happening  60.5\n 4  1003 Baldwin County, Alabama human      44.7\n 5  1005 Barbour County, Alabama happening  68.1\n 6  1005 Barbour County, Alabama human      50.5\n 7  1007 Bibb County, Alabama    happening  57.6\n 8  1007 Bibb County, Alabama    human      42.6\n 9  1009 Blount County, Alabama  happening  52.5\n10  1009 Blount County, Alabama  human      41.5\n# … with 6,274 more rows"
  },
  {
    "objectID": "DANL310_lab1q.html#q1a.",
    "href": "DANL310_lab1q.html#q1a.",
    "title": "R Lab - Map Visualization",
    "section": "Q1a.",
    "text": "Q1a.\n\nFilter climate_opinion_long, so that climate_opinion_long has only estimated percentage who think that global warming is caused mostly by human activities.\nThen join the two data.frames socviz::county_map and the resulting data.frame above.\n\n\n\nJoining, by = \"id\""
  },
  {
    "objectID": "DANL310_lab1q.html#q1b.",
    "href": "DANL310_lab1q.html#q1b.",
    "title": "R Lab - Map Visualization",
    "section": "Q1b.",
    "text": "Q1b.\n\nReplicate the following map.\n\nDo not use coord_map(projection = \"albers\", lat0 = 39, lat1 = 45)."
  },
  {
    "objectID": "DANL310_hw2q.html",
    "href": "DANL310_hw2q.html",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2",
    "section": "",
    "text": "Add a web-page of the team project proposal to your website. The tab menu to link the web-page must be provided."
  },
  {
    "objectID": "DANL310_hw2q.html#q2a.",
    "href": "DANL310_hw2q.html#q2a.",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2",
    "section": "Q2a.",
    "text": "Q2a.\nUse the following data.frame for Q2a, Q2b, and Q2c.\n\nhdi_corruption <- read_csv(\n  'https://bcdanl.github.io/data/hdi_corruption.csv')"
  },
  {
    "objectID": "DANL310_hw2q.html#q2b",
    "href": "DANL310_hw2q.html#q2b",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\n\n# path name for the labor_supply.csv file in the local drive:\npath <- '/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/labor_supply.csv'\n\ncps_labor <- read_csv(path)\n\n\nVariable description\n\nSEX: 1 if Male; 2 if Female; 9 if NIU (Not in universe)\nNCHLT5: Number of own children under age 5 in a household; 9 if 9+\nLABFORCE: 0 if NIU or members of the armed forces; 1 if not in the labor force; 2 if in the labor force.\nASECWT: sample weights"
  },
  {
    "objectID": "DANL310_hw2q.html#q2c",
    "href": "DANL310_hw2q.html#q2c",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2",
    "section": "Q2c",
    "text": "Q2c\n\nlibrary(ggcorrplot) # to create correlation heatmaps using ggcorrplot()\n\nbeer_mkt <- read_csv('https://bcdanl.github.io/data/beer_markets.csv')\n\n\nMake a correlation heat-map with variables that are either strongly correlated or promo-related.\n\n\n\n\n\n\n\n\n\n\n\nThen, make a correlation heat-map for NY markets with the same selection of variables.\n\n\n\n\n\n\n\n\n\n\nReferences:\n\nFundamentals of Data Visualization by Claus O. Wilke\nData Visualization: A practical introduction by Kieran Healy"
  },
  {
    "objectID": "DANL200_hw1q.html",
    "href": "DANL200_hw1q.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "",
    "text": "Step 1. Download the compressed ZIP file, ny_colleges.zip, from the Homework Assignment 1 in our Canvas.\nStep 2. Extract the file, ny_colleges.zip, so that you can access the file, NY_colleges.csv, for Homework Assignment 1.\n\n\n\n\n\n\nNY_colleges.csv is university-year level data about\n\n\nmedian debt of college students,\n\n\nnet price of college\n\n\nnumber of college students,\n\n\ncollege majors\n\n\n\nwith a variety of segmentation.\n\nA description of each variable in NY_colleges.csv is provided in ny_colleges.yaml, which we can open in RStudio or other text editors.\nA description of values of each variable in NY_colleges.csv is provided in columns, VARIABLE_NAME, VALUE, and LABEL, in ny_colleges_vars.xlsx, which we can open in Microsoft Excel.\nUsing Ctrl + F (cmd + F for mac users) would be useful to find the variable description.\nI recommend you to copy and paste the description of the variable you use in your R code to your R script with a comment (# …).\nFrom the file, ny_colleges_vars.xlsx, we can find that\n\nCIP**BACHL, whose value is either 0, 1, or 2, indicates whether a university offers a bachelor’s degree in SOME MAJOR.\n\n0: Program not offered\n1: Program offered\n2: Program offered through an exclusively distance-education program"
  },
  {
    "objectID": "DANL200_hw1q.html#q1a",
    "href": "DANL200_hw1q.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1a",
    "text": "Q1a\nRead the data file, NY_colleges.csv, as the data.frame object with the name, NY_colleges, using (1) the read_csv() function and (2) the absolute path name of the file NY_colleges.csv from your local hard disk drive in your laptop."
  },
  {
    "objectID": "DANL200_hw1q.html#q1b",
    "href": "DANL200_hw1q.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1b",
    "text": "Q1b\nWhat are the mean, median1, minimum, maximum, and standard deviation for each of the following variables?\n\n\naverage net price of public institution;\n\n\naverage net price of private institution;\n\n\nmedian debt for students who have completed;\n\n\nmedian debt for students who have not completed."
  },
  {
    "objectID": "DANL200_hw1q.html#instruction-for-q1c-q1i",
    "href": "DANL200_hw1q.html#instruction-for-q1c-q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Instruction for Q1c-Q1i",
    "text": "Instruction for Q1c-Q1i\nFrom Q1c to Q1g, provide both (1) ggplot codes and (2) a couple of sentences to answer the questions."
  },
  {
    "objectID": "DANL200_hw1q.html#q1c",
    "href": "DANL200_hw1q.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1c",
    "text": "Q1c\nCompare between public institutions and private institutions in terms of the distribution of average net price."
  },
  {
    "objectID": "DANL200_hw1q.html#q1d",
    "href": "DANL200_hw1q.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1d",
    "text": "Q1d\nCompare between public institutions and private institutions in terms of the relationship between (1) average net price and (2) median debt for students who have completed."
  },
  {
    "objectID": "DANL200_hw1q.html#q1e",
    "href": "DANL200_hw1q.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1e",
    "text": "Q1e\nHow does the relationship between (1) average net price and (2) median debt for students who have completed vary by levels of family income?"
  },
  {
    "objectID": "DANL200_hw1q.html#q1f",
    "href": "DANL200_hw1q.html#q1f",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1f",
    "text": "Q1f\nCompare between public institutions and private institutions in terms of the proportion of high-income students."
  },
  {
    "objectID": "DANL200_hw1q.html#q1g",
    "href": "DANL200_hw1q.html#q1g",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1g",
    "text": "Q1g\nDescribe how the number of students in a college varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services."
  },
  {
    "objectID": "DANL200_hw1q.html#q1h",
    "href": "DANL200_hw1q.html#q1h",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1h",
    "text": "Q1h\nDescribe how the median debt for students who have completed varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services."
  },
  {
    "objectID": "DANL200_hw1q.html#q1i",
    "href": "DANL200_hw1q.html#q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1i",
    "text": "Q1i\nDescribe how the yearly trend of the median debt for students varies by levels of family income."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analytics at SUNY Geneseo",
    "section": "",
    "text": "data visualization and machine learning for business and economics with python and r\nweb projects with quarto, r markdown, and python notebook\ndata preparation and management with python, r, sql, and web-api\nweb scrapping and automation with python selenium\nversion control with git and github"
  },
  {
    "objectID": "DANL200_lab1q.html",
    "href": "DANL200_lab1q.html",
    "title": "R Lab 1 - ggplot visualization",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab1q.html#q1a",
    "href": "DANL200_lab1q.html#q1a",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1a",
    "text": "Q1a\nRead the data file, bikeshare_cleaned.csv, as the data.frame object with the name, bikeshare, using (1) the read_csv() function and (2) its URL, https://bcdanl.github.io/data/bikeshare_cleaned.csv.\n\n\n\nUse the data.frame bikeshare for the rest of questions in Question 1.\n\n\nDescription of variables in the data file, bikeshare_cleaned.csv\nThe data set, bikeshare_cleaned.csv, includes 17376 observations of hourly counts from 2011 to 2012 for bike rides (rentals) in Washington D.C.\n\ncnt: count of total bikes rented out\nyear: year\nmonth: month\ndate: date\nhr: hours\nwkday: week day\nholiday: holiday if holiday == 1; non-holiday otherwise\nseasons: season\nweather_cond: weather condition\ntemp: temperature, measured in standard deviations from average.\nhum: humidity, measured in standard deviations from average.\nwindspeed: wind speed, measured in standard deviations from average."
  },
  {
    "objectID": "DANL200_lab1q.html#q1b",
    "href": "DANL200_lab1q.html#q1b",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1b",
    "text": "Q1b\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of cnt."
  },
  {
    "objectID": "DANL200_lab1q.html#q1c",
    "href": "DANL200_lab1q.html#q1c",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1c",
    "text": "Q1c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of cnt by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1d",
    "href": "DANL200_lab1q.html#q1d",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1d",
    "text": "Q1d\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of temp by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1e",
    "href": "DANL200_lab1q.html#q1e",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1e",
    "text": "Q1e\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of hum by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1f",
    "href": "DANL200_lab1q.html#q1f",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1f",
    "text": "Q1f\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of windspeed by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1g",
    "href": "DANL200_lab1q.html#q1g",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1g",
    "text": "Q1g\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between temp and cnt."
  },
  {
    "objectID": "DANL200_lab1q.html#q1h",
    "href": "DANL200_lab1q.html#q1h",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1h",
    "text": "Q1h\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between temp and cnt by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1i",
    "href": "DANL200_lab1q.html#q1i",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1i",
    "text": "Q1i\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between weather_cond and cnt."
  },
  {
    "objectID": "DANL200_lab1q.html#q1j",
    "href": "DANL200_lab1q.html#q1j",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1j",
    "text": "Q1j\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between weather_cond and cnt by hr."
  },
  {
    "objectID": "DANL200_lab1q.html#q1k",
    "href": "DANL200_lab1q.html#q1k",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1k",
    "text": "Q1k\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between wkday and cnt."
  },
  {
    "objectID": "DANL200_lab1q.html#q1l",
    "href": "DANL200_lab1q.html#q1l",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1l",
    "text": "Q1l\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between wkday and cnt by hr."
  },
  {
    "objectID": "DANL200_lab1q.html#q2a",
    "href": "DANL200_lab1q.html#q2a",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2a",
    "text": "Q2a\nRead the data file, NY_school_enrollment_socioecon.csv, as the data.frame object with the name, NY_school_enrollment_socioecon, using (1) the read_csv() function and (2) its URL, https://bcdanl.github.io/data/NY_school_enrollment_socioecon.csv.\n\n\n\nFor description of variables in NY_school_enrollment_socioecon, refer to the file, ny_school_enrollment_socioecon_description.zip, which is in the Files section in our Canvas web-page. (I recommend you to extract the zip file, and then read the file, ny_school_enrollment_socioecon_description.csv, using Excel or Numbers.)\n\nHere are some details about the data.frame, NY_school_enrollment_socioecon:\nThe geographic and time units of observation (row) in the data.frame, NY_school_enrollment_socioecon, are New York county and year.\n\n\n\n\n\n \n  \n    FIPS \n    year \n    county_name \n    pincp \n    c01_001 \n    c02_002 \n  \n \n\n  \n    36001 \n    2015 \n    Albany \n    55793 \n    84463 \n    4.7 \n  \n\n\n\n\n\n\nFor example, the observation above means that in Albany county in year 2015 …\n\nAverage personal income of people is $55,793.\nPopulation 3 years and over enrolled in school is 84,463.\nPercent of population 3 years and over enrolled in nursery school and preschool is 4.7%.\n\nThe following is sample observations from Bronx and Livingston counties:\n\n\n\n\n\n\n\n\n\n\n\nThe following describes the variables:\n\nc01_010: Total!!Population enrolled in college or graduate school\nSo, c01_010 is total population enrolled in college or graduate school;\nc02_010: Percent!!Population enrolled in college or graduate school\nSo, c02_010 is a percent of total population enrolled in college or graduate school;\nIn which county is more likely for a person to be enrolled in a college or graduate school?\n\nA county’s college enrollment level can be represented by an overall tendency of that county’s residents to be enrolled in college (as long as we are interested in analyzing how human behaves overall).\nThe size of a county’s population enrolled in college or graduate school (c01_010) may not be appropriate to represent a county’s college enrollment level.\n\nA county’s larger size of population enrolled in college does not necessarily mean people in people in that county are likely to be enrolled in college.\n\nConsider the following example:\n\n\n\n\n\n \n  \n    County \n    Total.Population \n    Bachelor.s.Degree \n    High.School \n    Percent.of.Bachelor.s.Degree \n    Percent.of.High.School \n  \n \n\n  \n    A \n    100,000 \n    1,000 \n    99,000 \n    1.0% \n    99.0% \n  \n  \n    B \n    1,000 \n    999 \n    1 \n    99.9% \n    0.1% \n  \n\n\n\n\n\n\nAlthough County A has the larger number of people that have bachelor’s degrees than County B, it is more appropriate to say that people in County B have a higher college enrollment than people in County A.\nThis is because the overall tendency of County B’s people to attend college is stronger than that of County A’s people.\nSimilarly, to represent a standard of living of people in a country, we do not use a country’s gross domestic product (GDP) but its GDP per capita (GDP per capita is GDP devided by population).\n\nFor example, China records the second largest GDP in the world as of now. However, World Bank still considers China a middle-income country, because of its relatively low level of GDP per capita."
  },
  {
    "objectID": "DANL200_lab1q.html#q2b",
    "href": "DANL200_lab1q.html#q2b",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2b",
    "text": "Q2b\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between college enrollment and educational attainment of population 45 to 64 years, and how such relationship varies by the type (public or private) of colleges."
  },
  {
    "objectID": "DANL200_lab1q.html#q2c",
    "href": "DANL200_lab1q.html#q2c",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2c",
    "text": "Q2c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe how the relationships described in Q3b vary by gender of population 45 to 64 years."
  },
  {
    "objectID": "DANL200_lab1q.html#q2d",
    "href": "DANL200_lab1q.html#q2d",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2d",
    "text": "Q2d\nProvide both (1) ggplot codes and (2) a couple of sentences to describe how the relationships described in Q3b vary by gender of college enrollment."
  },
  {
    "objectID": "choe-beer-markets.html",
    "href": "choe-beer-markets.html",
    "title": "Beer Markets",
    "section": "",
    "text": "Loading Data and Packages\n\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(skimr)   # a better summary of data.frame\nlibrary(scales)  # scales for ggplot\nlibrary(ggthemes)  # additional ggplot themes\nlibrary(hrbrthemes) # additional ggplot themes and color pallets\nlibrary(lubridate)\nlibrary(ggridges)\nlibrary(stargazer)\ntheme_set(theme_ipsum())\n\nbeer_mkt <- read.csv('https://bcdanl.github.io/data/beer_markets.csv')"
  },
  {
    "objectID": "DANL200_hw2q.html",
    "href": "DANL200_hw2q.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "",
    "text": "Load R packages you need for Homework Assignment 2"
  },
  {
    "objectID": "DANL200_hw2q.html#q1a",
    "href": "DANL200_hw2q.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1a",
    "text": "Q1a\nProvide both ggplot code and a simple comment to describe the yearly trend of GHG emissions for each sector."
  },
  {
    "objectID": "DANL200_hw2q.html#q1b",
    "href": "DANL200_hw2q.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1b",
    "text": "Q1b\nProvide both ggplot code and a simple comment to describe the yearly trend of United States of America’s GHG emissions for each sector."
  },
  {
    "objectID": "DANL200_hw2q.html#q1c",
    "href": "DANL200_hw2q.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1c",
    "text": "Q1c\nFor each party, calculate the yearly percentage change in GHG emissions for each sector."
  },
  {
    "objectID": "DANL200_hw2q.html#q1d",
    "href": "DANL200_hw2q.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1d",
    "text": "Q1d\nWhich party has reduced GHG emissions most from 1990 level to 2017 level in terms of the percentage change in GHG emissions?"
  },
  {
    "objectID": "DANL200_hw2q.html#q1e",
    "href": "DANL200_hw2q.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1e",
    "text": "Q1e\nWhich sector has reduced GHG emissions most from 1990 level to 2017 level in terms of the percentage change in GHG emissions?"
  },
  {
    "objectID": "DANL200_hw2q.html#q1f",
    "href": "DANL200_hw2q.html#q1f",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1f",
    "text": "Q1f\nCompare between public institutions and private institutions in terms of the proportion of high-income students."
  },
  {
    "objectID": "DANL200_hw2q.html#q1g",
    "href": "DANL200_hw2q.html#q1g",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1g",
    "text": "Q1g\nDescribe how the number of students in a college varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services."
  },
  {
    "objectID": "DANL200_hw2q.html#q1h",
    "href": "DANL200_hw2q.html#q1h",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1h",
    "text": "Q1h\nDescribe how the median debt for students who have completed varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services."
  },
  {
    "objectID": "DANL200_hw2q.html#q1i",
    "href": "DANL200_hw2q.html#q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1i",
    "text": "Q1i\nDescribe how the yearly trend of the median debt for students varies by levels of family income."
  },
  {
    "objectID": "DANL200_hw2q.html#q2a",
    "href": "DANL200_hw2q.html#q2a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2a",
    "text": "Q2a\nHow many parties have provided or disbursed positive funding contributions to other countries or regions for their adaptation projects for every single year from 2011 to 2018?"
  },
  {
    "objectID": "DANL200_hw2q.html#q2b",
    "href": "DANL200_hw2q.html#q2b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\nFor each party, calculate the total funding contributions that were disbursed or provided for mitigation projects for each year."
  },
  {
    "objectID": "DANL200_hw2q.html#q2c",
    "href": "DANL200_hw2q.html#q2c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2c",
    "text": "Q2c\nFor each party, calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each year."
  },
  {
    "objectID": "DANL200_hw2q.html#q2d",
    "href": "DANL200_hw2q.html#q2d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2d",
    "text": "Q2d\nProvide both seaborn code and a simple comment to visualize the distribution of the ratio between adaptation contribution and mitigation contribution, which is calculated in Q2c."
  },
  {
    "objectID": "DANL200_hw2q.html#q2e",
    "href": "DANL200_hw2q.html#q2e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2e",
    "text": "Q2e\nVisualize how the distribution of Contribution varies by Type of support and Status."
  },
  {
    "objectID": "DANL210_lab1q_v3.html",
    "href": "DANL210_lab1q_v3.html",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "",
    "text": "import pandas as pd\nfrom skimpy import skim"
  },
  {
    "objectID": "DANL210_lab1q_v3.html#load-dataframe",
    "href": "DANL210_lab1q_v3.html#load-dataframe",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\ndf_ny = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_pop.csv')\ndf_ny.head(10)\n\n\n\n\n\n  \n    \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n  \n  \n    \n      0\n      36001\n      Albany\n      2015\n      55120\n      44478\n      204024\n    \n    \n      1\n      36001\n      Albany\n      2016\n      55126\n      45357\n      204003\n    \n    \n      2\n      36001\n      Albany\n      2017\n      58814\n      45589\n      204833\n    \n    \n      3\n      36001\n      Albany\n      2018\n      59547\n      45521\n      204509\n    \n    \n      4\n      36001\n      Albany\n      2019\n      61876\n      45150\n      204918\n    \n    \n      5\n      36001\n      Albany\n      2020\n      66632\n      44608\n      205082\n    \n    \n      6\n      36003\n      Allegany\n      2015\n      32205\n      7461\n      30568\n    \n    \n      7\n      36003\n      Allegany\n      2016\n      32417\n      7493\n      30449\n    \n    \n      8\n      36003\n      Allegany\n      2017\n      34001\n      7377\n      30331\n    \n    \n      9\n      36003\n      Allegany\n      2018\n      34553\n      7284\n      30155\n    \n  \n\n\n\n\n\nVariable Description\n\nFIPS: ID number for a county\npincp: average personal income in a county X in year Y\npop_18_24: population 18 to 24 years\npop_25_over: population 25 years and over\n\nSummarize DataFrame df_ny.\n\n\nskim(df_ny)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 372    │ │ int64       │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ string      │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━┓  │\n│ ┃ column_name     ┃ NA   ┃ NA %   ┃ mean     ┃ sd       ┃ p0      ┃ p25     ┃ p75      ┃ p100      ┃ hist    ┃  │\n│ ┡━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━┩  │\n│ │ FIPS            │    0 │      0 │    36000 │       36 │   36000 │   36000 │    36000 │     36000 │ █▇▇▇▇█  │  │\n│ │ year            │    0 │      0 │     2000 │      1.7 │    2000 │    2000 │     2000 │      2000 │ ██████  │  │\n│ │ pincp           │    0 │      0 │    50000 │    20000 │   32000 │   41000 │    52000 │    190000 │   █▁    │  │\n│ │ pop_18_24       │    0 │      0 │    31000 │    49000 │     320 │    4700 │    28000 │    250000 │   █▁    │  │\n│ │ pop_25_over     │    0 │      0 │   220000 │   380000 │    3500 │   35000 │   160000 │   1800000 │    █    │  │\n│ └─────────────────┴──────┴────────┴──────────┴──────────┴─────────┴─────────┴──────────┴───────────┴─────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name               ┃ NA      ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ county_name               │       0 │          0 │                            1 │                      380 │  │\n│ └───────────────────────────┴─────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\ndf_ny.describe()\n\n\n\n\n\n  \n    \n      \n      FIPS\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n  \n  \n    \n      count\n      372.000000\n      372.000000\n      372.000000\n      372.000000\n      3.720000e+02\n    \n    \n      mean\n      36062.000000\n      2017.500000\n      50102.801075\n      30601.064516\n      2.190541e+05\n    \n    \n      std\n      35.839264\n      1.710125\n      20291.643067\n      49283.641237\n      3.797498e+05\n    \n    \n      min\n      36001.000000\n      2015.000000\n      31831.000000\n      323.000000\n      3.485000e+03\n    \n    \n      25%\n      36031.000000\n      2016.000000\n      40612.250000\n      4675.750000\n      3.520200e+04\n    \n    \n      50%\n      36062.000000\n      2017.500000\n      45269.000000\n      9668.500000\n      5.985150e+04\n    \n    \n      75%\n      36093.000000\n      2019.000000\n      51998.000000\n      28099.250000\n      1.605870e+05\n    \n    \n      max\n      36123.000000\n      2020.000000\n      191220.000000\n      251964.000000\n      1.789355e+06\n    \n  \n\n\n\n\n\ndf_ny.groupby('year')['pincp'].describe()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      62.0\n      45449.580645\n      17342.754837\n      31831.0\n      37523.75\n      40790.5\n      47230.50\n      152793.0\n    \n    \n      2016\n      62.0\n      46474.064516\n      18544.679598\n      32417.0\n      38140.00\n      41283.0\n      48046.25\n      163112.0\n    \n    \n      2017\n      62.0\n      49078.548387\n      20323.712796\n      34001.0\n      40197.25\n      43617.5\n      50476.00\n      179655.0\n    \n    \n      2018\n      62.0\n      50400.483871\n      21110.934480\n      34553.0\n      40867.25\n      44751.5\n      52062.25\n      184539.0\n    \n    \n      2019\n      62.0\n      52551.370968\n      21381.379235\n      37131.0\n      42687.25\n      46772.0\n      54339.25\n      187213.0\n    \n    \n      2020\n      62.0\n      56662.758065\n      21384.820890\n      40840.0\n      47044.75\n      50577.5\n      58764.00\n      191220.0\n    \n  \n\n\n\n\n\ndf_ny.groupby('year')['pop_18_24'].describe()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      62.0\n      32025.887097\n      52851.482119\n      466.0\n      5039.00\n      9875.5\n      28233.25\n      251964.0\n    \n    \n      2016\n      62.0\n      31725.241935\n      51964.450784\n      382.0\n      5082.75\n      9900.5\n      28467.75\n      244788.0\n    \n    \n      2017\n      62.0\n      31195.403226\n      50824.225228\n      386.0\n      4903.50\n      9762.5\n      28317.00\n      236951.0\n    \n    \n      2018\n      62.0\n      30169.000000\n      48502.347023\n      406.0\n      4740.50\n      9577.5\n      27736.25\n      223707.0\n    \n    \n      2019\n      62.0\n      29546.500000\n      47141.642477\n      323.0\n      4550.25\n      9473.0\n      27574.00\n      215081.0\n    \n    \n      2020\n      62.0\n      28944.354839\n      45967.793981\n      369.0\n      4403.25\n      9394.0\n      27272.00\n      207966.0\n    \n  \n\n\n\n\n\ndf_ny.groupby('year')['pop_25_over'].describe()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      62.0\n      216706.370968\n      376492.870083\n      3546.0\n      35351.25\n      60028.0\n      159284.50\n      1735647.0\n    \n    \n      2016\n      62.0\n      217807.790323\n      379394.903521\n      3575.0\n      35209.50\n      60022.5\n      159512.25\n      1753695.0\n    \n    \n      2017\n      62.0\n      220335.629032\n      386192.777963\n      3604.0\n      35328.75\n      60108.0\n      160193.00\n      1789355.0\n    \n    \n      2018\n      62.0\n      219457.129032\n      383470.172157\n      3536.0\n      35492.25\n      59851.5\n      160939.50\n      1777281.0\n    \n    \n      2019\n      62.0\n      219869.951613\n      384159.431483\n      3567.0\n      35138.50\n      59689.0\n      161993.50\n      1780247.0\n    \n    \n      2020\n      62.0\n      220147.693548\n      384199.007544\n      3485.0\n      35528.00\n      59656.5\n      163294.50\n      1780524.0"
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1a",
    "href": "DANL210_lab1q_v3.html#q1a",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1a",
    "text": "Q1a\n\nUse .sort_values() to find the top 5 rich counties in NY for each year.\n\nDo not use .apply()"
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1b",
    "href": "DANL210_lab1q_v3.html#q1b",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1b",
    "text": "Q1b\n\nUse .rank() to find the top 5 rich counties in NY for each year.\n\nDo not use .apply()"
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1c",
    "href": "DANL210_lab1q_v3.html#q1c",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1c",
    "text": "Q1c\n\nUse apply() with a lambda function and .sort_values() to find the top 5 rich counties in NY for each year."
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1d",
    "href": "DANL210_lab1q_v3.html#q1d",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1d",
    "text": "Q1d\n\nWrite a function with def and .sort_values() that selects the top 5 pincp values.\nThen, use the defined function in apply() to find the top 5 rich counties in NY for each year."
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1e",
    "href": "DANL210_lab1q_v3.html#q1e",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1e",
    "text": "Q1e\n\nVisualize the yearly trend of the mean level of pincp."
  },
  {
    "objectID": "DANL200_lab2q.html",
    "href": "DANL200_lab2q.html",
    "title": "R Lab 2 - Data Transformation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab2q.html#variable-description",
    "href": "DANL200_lab2q.html#variable-description",
    "title": "R Lab 2 - Data Transformation",
    "section": "Variable Description",
    "text": "Variable Description\n\nid_user: a unique identification number for a Twitter user whom retweeted to a tweet with #climatechange or #globalwarming.\nid_city: a unique identification number for a city.\nFIPS: a unique identification number for a county.\n\nEach row represents an observation of a retweet to a tweet with #climatechange or #globalwarming.\nEach row includes a Twitter user’s geographic information at city or county levels (variables FIPS, county, city) as well as information about timing when a Twitter user retweeted (variables year, month, day, hour, minute, second)."
  },
  {
    "objectID": "DANL200_lab2q.html#q1a",
    "href": "DANL200_lab2q.html#q1a",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1a",
    "text": "Q1a\nHow many Twitter users retweeted on the date, January 1, 2017?"
  },
  {
    "objectID": "DANL200_lab2q.html#q1b",
    "href": "DANL200_lab2q.html#q1b",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1b",
    "text": "Q1b\nWhich city is with the third highest number of retweets on the date, December 1, 2017?"
  },
  {
    "objectID": "DANL200_lab2q.html#q1c",
    "href": "DANL200_lab2q.html#q1c",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1c",
    "text": "Q1c\nFor each year, find the top 5 Twitter users in NY state in terms of the number of retweets they made in NY state. In which city and county do these users live in?"
  },
  {
    "objectID": "DANL200_lab2q.html#q1d",
    "href": "DANL200_lab2q.html#q1d",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1d",
    "text": "Q1d\nSummarize the data set into the data frame with county and month levels of retweets with the following variables:\n\nFIPS, county, year, month;\nn_retweets: the number of retweets in year YYYY and month MM from county C.\n\nThe unique() or distinct() functions can be used to keep only unique/distinct rows from a data frame."
  },
  {
    "objectID": "DANL200_lab2q.html#q1e",
    "href": "DANL200_lab2q.html#q1e",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1e",
    "text": "Q1e\nDescribe the relationship between the number of retweets and county using ggplot. Make a simple comment on your plot."
  },
  {
    "objectID": "DANL200_lab2q.html#variable-description-1",
    "href": "DANL200_lab2q.html#variable-description-1",
    "title": "R Lab 2 - Data Transformation",
    "section": "Variable Description",
    "text": "Variable Description\nEach observation in beer_markets.csv is a household-level record for one transaction of beer.\n\n\nhh: an identifier of the household;\nX_purchase_desc: details on the purchased item;\nquantity: the number of items purchased;\nbrand: Bud Light, Busch Light, Coors Light, Miller Lite, or Natural Light;\nspent: total dollar value of purchase;\nbeer_floz: total volume of beer, in fluid ounces;\nprice_per_floz: price per fl.oz. (i.e., beer spent/beer floz);\ncontainer: the type of container;\npromo: Whether the item was promoted (coupon or otherwise);\nmarket: Scan-track market (or state if rural);\ndemographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher."
  },
  {
    "objectID": "DANL200_lab2q.html#q2a",
    "href": "DANL200_lab2q.html#q2a",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2a",
    "text": "Q2a\n\nFind the top 5 markets in terms of the total volume of beer.\nFind the top 5 markets in terms of the total volume of BUD LIGHT.\nFind the top 5 markets in terms of the total volume of BUSCH LIGHT.\nFind the top 5 markets in terms of the total volume of COORS LIGHT.\nFind the top 5 markets in terms of the total volume of MILLER LITE.\nFind the top 5 markets in terms of the total volume of NATURAL LIGHT."
  },
  {
    "objectID": "DANL200_lab2q.html#q2b",
    "href": "DANL200_lab2q.html#q2b",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2b",
    "text": "Q2b\n\nFor households that purchased BUD LIGHT, what fraction of households did purchase only BUD LIGHT?\nFor households that purchased BUSCH LIGHT, what fraction of households did purchase only BUSCH LIGHT?\nFor households that purchased COORS LIGHT, what fraction of households did purchase only COORS LIGHT?\nFor households that purchased MILLER LITE, what fraction of households did purchase only MILLER LITE?\nFor households that purchased NATURAL LIGHT, what fraction of households did purchase only NATURAL LIGHT?\nWhich beer brand does have the largest base of loyal consumers?"
  },
  {
    "objectID": "DANL200_lab2q.html#q2c",
    "href": "DANL200_lab2q.html#q2c",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2c",
    "text": "Q2c\n\nCalculate the number of beer transactions for each household.\nCalculate the fraction of each beer brand for each household."
  },
  {
    "objectID": "DANL310_hw1q.html",
    "href": "DANL310_hw1q.html",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "",
    "text": "Renovate your personal website on GitHub using Quarto.\n\nFAQ about Quarto for R Markdown users are provided below: https://quarto.org/docs/faq/rmarkdown.html\nA guide for creating a Quarto website is provided in the following webpage: https://quarto.org/docs/websites/."
  },
  {
    "objectID": "DANL310_hw1q.html#q2a.",
    "href": "DANL310_hw1q.html#q2a.",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2a.",
    "text": "Q2a.\nUse the following data.frame for Q2a, Q2b, and Q2c.\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')"
  },
  {
    "objectID": "DANL310_hw1q.html#q2b",
    "href": "DANL310_hw1q.html#q2b",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2b",
    "text": "Q2b"
  },
  {
    "objectID": "DANL310_hw1q.html#q2c",
    "href": "DANL310_hw1q.html#q2c",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2c",
    "text": "Q2c\nUse ggridges::geom_density_ridges() for Q2c."
  },
  {
    "objectID": "DANL310_hw1q.html#q2d",
    "href": "DANL310_hw1q.html#q2d",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2d",
    "text": "Q2d\nUse ggplot::mtcars for Q2d."
  },
  {
    "objectID": "DANL310_hw1q.html#q2e",
    "href": "DANL310_hw1q.html#q2e",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2e",
    "text": "Q2e\nUse the following data.frame for Q2e.\n\npopgrowth_df <- read_csv(\n  'https://bcdanl.github.io/data/popgrowth.csv')"
  },
  {
    "objectID": "DANL310_hw1q.html#q2f",
    "href": "DANL310_hw1q.html#q2f",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2f",
    "text": "Q2f\nUse the following data.frame for Q2f.\n\nmale_Aus <- read_csv(\n  'https://bcdanl.github.io/data/aus_athletics_male.csv')"
  },
  {
    "objectID": "DANL310_hw1q.html#q2g",
    "href": "DANL310_hw1q.html#q2g",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2g",
    "text": "Q2g\nUse the following data.frame for Q2g.\n\ntitanic <- read_csv(\n  'https://bcdanl.github.io/data/titanic_cleaned.csv')"
  },
  {
    "objectID": "DANL310_hw1q.html#q2h",
    "href": "DANL310_hw1q.html#q2h",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2h",
    "text": "Q2h\nUse the following data.frame for Q2h.\n\ncows_filtered <- read_csv(\n  'https://bcdanl.github.io/data/cows_filtered.csv')"
  },
  {
    "objectID": "DANL200_lab2a.html",
    "href": "DANL200_lab2a.html",
    "title": "R Lab 2 - Data Transformation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab2a.html#variable-description",
    "href": "DANL200_lab2a.html#variable-description",
    "title": "R Lab 2 - Data Transformation",
    "section": "Variable Description",
    "text": "Variable Description\n\nid_user: a unique identification number for a Twitter user whom retweeted to a tweet with #climatechange or #globalwarming.\nid_city: a unique identification number for a city.\nFIPS: a unique identification number for a county.\n\nEach row represents an observation of a retweet to a tweet with #climatechange or #globalwarming.\nEach row includes a Twitter user’s geographic information at city or county levels (variables FIPS, county, city) as well as information about timing when a Twitter user retweeted (variables year, month, day, hour, minute, second)."
  },
  {
    "objectID": "DANL200_lab2a.html#q1a",
    "href": "DANL200_lab2a.html#q1a",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1a",
    "text": "Q1a\nHow many Twitter users retweeted on the date, January 1, 2017?\n\n# 1. using filter(), select(), and distinct()\nQ1a <- NY_CC_tweets %>% \n  filter(year == 2017, month == 1, day == 1) %>% \n  select(id_user) %>% \n  distinct()  \n# 19\n\n\n# 2. using filter() and summarise() with n_distinct()\nQ1a <- NY_CC_tweets %>% \n  filter(year == 2017, month == 1, day == 1) %>% \n  summarise(n_users = n_distinct(id_user))\n# 19\n\n\n# 3. using filter(), select() and mutate() with n() and distinct()\nQ1a <- NY_CC_tweets %>% \n  filter(year == 2017, month == 1, day == 1) %>% \n  select(id_user, year, month, day) %>% \n  distinct() %>% \n  mutate( n_users = n() ) %>% \n  select( year, month, day, n_users ) %>% \n  distinct()\n# 19"
  },
  {
    "objectID": "DANL200_lab2a.html#q1b",
    "href": "DANL200_lab2a.html#q1b",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1b",
    "text": "Q1b\nWhich city is with the third highest number of retweets on the date, December 1, 2017?\n\n# 1. filter(), group_by(), \n# mutate() with n(), select(), distinct(), and arrange()\nQ1b <- NY_CC_tweets %>% \n  filter(year == 2017, month == 12, day == 1) %>% \n  group_by(id_city) %>% \n  mutate(n_rt = n()) %>% \n  select(id_city, city, n_rt) %>% \n  distinct() %>% \n  arrange(-n_rt)\n# brooklyn, ny \n\n\n# 2. filter(), group_by(), \n# summarise() with n(), and arrange()\nQ1b <- NY_CC_tweets %>% \n  filter(year == 2017, month == 12, day == 1) %>% \n  group_by(id_city, city) %>% \n  summarize(n_rt = n()) %>% \n  arrange(-n_rt)\n# brooklyn, ny \n\n\n# 3. filter(), group_by(), \n# summarise() with n(), ungroup(), and mutate() with dense_rank()\nQ1b <- NY_CC_tweets %>% \n  filter(year == 2017, month == 12, day == 1) %>% \n  group_by(id_city, city) %>% \n  summarize(n_rt = n()) %>%\n  ungroup() %>%  # dense_rank() calculates rankings within a group.\n  mutate(ranking = dense_rank(desc(n_rt))) %>% \n  filter(ranking == 3)\n# brooklyn, ny"
  },
  {
    "objectID": "DANL200_lab2a.html#q1c",
    "href": "DANL200_lab2a.html#q1c",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1c",
    "text": "Q1c\nFor each year, find the top 5 Twitter users in NY state in terms of the number of retweets they made in NY state. In which city and county do these users live in?\n\n# 1. group_by(), mutate() with n(), \n# group_by(), mutate() with dense_rank(), \n# filter(), select(), distinct(), and arrange()\nQ1c <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% # the number of retweets for each user for each year\n  group_by( year ) %>%  # dense_rank() calculates rankings within a group.\n  mutate( n_rt_rank = dense_rank( desc(n_rt) ) ) %>% \n  filter( n_rt_rank <= 5 ) %>% \n  select(-(month:second)) %>% # Need to remove from 'month' to 'second' to use distinct().\n  distinct() %>% \n  arrange(year, n_rt_rank)\n\n\n# 2. group_by(), summarise() with n(), \n# filter(), arrange(), and head()\nQ1c_2012 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2012 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2013 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2013 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2014 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2014 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2015 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2015 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2016 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2016 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2017 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2017 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\n\n# 4. group_by(), mutate() with n(), \n# filter(), select(), unique(), arrange(), and head()\nQ1c_2012 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2012 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2013 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2013 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2014 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2014 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2015 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2015 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2016 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2016 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2017 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2017 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\n\n# cf. I do not require using functions, \n# but here I am showing you how functions make code simpler\ntop_users_yr <- function(yr){\n  NY_CC_tweets %>% \n    group_by( id_user, year ) %>% \n    mutate( n_rt = n() ) %>% \n    filter( year == yr) %>% \n    select(year, id_user, n_rt, city, county) %>% \n    unique() %>% \n    arrange(-n_rt)\n}\n\nfor (i in 2012:2017){\n  print(head(top_users_yr(i), n = 5))\n}"
  },
  {
    "objectID": "DANL200_lab2a.html#q1d",
    "href": "DANL200_lab2a.html#q1d",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1d",
    "text": "Q1d\nSummarize the data set into the data frame with county and month levels of retweets with the following variables:\n\nFIPS, county, year, month;\nn_retweets: the number of retweets in year YYYY and month MM from county C.\n\nThe unique() or distinct() functions can be used to keep only unique/distinct rows from a data frame.\n\n# 1. group_by() and summarize() with n()\nQ1d <- NY_CC_tweets %>% \n  group_by(FIPS, county, year, month) %>% \n  summarise(n_retweets = n())\n\n# 2. count() and rename()\nQ1d <- NY_CC_tweets %>% \n  count(FIPS, county, year, month) %>% \n  rename(n_retweets = n)\n\n# 3. group_by(), mutate() with n(), \n# select(), arrange() and distinct()\nQ1d <- NY_CC_tweets %>% \n  group_by(FIPS, county, year, month) %>% \n  mutate(n_retweets = n()) %>% \n  select(FIPS, county, year, month, n_retweets) %>% \n  arrange(FIPS, county, year, month) %>% \n  distinct()\n\nQ1d <- NY_CC_tweets %>% \n  mutate(dum = 1) %>% \n  group_by(FIPS, county, year, month) %>% \n  mutate(n_retweets = sum(dum)) %>% \n  select(FIPS, county, year, month, n_retweets) %>% \n  arrange(FIPS, county, year, month) %>% \n  distinct()"
  },
  {
    "objectID": "DANL200_lab2a.html#q1e",
    "href": "DANL200_lab2a.html#q1e",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1e",
    "text": "Q1e\nDescribe the relationship between the number of retweets and county using ggplot. Make a simple comment on your plot.\n\nQ1e <- NY_CC_tweets %>% \n  group_by(county) %>% \n  summarize(n_retweets = n())\n\n\n\n\n\n  \n\n\n\n\nggplot(data = Q1e) +\n  geom_col(aes(x = n_retweets, y = county)) \n# Most retweets about #climatechange came from New York county.\n\n\n\n\n\n\n\n\nggplot(data = Q1e) +\n  geom_col(aes(x = n_retweets, \n               y = reorder(county, n_retweets) ))\n# reorder(CATEGORICAL_VAR, CONTINUOUS_VAR) reorders \n# CATEGORICAL_VAR based on a values of CONTINUOUS_VAR"
  },
  {
    "objectID": "DANL200_lab2a.html#variable-description-1",
    "href": "DANL200_lab2a.html#variable-description-1",
    "title": "R Lab 2 - Data Transformation",
    "section": "Variable Description",
    "text": "Variable Description\nEach observation in beer_markets.csv is a household-level record for one transaction of beer.\n\n\nhh: an identifier of the household;\nX_purchase_desc: details on the purchased item;\nquantity: the number of items purchased;\nbrand: Bud Light, Busch Light, Coors Light, Miller Lite, or Natural Light;\nspent: total dollar value of purchase;\nbeer_floz: total volume of beer, in fluid ounces;\nprice_per_floz: price per fl.oz. (i.e., beer spent/beer floz);\ncontainer: the type of container;\npromo: Whether the item was promoted (coupon or otherwise);\nmarket: Scan-track market (or state if rural);\ndemographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher."
  },
  {
    "objectID": "DANL200_lab2a.html#q2a",
    "href": "DANL200_lab2a.html#q2a",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2a",
    "text": "Q2a\n\nFind the top 5 markets in terms of the total volume of beer.\nFind the top 5 markets in terms of the total volume of BUD LIGHT.\nFind the top 5 markets in terms of the total volume of BUSCH LIGHT.\nFind the top 5 markets in terms of the total volume of COORS LIGHT.\nFind the top 5 markets in terms of the total volume of MILLER LITE.\nFind the top 5 markets in terms of the total volume of NATURAL LIGHT.\n\n\n\n\n\nQ2a1 <- beer_markets %>% \n   group_by(market) %>% \n   summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n   arrange(-beer_floz_tot) %>% \n   slice(1:5)\n\nQ2a_bud <- beer_markets %>% \n  filter(brand == \"BUD LIGHT\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)\n\nQ2a_busch <- beer_markets %>% \n  filter(brand == \"BUSCH LIGHT\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)\n\nQ2a_coors <- beer_markets %>% \n  filter(brand == \"COORS LIGHT\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)\n\nQ2a_miller <- beer_markets %>% \n  filter(brand == \"MILLER LITE\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)\n\nQ2a_natural <- beer_markets %>% \n  filter(brand == \"NATURAL LIGHT\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)"
  },
  {
    "objectID": "DANL200_lab2a.html#q2b",
    "href": "DANL200_lab2a.html#q2b",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2b",
    "text": "Q2b\n\nFor households that purchased BUD LIGHT, what fraction of households did purchase only BUD LIGHT?\nFor households that purchased BUSCH LIGHT, what fraction of households did purchase only BUSCH LIGHT?\nFor households that purchased COORS LIGHT, what fraction of households did purchase only COORS LIGHT?\nFor households that purchased MILLER LITE, what fraction of households did purchase only MILLER LITE?\nFor households that purchased NATURAL LIGHT, what fraction of households did purchase only NATURAL LIGHT?\nWhich beer brand does have the largest base of loyal consumers?\n\n\nbeer_markets <- beer_markets %>% \n  mutate(bud = ifelse(brand==\"BUD LIGHT\", 1, 0),\n         busch = ifelse(brand==\"BUSCH LIGHT\", 1, 0),\n         coors = ifelse(brand==\"COORS LIGHT\", 1, 0),\n         miller = ifelse(brand==\"MILLER LITE\", 1, 0),\n         natural = ifelse(brand==\"NATURAL LIGHT\", 1, 0) )\n\nQ2b_bud <- beer_markets %>%\n  select(hh, bud) %>%\n  arrange(hh, -bud) %>%\n  group_by(hh) %>%\n  filter(sum(bud) > 0) %>% \n  mutate(frac_bud = sum(bud)/n(),\n         loyal_bud = ifelse(frac_bud == 1, 1, 0)) %>%\n  select(hh, frac_bud, loyal_bud) %>% \n  unique() %>% \n  ungroup() %>% \n  mutate(n_hh_bud = n()) %>%\n  group_by(loyal_bud, n_hh_bud) %>% \n  summarise(n_obs = n()) %>% \n  ungroup() %>% \n  mutate(n_frac = n_obs/n_hh_bud )  # 0.6600816\n\n\nQ2b_busch <- beer_markets %>%\n  select(hh, busch) %>%\n  arrange(hh, -busch) %>%\n  group_by(hh) %>%\n  filter(sum(busch) > 0) %>% \n  mutate(frac_busch = sum(busch)/n(),\n         loyal_busch = ifelse(frac_busch == 1, 1, 0)) %>%\n  select(hh, frac_busch, loyal_busch) %>% \n  unique() %>% \n  ungroup() %>% \n  mutate(n_hh_busch = n()) %>%\n  group_by(loyal_busch, n_hh_busch) %>% \n  summarise(n_obs = n()) %>% \n  ungroup() %>% \n  mutate(n_frac = n_obs/n_hh_busch )  # 0.472973\n\n\nQ2b_coors <- beer_markets %>%\n  select(hh, coors) %>%\n  arrange(hh, -coors) %>%\n  group_by(hh) %>%\n  filter(sum(coors) > 0) %>% \n  mutate(frac_coors = sum(coors)/n(),\n         loyal_coors = ifelse(frac_coors == 1, 1, 0)) %>%\n  select(hh, frac_coors, loyal_coors) %>% \n  unique() %>% \n  ungroup() %>% \n  mutate(n_hh_coors = n()) %>%\n  group_by(loyal_coors, n_hh_coors) %>% \n  summarise(n_obs = n()) %>% \n  ungroup() %>% \n  mutate(n_frac = n_obs/n_hh_coors )  # 0.6390805\n\n\nQ2b_miller <- beer_markets %>%\n  select(hh, miller) %>%\n  arrange(hh, -miller) %>%\n  group_by(hh) %>%\n  filter(sum(miller) > 0) %>% \n  mutate(frac_miller = sum(miller)/n(),\n         loyal_miller = ifelse(frac_miller == 1, 1, 0)) %>%\n  select(hh, frac_miller, loyal_miller) %>% \n  unique() %>% \n  ungroup() %>% \n  mutate(n_hh_miller = n()) %>%\n  group_by(loyal_miller, n_hh_miller) %>% \n  summarise(n_obs = n()) %>% \n  ungroup() %>% \n  mutate(n_frac = n_obs/n_hh_miller )  # 0.6312989\n\nQ2b_natural <- beer_markets %>%\n  select(hh, natural) %>%\n  arrange(hh, -natural) %>%\n  group_by(hh) %>%\n  filter(sum(natural) > 0) %>% \n  mutate(frac_natural = sum(natural)/n(),\n         loyal_natural = ifelse(frac_natural == 1, 1, 0)) %>%\n  select(hh, frac_natural, loyal_natural) %>% \n  unique() %>% \n  ungroup() %>% \n  mutate(n_hh_natural = n()) %>%\n  group_by(loyal_natural, n_hh_natural) %>% \n  summarise(n_obs = n()) %>% \n  ungroup() %>% \n  mutate(n_frac = n_obs/n_hh_natural )  # 0.5096234\n\n# Here I do not provide any comments on the result."
  },
  {
    "objectID": "DANL200_lab2a.html#q2c",
    "href": "DANL200_lab2a.html#q2c",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2c",
    "text": "Q2c\n\nCalculate the number of beer transactions for each household.\nCalculate the fraction of each beer brand for each household.\n\n\nQ2c <- beer_markets %>% \n  count(hh, brand) %>% \n  group_by(hh) %>% \n  mutate(n_tot = sum(n)) %>% \n  arrange(hh, brand) %>% \n  mutate( prop = n / n_tot )"
  },
  {
    "objectID": "mba-ch3-regularization.html#bias-variance-tradeoff",
    "href": "mba-ch3-regularization.html#bias-variance-tradeoff",
    "title": "Regularized Regression",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nknitr::include_graphics('lec_figs/bias-variance-tradeoff-1.png')\n\n\n\n\n\n\n\n\n\nDecomposition of MSE\n\nThe mean squared error (MSE) of the model with unknown set of parameters \\(\\mathbf{\\theta}\\) can be decomposed into bias and variance.\n\n\\[\n\\begin{align}\nM S E &\\,=\\, \\frac{\\sum_{i = 1}^{ n}\\,(\\, y_{i} - \\hat{y}_{i} \\,)^2}{n}\\\\\n&\\,=\\,E \\,[\\, y_{i} - \\hat{f}(\\mathbf{X}) \\,]^{2}\\\\\n&\\,=\\, [\\text{Bias}(\\, \\hat{f}(\\mathbf{X}) \\,)]^{2} \\,+\\, Var(\\,\\hat{f}(\\mathbf{X})\\,) \\,+\\, \\text{Var}(\\epsilon)\n\\end{align}\n\\]\n\nIn order to minimize the MSE above, we need to select a machine learning methods that simultaneously achieves low variance and low bias.\n\nNote that both squared bias and variance are nonnegative.\nThe MSE can never lie below \\(\\text{Var}(\\epsilon)\\), the irreducible error.\n\n\n\nknitr::include_graphics('lec_figs/bias-variance-tradeoff-2.png')\n\n\n\n\n\n\n\n\n\nThe horizontal dashed line represents \\(\\text{Var}(\\epsilon)\\), the irreducible error.\nThe vertical dotted line indicates the flexibility level corresponding to the smallest test MSE.\n\n\nknitr::include_graphics('lec_figs/bias-variance-tradeoff-3.png')"
  },
  {
    "objectID": "mba-ch3-regularization.html#regularization",
    "href": "mba-ch3-regularization.html#regularization",
    "title": "Regularized Regression",
    "section": "Regularization",
    "text": "Regularization\n\nRegularization helps find some optimal amount of flexibility that minimizes MSE.\nRegularized regression can resolve the following problems\n\nQuasi-separation in logistic regression:\n\nThe input variable yields an almost perfect prediction of the binary response variable.\n\nMulticolinearity in regression:\n\nVariables age and years_of_workforce in linear regression of income are too highly correlated.\n\n\n\n\nRidge Regression\n\nRidge regression introduces a shrinkage penalty \\(\\lambda \\geq 0\\) by minimizing:\n\n\\[\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j x_{ij}\\big)^2 + \\lambda \\sum_j^p \\beta_j^2 = \\text{SSE} + \\lambda \\sum_j^p \\beta_j^2\\] - As \\(\\lambda\\) increases \\(\\Rightarrow\\) flexibility of models decreases\n\nincreases bias, but decreases variance\nFor fixed value of \\(\\lambda\\), ridge regression fits only a single model\n\nneed to use cross-validation to tune \\(\\lambda\\)\n\nFor example: note how the magnitude of the coefficient for Income trends as \\(\\lambda \\rightarrow \\infty\\)\n\n\n\n\n\n\n\n\n\n\n\nThe coefficient shrinks towards zero, but never actually reaches it.\n\nRidge regression tends to average the collinear variables together.\n\nIncome is always a variable in the learned model, regardless of the value of \\(\\lambda\\).\n\n\n\n\nLasso Regression\n\nRidge regression keeps all variables.\nBut Lasso regression gets lid of variables.\n\nLasso enables variable selection with \\(\\lambda\\) by minimizing:\n\n\n\\[\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j X_{ij}\\big)^2 + \\lambda \\sum_j^p\\vert  \\beta_j \\vert = \\text{SSE} + \\lambda \\sum_j^p \\vert \\beta_j \\vert\\]\n\nAs \\(\\lambda\\) increases \\(\\Rightarrow\\) flexibility of models decreases\n\nincreases bias, but decreases variance\n\nLasso can handle the \\(p > n\\) case, i.e. more variables than observations!\nLasso regression performs variable selection yielding sparse models, which only includes a small subset of the available variables that are relevant to predicting the outcome of interest.\n\n\n\n\n\n\n\n\n\n\n\nThe coefficient shrinks towards and eventually equals zero at \\(\\lambda \\approx 1000\\)\nIf the optimum value of \\(\\lambda\\) is larger, then Income would NOT be included in the learned model.\n\n\n\n\nElastic net\n\\[\\sum_{i}^{n}\\left(Y_{i}-\\beta_{0}-\\sum_{j}^{p} \\beta_{j} X_{i j}\\right)^{2}+\\lambda\\left[(1-\\alpha)\\|\\beta\\|_{2}^{2} / 2+\\alpha\\|\\beta\\|_{1} \\right]\\] - \\(\\vert \\vert \\beta \\vert \\vert_1\\) is the \\(\\ell_1\\) norm: \\(\\vert \\vert \\beta \\vert \\vert_1 = \\sum_j^p \\vert \\beta_j \\vert\\)\n\n\\(\\vert \\vert \\beta \\vert \\vert_2\\) is the \\(\\ell_2\\), Euclidean, norm: \\(\\vert \\vert \\beta \\vert \\vert_2 = \\sqrt{\\sum_j^p \\beta_j^2}\\)\nRidge penalty: \\(\\lambda \\cdot (1 - \\alpha) / 2\\)\nLasso penalty: \\(\\lambda \\cdot \\alpha\\)\n\\(\\alpha\\) controls the mixing between the two types, ranges from 0 to 1\n\n\\(\\alpha = 1\\) returns lasso\n\\(\\alpha = 0\\) return ridge\n\n\n\n\n\nGamma Lasso Regression\n\\[\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j X_{ij}\\big)^2 + \\lambda \\sum_j^p\\vert  \\beta_j \\vert = \\text{SSE} + \\lambda \\sum_j^p \\log(\\, 1 + \\vert \\beta_j \\vert\\,)\\]"
  },
  {
    "objectID": "mba-ch3-regularization.html#putting-a-cost-on-complexity",
    "href": "mba-ch3-regularization.html#putting-a-cost-on-complexity",
    "title": "Regularized Regression",
    "section": "Putting a Cost on Complexity",
    "text": "Putting a Cost on Complexity\n\nWith regularization, we put a cost on the magnitude of each \\(\\beta_{p}\\).\n\nThis penalizes complexity, because the \\(\\beta_{p}\\) coefficients are what allow our predicted \\(\\hat{y}\\) values to move around with different input \\(X\\) values.\n\nIf we force all the \\(\\hat{\\beta}_{p}\\) to be close to zero, then our \\(\\hat{y}\\) values will be shrunk toward \\(\\bar{y}\\) and when we jitter the data your predictions will not change as much as they would if we did not include a penalty term during estimation.\n\\(\\lambda\\) is the penalty weight that determines the price of complexity.\n\nIt is a tuning parameter that needs to be selected in some data-dependent manner.\n\n\nknitr::include_graphics('lec_figs/mba-3-4.png')\n\n\n\n\n\n\n\n\nThe ridge penalty (\\(\\beta^{2}\\)) places little penalty on small values of \\(\\beta\\).\nThe Lasso penalty (\\(\\vert \\beta \\vert\\)) places a constant penalty on incremental deviations from zero.\nThe gamma Lasso penalty (\\(\\log(1 + \\vert \\beta \\vert)\\)) places extreme cost on the move from zero to small values of \\(\\beta\\), but for large values the rate of penalty change is small.\n\nThis encourages lots of zeros in our fit while allowing large betas to be estimated without any bias!\n\n\n\n\nAdvantages of the Lasso\n\nThe Lasso gives the least possible amount of bias on large signals while still retaining the stability of a convex penalty like ridge (convex means that the penalty does not flatten out for large values.).\nThe Lasso will yield automatic variable screening—model selection—some of the \\(\\hat{\\beta}_{p}\\) will be zero!\n\n\nknitr::include_graphics('lec_figs/ridge-lasso-animation.gif')\n\n\n\n\n\nSource: Quora\nHere is another illustration of the Lasso and its path in 2D."
  },
  {
    "objectID": "mba-ch3-regularization.html#regularization-algorithms",
    "href": "mba-ch3-regularization.html#regularization-algorithms",
    "title": "Regularized Regression",
    "section": "Regularization Algorithms",
    "text": "Regularization Algorithms\n\nStandardizing Data\n\nFor either ridge, lasso, gamma lasso or elastic net: we should consider standardizing our data\nCommon convention: within each column, compute then subtract off the sample mean, and compute the divide off the sample standard deviation:\n\n\\[\\tilde{x}_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_{x,j}}\\]\n\nglmnet and gamlr package do this by default and reports coefficients on the original scale\n\n\\(\\lambda\\) and \\(\\alpha\\) are tuning parameters\nWhen using glmnet, the cv.glmnet() function will perform the cross-validation for us\n\ngamlr package does implements the gamma lasso algorithm.\n\nWhen using gamlr, the cv.gamlr() function will perform the cross-validation for us.\n\nglmnetUtils enables us to use a data.frame to provide data to the model.\nglmnet and gamlr use a sparse.matrix, instead of a data.frame, to provide data to the model.\nA sparse matrix is a matrix with many zero entries.\n\nA sparse matrix is almost essential in big data analysis because of its lower storage costs and faster computation.\n\n\n\nknitr::include_graphics('lec_figs/sparse-matrix.png')\n\n\n\n\n\n\n\nk-fold Cross-Validation\n\nSplit the data into k random and roughly evenly sized subsets, called folds. Then, for :\n\nStep 1. Use all data except the k-th fold to train a model\nStep 2. Record the error rate (e.g., MSE, \\(R^2\\), AICc) for predictions on the left-out fold based on the fitted model (out-of-sample (OOS) deviance).\n\n\n\nknitr::include_graphics('lec_figs/pds_fig69.png')\n\n\n\n\n\nFor each candidate model, \\(m \\in\\{1, \\cdots, M\\}\\), with \\(\\lambda_{m}\\), we do k-fold cross-validation (CV).\nSo k-fold CV will yield a set of k OOS deviances for each of our candidate models.\n\nSo we select the model with the best OOS performance!\n\nHow should we choose k?\n\nMore is better but only up to a point. \nWe can experiment on the choice of k.\n\n\n\n\n\nSchematic of cv.glmnet() and cv.gamlr()\n\nknitr::include_graphics('lec_figs/pds_fig718.png')\n\n\n\n\n\n\\(\\lambda_{.min}\\): the \\(\\lambda\\) for the model with the minimum cross-validation (CV) error.\n\\(\\lambda_{1se}\\): corresponds to the model with cross-validation error, which is one standard error (se) of CV error above the minimum CV error.\n\n\n\n\nSchematic of cva.glmnet()\n\nknitr::include_graphics('lec_figs/pds_fig721.png')"
  },
  {
    "objectID": "mba-ch3-regularization.html#references",
    "href": "mba-ch3-regularization.html#references",
    "title": "Regularized Regression",
    "section": "References",
    "text": "References\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani and Jerome Friedman.\n\n\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\nPractical Data Science with R by Nina Zumel and John Mount.\nSummer Undergraduate Research Experience (SURE) 2022 in Statistics at Carnegie Mellon University by Ron Yurko."
  },
  {
    "objectID": "DANL310_hw1a.html",
    "href": "DANL310_hw1a.html",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "",
    "text": "Renovate your personal website on GitHub using Quarto.\n\nFAQ about Quarto for R Markdown users are provided below: https://quarto.org/docs/faq/rmarkdown.html\nA guide for creating a Quarto website is provided in the following webpage: https://quarto.org/docs/websites/.\nLecture 7 provides a guide to creating a Quarto website on GitHub."
  },
  {
    "objectID": "DANL310_hw1a.html#q2a.",
    "href": "DANL310_hw1a.html#q2a.",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2a.",
    "text": "Q2a.\nUse the following data.frame for Q2a, Q2b, and Q2c.\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')\n\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')\n\nggplot(ncdc_temp, aes(x = date, y = temperature, color = location)) +\n\ngeom_line(size = 1) + # Adds a layer to the ggplot object with a line plot of the temperature data, with a size of 1.\n\nscale_x_date(name = \"month\", limits = c(ymd(\"0000-01-01\"), ymd(\"0001-01-04\")), # Adds a scale to the x-axis with the label \"month\" and limits of Jan 1, 0000 to Jan 4, 0001, and breaks at the beginning of each quarter (Jan, Apr, Jul, Oct), with corresponding labels.\nbreaks = c(ymd(\"0000-01-01\"), ymd(\"0000-04-01\"), ymd(\"0000-07-01\"),\nymd(\"0000-10-01\"), ymd(\"0001-01-01\")),\nlabels = c(\"Jan\", \"Apr\", \"Jul\", \"Oct\", \"Jan\"), expand = c(1/366, 0)) +\n\nscale_y_continuous(limits = c(19.9, 107), # Adds a scale to the y-axis with limits of 19.9 to 107, breaks at every 20 units, and label \"temperature (°F)\".\nbreaks = seq(20, 100, by = 20),\nname = \"temperature (°F)\") +\n\ntheme(legend.title.align = 0.5) # Adjusts the alignment of the legend title to be centered."
  },
  {
    "objectID": "DANL310_hw1a.html#q2b",
    "href": "DANL310_hw1a.html#q2b",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2b",
    "text": "Q2b\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')\n\np <- ggplot(ncdc_temp, aes(x = month, y = temperature)) \n\n  # add a box plot with grey fill\np + geom_boxplot(fill = 'grey90') + \n  # add labels for x and y axes\n  labs(x = \"month\",\n       y = \"mean temperature (°F)\") +\n  # apply a custom theme to the plot\n  theme_clean()"
  },
  {
    "objectID": "DANL310_hw1a.html#q2c",
    "href": "DANL310_hw1a.html#q2c",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2c",
    "text": "Q2c\nUse ggridges::geom_density_ridges() for Q2c.\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')\n\np <- ggplot(ncdc_temp, aes(x = temperature, y = month)) \n\np + geom_density_ridges( # Adds a layer to the ggplot object with a smoothed density plot of the temperature data using the 'ridgeline' plot type.\n  scale = 3, rel_min_height = 0.01, # Sets the scaling and minimum relative height for the plot.\n  bandwidth = 3.4, fill = \"#56B4E9\", color = \"white\" # Sets the bandwidth for the plot, as well as the fill and color for the plot elements.\n) +\n\nscale_x_continuous( # Adds a scale to the x-axis for continuous values.\n  name = \"mean temperature (°F)\", # Sets the label for the x-axis.\n  expand = c(0, 0), breaks = c(0, 25, 50, 75) # Sets the expansion and the break points for the x-axis.\n) +\n\nscale_y_discrete(\n  name = \"month\", expand = c(0, .2, 0, 2.6)) + # Adds a scale to the y-axis for discrete (categorical) values, with a label and a custom expansion.\n\ntheme( # Applies a custom theme to the ggplot object.\n  plot.margin = margin(3, 7, 3, 1.5) # Sets the margin of the plot.\n)"
  },
  {
    "objectID": "DANL310_hw1a.html#q2d",
    "href": "DANL310_hw1a.html#q2d",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2d",
    "text": "Q2d\nUse ggplot::mtcars for Q2d.\n\nm <- ggplot(data = mtcars, aes(x = disp, y = mpg, color = hp)) \n\nm + geom_point(aes(color = hp)) + # add scatter plot with color mapped to \"hp\" variable\n  labs(x = \"displacement(cu. in.)\", y = \"fuel efficiency(mpg)\")+ # add labels to x and y axes\n  scale_color_gradient()+ # add color gradient scale legend\n  scale_fill_brewer(palette = \"Emrld\") # add fill color palette with \"Emrld\" scheme to the legend"
  },
  {
    "objectID": "DANL310_hw1a.html#q2e",
    "href": "DANL310_hw1a.html#q2e",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2e",
    "text": "Q2e\nUse the following data.frame for Q2e.\n\npopgrowth_df <- read_csv(\n  'https://bcdanl.github.io/data/popgrowth.csv')\n\n\np <- ggplot(popgrowth_df, \n            aes(x = reorder(state, popgrowth), \n                y = 100*popgrowth, \n                fill = region))\np + geom_col() + # Add the geom for the columns\n  scale_y_continuous(\n    limits = c(-.6, 37.5), expand = c(0, 0), # Set y axis limits and expansion\n    labels = scales::percent_format(accuracy = 1, scale = 1), # Set percent labels for y axis\n    name = \"population growth, 2000 to 2010\" # Set name for y axis\n    ) +\n  coord_flip() + # Flip the x and y axis\n  theme(legend.position = c(.67, .4), # Set legend position\n        axis.text.y = element_text( size = 6, \n                                    margin = margin(t = 0, r = 0, b = 0, l = 0) )) # Adjust the size and margin for y axis text"
  },
  {
    "objectID": "DANL310_hw1a.html#q2f",
    "href": "DANL310_hw1a.html#q2f",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2f",
    "text": "Q2f\nUse the following data.frame for Q2f.\n\nmale_Aus <- read_csv(\n  'https://bcdanl.github.io/data/aus_athletics_male.csv')\n\n\n# Define color and fill vectors for use in plot\ncolors <- c(\"#BD3828\", rep(\"#808080\", 4))\nfills <- c(\"#BD3828D0\", rep(\"#80808080\", 4))\n\np <- ggplot(male_Aus, aes(x=height, y=pcBfat, shape=sport, color = sport, fill = sport))\n\n# Add geom_point layer with custom size\np + geom_point(size = 3) +\n\n# Set shape values for different sports\n  scale_shape_manual(values = 21:25) +\n\n# Set color values for different sports\n  scale_color_manual(values = colors) +\n\n# Set fill values for different sports\n  scale_fill_manual(values = fills) +\n\n# Set x and y axis labels\n  labs(x = \"height (cm)\",\n       y = \"% body fat\" )"
  },
  {
    "objectID": "DANL310_hw1a.html#q2g",
    "href": "DANL310_hw1a.html#q2g",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2g",
    "text": "Q2g\nUse the following data.frame for Q2g.\n\ntitanic <- read_csv(\n  'https://bcdanl.github.io/data/titanic_cleaned.csv')\n\n\np <- ggplot(titanic, aes(x = age, y = after_stat(count) ) ) \n\n# Add a density line plot for all passengers with transparent color, and fill legend with \"all passengers\"\np + geom_density_line(\n    data = select(titanic, -sex), \n    aes(fill = \"all passengers\"),\n    color = \"transparent\"\n  ) + \n  # Add another density line plot for each sex with transparent color, and fill legend with sex\n  geom_density_line(aes(fill = sex), bw = 2, color = \"transparent\") +\n  # Set the x-axis limits, name, and expand arguments\n  scale_x_continuous(limits = c(0, 75), name = \"passenger age (years)\", expand = c(0, 0)) +\n  # Set the y-axis limits, name, and expand arguments\n  scale_y_continuous(limits = c(0, 26), name = \"count\", expand = c(0, 0)) +\n  # Set the manual color and fill values, breaks, and labels for the legend\n  scale_fill_manual(\n    values = c(\"#b3b3b3a0\", \"#0072B2\", \"#D55E00\"), \n    breaks = c(\"all passengers\", \"male\", \"female\"),\n    labels = c(\"all passengers  \", \"males  \", \"females\"),\n    name = NULL,\n    guide = guide_legend(direction = \"horizontal\")\n  ) +\n  # Set the Cartesian coordinate system to allow for data points to fall outside the plot limits\n  coord_cartesian(clip = \"off\") +\n  # Create separate density line plots for male and female passengers\n  facet_wrap(~sex) +\n  # Set the x-axis line to blank, increase the strip text size, and set the legend position and margin\n  theme(\n    axis.line.x = element_blank(),\n    strip.text = element_text(size = 14, margin = margin(0, 0, 0.2, 0, \"cm\")),\n    legend.position = \"bottom\",\n    legend.justification = \"right\",\n    legend.margin = margin(4.5, 0, 1.5, 0, \"pt\"),\n    legend.spacing.x = grid::unit(4.5, \"pt\"),\n    legend.spacing.y = grid::unit(0, \"pt\"),\n    legend.box.spacing = grid::unit(0, \"cm\")\n  )"
  },
  {
    "objectID": "DANL310_hw1a.html#q2h",
    "href": "DANL310_hw1a.html#q2h",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2h",
    "text": "Q2h\nUse the following data.frame for Q2h.\n\ncows_filtered <- read_csv(\n  'https://bcdanl.github.io/data/cows_filtered.csv')\n\n\np <- ggplot(cows_filtered, aes(x = butterfat, color = breed, fill = breed))\n\n# add a density line for each breed with some transparency\np + geom_density_line(alpha = .2) +\n\n# set x-axis properties\nscale_x_continuous(\n  expand = c(0, 0), # remove padding from axis limits\n  labels = scales::percent_format(accuracy = 1, scale = 1), # format axis labels as percentages with 1 decimal point\n  name = \"butterfat contents\" # set axis label\n) +\n\n# set y-axis properties\nscale_y_continuous(limits = c(0, 1.99), expand = c(0, 0)) +\n\n# set plot area properties\ncoord_cartesian(clip = \"off\") + # allow density lines to extend beyond axis limits\ntheme(axis.line.x = element_blank()) # remove x-axis line\n\n\n\n\n\n\nReference: Fundamentals of Data Visualization by Claus O. Wilke"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "B.H. CHOE",
    "section": "",
    "text": "Office: South Hall 117B Email: bchoe@geneseo.edu\n\n\n\n\n\n\n\n\n\nI view myself as an applied economist with interest in environmental economics and a particular focus on climate change. Methodologically, I make use of causal inference, econometrics, machine learning methods, and other data science tools to conduct empirical analysis. I also use computational methods to solve integrated assessment models of climate change and theoretical economic models, such as dynamic contracts.\nMy research focuses on how to improve effectiveness of climate policy at both micro and macro levels. In particular, I am interested in building relevant climate-economy models that analyze the interaction between economies and the climate under risks arising from (1) climate change and (2) a transition to carbon-neutral economies.\nAs an economics and data science teacher, my goal is to equip students with the essential tools of machine learning, econometrics, and data science to think critically about business and real-world socioeconomic issues. In this regard, I teach students to use those tools to address important business strategies, socioeconomic issues, and individual decision-making.\n\n\n\n\n\n\n\n\n\n\n  \n    2021 --- present \n    Lecturer in Data AnalyticsSchool of Business, SUNY Geneseo \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    2016 --- 2021 \n    Ph.D. in Economics, University of WyomingSupervisors: Chuck Mason and Thorsten Janus Mentors: Stephen Newbold and Jay ShogrenDissertaion: Essays in Political Economy of Climate Change \n  \n  \n    2014 --- 2015 \n    M.Sc. in Economics, Arizona State University \n  \n  \n    2012 --- 2014 \n    M.A. in Economics, SUNY Stony Brook \n  \n  \n    2006 --- 2012 \n    B.Sc. in Applied Mathematics, Hanyang University at Ansan, South Korea \n  \n  \n    2006 --- 2012 \n    B.A. in Economics, Hanyang University at Ansan, South Korea \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n     \n    Social Media Campaigns, Lobbying and Legislation: Evidence from #climatechange/#globalwarming and Energy Lobbies   *Abstract*: To what extent do social media campaigns compete with fossil fuel lobbying on climate change legislation? In this article, I estimate the effect of social media campaigns on a congressperson's legislative activities against climate change actions during the U.S. Congresses (January 2013-January 2019). I find that (1) a 1% increase in the per-capita level of activities of climate change campaigns using Twitter decreases Democrats' tendency to support climate-unfriendly legislation by 0.9%, while it increases Republicans' one by 0.2%; and (2) a 1% increase in the fossil fuel industry's lobbying expenditure relative to the rest of industries' lobbying expenditure increases Republicans' tendency to support climate-unfriendly legislation by 1.1%. I also find that negative sentiment in social media campaigns contributes to affecting congresspersons' support for climate-unfriendly legislation. \n  \n  \n     \n    Climate Finance under Conflicts and Renegotiations: A Dynamic Contract Approach  (AEA 2019 poster)    *Abstract*: Considering climate funds (e.g. the Green Climate Fund) as the financial mechanism to provide funding to developing countries, this paper examines a long-term climate funding relationship between two agents---the rich country and the poor country. Conflicts between the rich and poor countries arise when determining 1) the size of climate funding that the rich country contributes to the poor country, and 2) the funding allocation between climate adaptation and mitigation projects in the poor country. In addition, the rich country cannot be forced to commit contractual contributions to the poor country, and the climate funding relationship can be repeatedly renegotiated. This paper derives the following results: (1) climate funds converge to the first-best in the long-run, in terms of the size of climate funding and its balance between adaptation and mitigation projects, if and only if climate damage becomes sufficiently severe. (2) funding allocation between adaptation and mitigation projects becomes more favorable to the poor country if marginal climate costs in the poor country grow faster than in the rich country. (3) fewer conflicts and fewer renegotiations between the rich and poor countries make climate funding contracts more efficient, remedying inequality between the poor and rich countries. \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n     \n    Hiding Behind a Small Cake' in an Online Dictator Game: The Way You Hide Matters!, (with Tabare Capitan (1st author) , Jason Shogren, and Benjamin White)  *Abstract*: Using an online dictator game in which receivers have incomplete information of the size of the endowment (big or small), the article, ''Hiding behind a small cake' in a newspaper dictator game (Ockenfels and Werner (2012))'' shows that a few givers who received the big endowment use their giving to signal they received the small endowment (i.e., to lie). In other words, even though a giver will never meet the corresponding receiver, he cares enough about how he could be perceived by others to lie (i.e., second-order beliefs enters his utility function). In our experiment we provide givers with the opportunity to lie about the size of the endowment without using their giving. Similar to Ockenfels and Werner (2012) we find that (i) few take the opportunity to lie---but those who do give less when their giving is not constrained by its role as a signal---and (ii) givers are more likely to lie when the lie is private. However, using a second stage in the experimental design, we show that liars are the most responsive group of givers to a simple message stating the expectation of the receiver. \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    2022 --- present \n    Advisor, Business Analytics Competition  Manhattan CollegeHonorable Mention in the 2022 event, Advising the SUNY Geneseo team (Jason Rappazzo and James Jordan) \n  \n  \n    2022 --- present \n    Instructor, DANL 399: Data Analytics Capstone  SUNY GeneseoAdvised students' research project that integrates the technical data analytic skills and critical analysis on a business/economics topic \n  \n  \n    2022 --- present \n    Instructor, DANL 310: Data Visualization and Presentation  SUNY GeneseoTaught for data visualization and reproducible writing of research papers on the web. \n  \n  \n    2021 --- present \n    Instructor, DANL 210: Data Preparation and Management  SUNY GeneseoTaught the tidyverse R package, Python Pandas, Selenium, Web API, SQL Database and Google Clouds \n  \n  \n    2021 --- present \n    Instructor, DANL 200: Introduction to Data Analytics  SUNY GeneseoCovered data cleaning and basics in machine learning methods using business and economics applications \n  \n  \n    2022 \n    Instructor, DANL 100: Programming for Data Analytics  SUNY GeneseoCovered basics in Python programming and exploratory data analysis using Python pandas/seaborn and R ggplot2 \n  \n  \n    2021 \n    Instructor, DANL 300: Advanced Data Analytics  SUNY GeneseoTaught for machine learning methods (classification, regularized regression, random forest, clustering, association rules) using business and economics applications \n  \n  \n    2018 \n    Instructor, ECON 3010: Intermediate Macroeconomics  University of WyomingTaught (1) the concept of GDP, (2) the causes of economic growth and inequality, (3) how goods, labor, and credit markets work, (4) the role of the Fed and the monetary system, (5) the theory and policymaking for short-run economic fluctuations, (6) the influence of international trade and international finance on the domestic economy \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    2022 \n    Interdisciplinary Data Science Workshop, Brigham Young University, Provo, Utah \n  \n  \n    2021 \n    Online Seminar, Department of Environmental and Business Economics, University of Southern Denmark \n  \n  \n    2019 \n    University of Wyoming & Colorado State University Economics Graduate Student Symposium, Laramie, Wyoming \n  \n  \n    2019 \n    American Economic Association Annual Meeting (poster session), Atlanta, Georgia \n  \n  \n    2018 \n    The 29th International Conference on Game Theory, Stony Brook, New York \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    2022 --- present \n    Southern Economic Association \n  \n  \n    2018 --- present \n    Association of Environmental and Resource Economists \n  \n  \n    2017 --- present \n    American Economic Association \n  \n  \n    2017 --- 2019 \n    Econometric Society \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    2019 \n    H. A. True Chair in Petroleum and Economics Scholarship, University of Wyoming \n  \n  \n    2019 \n    John S. Bugas Economics Scholarship, University of Wyoming \n  \n  \n    2018 \n    Ralph d'Arge Scholarship in Natural Resource Economics and Finance, University of Wyoming \n  \n  \n    2018 \n    McMurry Excellence Fund, University of Wyoming \n  \n  \n    2018 \n    Joseph C. and Katherine A. Drew Scholarship, University of Wyoming \n  \n  \n    2012 \n    Highest Grade Point Average in Economics Class of 2012, Hanyang University at Ansan, South Korea \n  \n  \n    2010 --- 2011 \n    University Tutor of the Excellent Tutoring Team, Hanyang University at Ansan, South Korea \n  \n  \n    2006 --- 2011 \n    Merit-based Scholarships, Hanyang University at Ansan, South Korea \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    2022 \n    Travel Funding for Southern Economic Association Annual Meeting, SUNY Geneseo \n  \n  \n    2022 \n    Travel Funding for Interdisciplinary Data Science Workshop, Brigham Young University \n  \n  \n    2020 \n    Summer Augmentation Scholarship for Ph.D. Dissertation Research, University of Wyoming \n  \n  \n    2018 --- 2019 \n    Travel Funding for International Conference on Game Theory, University of Wyoming \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    2021 --- present \n    Data Analytics Curriculum MeetingsSchool of Business, SUNY Geneseo \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    Computing Skills  \n    Python, R, Stata, MATLAB, Git, SQL, LaTeX, R Markdown \n  \n  \n    Language  \n    English, Korean (native) \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    2007 --- 2009 \n    Completion of Military ObligationRepublic of Korea Army, South KoreaAttained the Rank of Sergeant \n  \n\n\n\n\n\n\n\n\nLast updated: 2023-03-23"
  },
  {
    "objectID": "DANL210_hw2q.html",
    "href": "DANL210_hw2q.html",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here.\n\n\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_hw2q.html#q1a",
    "href": "DANL210_hw2q.html#q1a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1a",
    "text": "Q1a\nWhat are the minimum, first quartile, median, thrid quartile, maximum, mean, and standard deviation of Close and Volume for each company?"
  },
  {
    "objectID": "DANL210_hw2q.html#q1b",
    "href": "DANL210_hw2q.html#q1b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1b",
    "text": "Q1b\nFind the 10 largest values for Volume. What are the companies and dates associated with those 10 largest values for Volume?"
  },
  {
    "objectID": "DANL210_hw2q.html#q1c",
    "href": "DANL210_hw2q.html#q1c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1c",
    "text": "Q1c\nCalculate the Z-scores of Open and Close for each company using apply()."
  },
  {
    "objectID": "DANL210_hw2q.html#q1d",
    "href": "DANL210_hw2q.html#q1d",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1d",
    "text": "Q1d\nUse the transform() method on the stock data to represent all the values of Open, High, Low, Close, Adj Close, and Volume in terms of the first date in the data.\nTo do so, divide all values for each company by the values of the first date in the data for that company."
  },
  {
    "objectID": "DANL210_hw2q.html#q1e",
    "href": "DANL210_hw2q.html#q1e",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1e",
    "text": "Q1e\nVisualize the daily trend of normalized values of Close for each company in one plot. The normalized values of Close are the one calculated from Q1d."
  },
  {
    "objectID": "DANL210_hw2q.html#q1f",
    "href": "DANL210_hw2q.html#q1f",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1f",
    "text": "Q1f\nCreate a box plot of Close for each company in one plot."
  },
  {
    "objectID": "DANL210_hw2q.html#q2a",
    "href": "DANL210_hw2q.html#q2a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2a",
    "text": "Q2a\nHow many parties have provided or disbursed positive funding contributions to other countries or regions for their adaptation projects for every single year from 2011 to 2018?"
  },
  {
    "objectID": "DANL210_hw2q.html#q2b",
    "href": "DANL210_hw2q.html#q2b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\nFor each party, calculate the total funding contributions that were disbursed or provided for mitigation projects for each year."
  },
  {
    "objectID": "DANL210_hw2q.html#q2c",
    "href": "DANL210_hw2q.html#q2c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2c",
    "text": "Q2c\nFor each party, calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each year."
  },
  {
    "objectID": "DANL210_hw2q.html#q2d",
    "href": "DANL210_hw2q.html#q2d",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2d",
    "text": "Q2d\nProvide both seaborn code and a simple comment to visualize the distribution of the ratio between adaptation contribution and mitigation contribution, which is calculated in Q2c."
  },
  {
    "objectID": "DANL210_hw2q.html#q2e",
    "href": "DANL210_hw2q.html#q2e",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2e",
    "text": "Q2e\nVisualize how the distribution of Contribution varies by Type of support and Status."
  },
  {
    "objectID": "DANL210_lab1a.html",
    "href": "DANL210_lab1a.html",
    "title": "Python Lab 1 - Pandas Group Operations",
    "section": "",
    "text": "Load Data\n\nimport pandas as pd\ndf_ny = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_pop.csv')\ndf_ny.head(10)\n\n\n\n\n\n  \n    \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n  \n  \n    \n      0\n      36001\n      Albany\n      2015\n      55120\n      44478\n      204024\n    \n    \n      1\n      36001\n      Albany\n      2016\n      55126\n      45357\n      204003\n    \n    \n      2\n      36001\n      Albany\n      2017\n      58814\n      45589\n      204833\n    \n    \n      3\n      36001\n      Albany\n      2018\n      59547\n      45521\n      204509\n    \n    \n      4\n      36001\n      Albany\n      2019\n      61876\n      45150\n      204918\n    \n    \n      5\n      36001\n      Albany\n      2020\n      66632\n      44608\n      205082\n    \n    \n      6\n      36003\n      Allegany\n      2015\n      32205\n      7461\n      30568\n    \n    \n      7\n      36003\n      Allegany\n      2016\n      32417\n      7493\n      30449\n    \n    \n      8\n      36003\n      Allegany\n      2017\n      34001\n      7377\n      30331\n    \n    \n      9\n      36003\n      Allegany\n      2018\n      34553\n      7284\n      30155\n    \n  \n\n\n\n\n\nVariable Description\n\nFIPS: ID number for a county\npincp: average personal income in a county X in year Y\npop_18_24: population 18 to 24 years\npop_25_over: population 25 years and over\n\n\n\n\nQ1a\n\nUse .sort_values() to find the top 5 rich counties in NY for each year.\n\nDo not use .apply().\n\n\n\n# Sorts the DataFrame 'df_ny' by the column 'pincp' in descending order \n# and takes the top 5 records for each 'year' group.\n# Then, sorts the resulting DataFrame by 'year' in ascending order and 'pincp' in descending order.\nq1a = (\n      df_ny\n      .sort_values(by='pincp', ascending=False)\n      .groupby('year').head(5)\n      .sort_values(by=['year', 'pincp'], ascending=[True, False])\n    )\n\nq1a\n\n\n\n\n\n  \n    \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n  \n  \n    \n      180\n      36061\n      New York\n      2015\n      152793\n      161844\n      1229036\n    \n    \n      354\n      36119\n      Westchester\n      2015\n      93495\n      83942\n      659258\n    \n    \n      174\n      36059\n      Nassau\n      2015\n      79301\n      120229\n      931785\n    \n    \n      270\n      36091\n      Saratoga\n      2015\n      61407\n      18696\n      156862\n    \n    \n      306\n      36103\n      Suffolk\n      2015\n      61203\n      138167\n      1022970\n    \n    \n      181\n      36061\n      New York\n      2016\n      163112\n      158011\n      1237623\n    \n    \n      355\n      36119\n      Westchester\n      2016\n      96251\n      85574\n      661283\n    \n    \n      175\n      36059\n      Nassau\n      2016\n      81500\n      121621\n      934765\n    \n    \n      307\n      36103\n      Suffolk\n      2016\n      63757\n      138819\n      1024860\n    \n    \n      271\n      36091\n      Saratoga\n      2016\n      63065\n      18928\n      158211\n    \n    \n      182\n      36061\n      New York\n      2017\n      179655\n      155089\n      1259137\n    \n    \n      356\n      36119\n      Westchester\n      2017\n      102861\n      86345\n      668102\n    \n    \n      176\n      36059\n      Nassau\n      2017\n      85859\n      122061\n      942504\n    \n    \n      308\n      36103\n      Suffolk\n      2017\n      66429\n      138040\n      1030141\n    \n    \n      272\n      36091\n      Saratoga\n      2017\n      65490\n      18899\n      160285\n    \n    \n      183\n      36061\n      New York\n      2018\n      184539\n      149638\n      1247071\n    \n    \n      357\n      36119\n      Westchester\n      2018\n      107252\n      85757\n      665958\n    \n    \n      177\n      36059\n      Nassau\n      2018\n      89242\n      120849\n      940610\n    \n    \n      273\n      36091\n      Saratoga\n      2018\n      70010\n      18915\n      161493\n    \n    \n      309\n      36103\n      Suffolk\n      2018\n      69209\n      136173\n      1028820\n    \n    \n      184\n      36061\n      New York\n      2019\n      187213\n      147692\n      1249365\n    \n    \n      358\n      36119\n      Westchester\n      2019\n      112037\n      85499\n      668290\n    \n    \n      178\n      36059\n      Nassau\n      2019\n      92159\n      119754\n      943013\n    \n    \n      274\n      36091\n      Saratoga\n      2019\n      72219\n      19003\n      163010\n    \n    \n      310\n      36103\n      Suffolk\n      2019\n      72180\n      134777\n      1030842\n    \n    \n      185\n      36061\n      New York\n      2020\n      191220\n      145611\n      1250303\n    \n    \n      359\n      36119\n      Westchester\n      2020\n      115386\n      85113\n      670717\n    \n    \n      179\n      36059\n      Nassau\n      2020\n      96253\n      118047\n      945100\n    \n    \n      275\n      36091\n      Saratoga\n      2020\n      77398\n      18725\n      164817\n    \n    \n      311\n      36103\n      Suffolk\n      2020\n      76713\n      133732\n      1033886\n    \n  \n\n\n\n\n\n\nQ1b\n\nUse .rank() to find the top 5 rich counties in NY for each year.\n\nDo not use apply().\n\n\n\n# Creates a new column 'ranking' in DataFrame 'df_ny' using the 'pincp' column to calculate rankings within each 'year' group\nq1b = (\n   df_ny.assign(\n          ranking = df_ny.groupby('year')['pincp']\n                         .rank(method = 'dense', ascending = False)\n                         )\n)\n\n# Filters the records in DataFrame 'q1b' where the 'ranking' column is less than or equal to 5.\n# Then, sorts the resulting DataFrame by 'year' and 'ranking'.\nq1b = (\n  q1b.query('ranking <= 5')\n  .sort_values(by=['year', 'ranking'])\n)\n\nq1b\n\n\n\n\n\n  \n    \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n      ranking\n    \n  \n  \n    \n      180\n      36061\n      New York\n      2015\n      152793\n      161844\n      1229036\n      1.0\n    \n    \n      354\n      36119\n      Westchester\n      2015\n      93495\n      83942\n      659258\n      2.0\n    \n    \n      174\n      36059\n      Nassau\n      2015\n      79301\n      120229\n      931785\n      3.0\n    \n    \n      270\n      36091\n      Saratoga\n      2015\n      61407\n      18696\n      156862\n      4.0\n    \n    \n      306\n      36103\n      Suffolk\n      2015\n      61203\n      138167\n      1022970\n      5.0\n    \n    \n      181\n      36061\n      New York\n      2016\n      163112\n      158011\n      1237623\n      1.0\n    \n    \n      355\n      36119\n      Westchester\n      2016\n      96251\n      85574\n      661283\n      2.0\n    \n    \n      175\n      36059\n      Nassau\n      2016\n      81500\n      121621\n      934765\n      3.0\n    \n    \n      307\n      36103\n      Suffolk\n      2016\n      63757\n      138819\n      1024860\n      4.0\n    \n    \n      271\n      36091\n      Saratoga\n      2016\n      63065\n      18928\n      158211\n      5.0\n    \n    \n      182\n      36061\n      New York\n      2017\n      179655\n      155089\n      1259137\n      1.0\n    \n    \n      356\n      36119\n      Westchester\n      2017\n      102861\n      86345\n      668102\n      2.0\n    \n    \n      176\n      36059\n      Nassau\n      2017\n      85859\n      122061\n      942504\n      3.0\n    \n    \n      308\n      36103\n      Suffolk\n      2017\n      66429\n      138040\n      1030141\n      4.0\n    \n    \n      272\n      36091\n      Saratoga\n      2017\n      65490\n      18899\n      160285\n      5.0\n    \n    \n      183\n      36061\n      New York\n      2018\n      184539\n      149638\n      1247071\n      1.0\n    \n    \n      357\n      36119\n      Westchester\n      2018\n      107252\n      85757\n      665958\n      2.0\n    \n    \n      177\n      36059\n      Nassau\n      2018\n      89242\n      120849\n      940610\n      3.0\n    \n    \n      273\n      36091\n      Saratoga\n      2018\n      70010\n      18915\n      161493\n      4.0\n    \n    \n      309\n      36103\n      Suffolk\n      2018\n      69209\n      136173\n      1028820\n      5.0\n    \n    \n      184\n      36061\n      New York\n      2019\n      187213\n      147692\n      1249365\n      1.0\n    \n    \n      358\n      36119\n      Westchester\n      2019\n      112037\n      85499\n      668290\n      2.0\n    \n    \n      178\n      36059\n      Nassau\n      2019\n      92159\n      119754\n      943013\n      3.0\n    \n    \n      274\n      36091\n      Saratoga\n      2019\n      72219\n      19003\n      163010\n      4.0\n    \n    \n      310\n      36103\n      Suffolk\n      2019\n      72180\n      134777\n      1030842\n      5.0\n    \n    \n      185\n      36061\n      New York\n      2020\n      191220\n      145611\n      1250303\n      1.0\n    \n    \n      359\n      36119\n      Westchester\n      2020\n      115386\n      85113\n      670717\n      2.0\n    \n    \n      179\n      36059\n      Nassau\n      2020\n      96253\n      118047\n      945100\n      3.0\n    \n    \n      275\n      36091\n      Saratoga\n      2020\n      77398\n      18725\n      164817\n      4.0\n    \n    \n      311\n      36103\n      Suffolk\n      2020\n      76713\n      133732\n      1033886\n      5.0\n    \n  \n\n\n\n\n\n\nQ1c\n\nUse apply() with a lambda function and .sort_values() to find the top 5 rich counties in NY for each year.\n\n\n# Groups the DataFrame 'df_ny' by 'year' and applies a lambda function on each group.\n# The lambda function sorts each group by 'pincp' in descending order and selects the top 5 records.\n# The resulting DataFrame is the concatenation of these groups.\nq1c = (\n      df_ny\n      .groupby('year')\n      .apply(lambda x: x.sort_values(['pincp'], ascending=False).head())\n    )\n\nq1c\n\n\n\n\n\n  \n    \n      \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      180\n      36061\n      New York\n      2015\n      152793\n      161844\n      1229036\n    \n    \n      354\n      36119\n      Westchester\n      2015\n      93495\n      83942\n      659258\n    \n    \n      174\n      36059\n      Nassau\n      2015\n      79301\n      120229\n      931785\n    \n    \n      270\n      36091\n      Saratoga\n      2015\n      61407\n      18696\n      156862\n    \n    \n      306\n      36103\n      Suffolk\n      2015\n      61203\n      138167\n      1022970\n    \n    \n      2016\n      181\n      36061\n      New York\n      2016\n      163112\n      158011\n      1237623\n    \n    \n      355\n      36119\n      Westchester\n      2016\n      96251\n      85574\n      661283\n    \n    \n      175\n      36059\n      Nassau\n      2016\n      81500\n      121621\n      934765\n    \n    \n      307\n      36103\n      Suffolk\n      2016\n      63757\n      138819\n      1024860\n    \n    \n      271\n      36091\n      Saratoga\n      2016\n      63065\n      18928\n      158211\n    \n    \n      2017\n      182\n      36061\n      New York\n      2017\n      179655\n      155089\n      1259137\n    \n    \n      356\n      36119\n      Westchester\n      2017\n      102861\n      86345\n      668102\n    \n    \n      176\n      36059\n      Nassau\n      2017\n      85859\n      122061\n      942504\n    \n    \n      308\n      36103\n      Suffolk\n      2017\n      66429\n      138040\n      1030141\n    \n    \n      272\n      36091\n      Saratoga\n      2017\n      65490\n      18899\n      160285\n    \n    \n      2018\n      183\n      36061\n      New York\n      2018\n      184539\n      149638\n      1247071\n    \n    \n      357\n      36119\n      Westchester\n      2018\n      107252\n      85757\n      665958\n    \n    \n      177\n      36059\n      Nassau\n      2018\n      89242\n      120849\n      940610\n    \n    \n      273\n      36091\n      Saratoga\n      2018\n      70010\n      18915\n      161493\n    \n    \n      309\n      36103\n      Suffolk\n      2018\n      69209\n      136173\n      1028820\n    \n    \n      2019\n      184\n      36061\n      New York\n      2019\n      187213\n      147692\n      1249365\n    \n    \n      358\n      36119\n      Westchester\n      2019\n      112037\n      85499\n      668290\n    \n    \n      178\n      36059\n      Nassau\n      2019\n      92159\n      119754\n      943013\n    \n    \n      274\n      36091\n      Saratoga\n      2019\n      72219\n      19003\n      163010\n    \n    \n      310\n      36103\n      Suffolk\n      2019\n      72180\n      134777\n      1030842\n    \n    \n      2020\n      185\n      36061\n      New York\n      2020\n      191220\n      145611\n      1250303\n    \n    \n      359\n      36119\n      Westchester\n      2020\n      115386\n      85113\n      670717\n    \n    \n      179\n      36059\n      Nassau\n      2020\n      96253\n      118047\n      945100\n    \n    \n      275\n      36091\n      Saratoga\n      2020\n      77398\n      18725\n      164817\n    \n    \n      311\n      36103\n      Suffolk\n      2020\n      76713\n      133732\n      1033886\n    \n  \n\n\n\n\n\n\nQ1d\n\nWrite a function with def and .sort_values() that selects the top 5 pincp values.\nThen, use the defined function in apply() to find the top 5 rich counties in NY for each year.\n\n\n# Defines a function 'top' that sorts a DataFrame by the specified column in descending order and \n# selects the top 'n' records. If 'n' is not specified, the function selects the top 5 records.\ndef top(df, n=5, column=\"pincp\"):\n    return df.sort_values(column, ascending=False)[:n]\n  \n# Groups the DataFrame 'df_ny' by 'year' and applies the 'top' function on each group.\n# The resulting DataFrame is the concatenation of these groups.\nq1d = df_ny.groupby('year').apply(top)\n\nq1d\n\n\n\n\n\n  \n    \n      \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      180\n      36061\n      New York\n      2015\n      152793\n      161844\n      1229036\n    \n    \n      354\n      36119\n      Westchester\n      2015\n      93495\n      83942\n      659258\n    \n    \n      174\n      36059\n      Nassau\n      2015\n      79301\n      120229\n      931785\n    \n    \n      270\n      36091\n      Saratoga\n      2015\n      61407\n      18696\n      156862\n    \n    \n      306\n      36103\n      Suffolk\n      2015\n      61203\n      138167\n      1022970\n    \n    \n      2016\n      181\n      36061\n      New York\n      2016\n      163112\n      158011\n      1237623\n    \n    \n      355\n      36119\n      Westchester\n      2016\n      96251\n      85574\n      661283\n    \n    \n      175\n      36059\n      Nassau\n      2016\n      81500\n      121621\n      934765\n    \n    \n      307\n      36103\n      Suffolk\n      2016\n      63757\n      138819\n      1024860\n    \n    \n      271\n      36091\n      Saratoga\n      2016\n      63065\n      18928\n      158211\n    \n    \n      2017\n      182\n      36061\n      New York\n      2017\n      179655\n      155089\n      1259137\n    \n    \n      356\n      36119\n      Westchester\n      2017\n      102861\n      86345\n      668102\n    \n    \n      176\n      36059\n      Nassau\n      2017\n      85859\n      122061\n      942504\n    \n    \n      308\n      36103\n      Suffolk\n      2017\n      66429\n      138040\n      1030141\n    \n    \n      272\n      36091\n      Saratoga\n      2017\n      65490\n      18899\n      160285\n    \n    \n      2018\n      183\n      36061\n      New York\n      2018\n      184539\n      149638\n      1247071\n    \n    \n      357\n      36119\n      Westchester\n      2018\n      107252\n      85757\n      665958\n    \n    \n      177\n      36059\n      Nassau\n      2018\n      89242\n      120849\n      940610\n    \n    \n      273\n      36091\n      Saratoga\n      2018\n      70010\n      18915\n      161493\n    \n    \n      309\n      36103\n      Suffolk\n      2018\n      69209\n      136173\n      1028820\n    \n    \n      2019\n      184\n      36061\n      New York\n      2019\n      187213\n      147692\n      1249365\n    \n    \n      358\n      36119\n      Westchester\n      2019\n      112037\n      85499\n      668290\n    \n    \n      178\n      36059\n      Nassau\n      2019\n      92159\n      119754\n      943013\n    \n    \n      274\n      36091\n      Saratoga\n      2019\n      72219\n      19003\n      163010\n    \n    \n      310\n      36103\n      Suffolk\n      2019\n      72180\n      134777\n      1030842\n    \n    \n      2020\n      185\n      36061\n      New York\n      2020\n      191220\n      145611\n      1250303\n    \n    \n      359\n      36119\n      Westchester\n      2020\n      115386\n      85113\n      670717\n    \n    \n      179\n      36059\n      Nassau\n      2020\n      96253\n      118047\n      945100\n    \n    \n      275\n      36091\n      Saratoga\n      2020\n      77398\n      18725\n      164817\n    \n    \n      311\n      36103\n      Suffolk\n      2020\n      76713\n      133732\n      1033886\n    \n  \n\n\n\n\n\n\ncf) SeriesGroupBy.nlargest()\n\n# set the 'county_name' column as the index\ndf_ny.set_index('county_name', inplace=True) # to know what counties are corresponding to selected rows after `SeriesGroupBy.nlargest()` operation.\n\n\n# Groups the DataFrame 'df_ny' by 'year' and selects the 'pincp' column of each group.\n# For each group, selects the top 5 records based on the values in the 'pincp' column.\nq1cf = df_ny.groupby('year')['pincp'].nlargest(5)\n\nq1cf\n\nyear  county_name\n2015  New York       152793\n      Westchester     93495\n      Nassau          79301\n      Saratoga        61407\n      Suffolk         61203\n2016  New York       163112\n      Westchester     96251\n      Nassau          81500\n      Suffolk         63757\n      Saratoga        63065\n2017  New York       179655\n      Westchester    102861\n      Nassau          85859\n      Suffolk         66429\n      Saratoga        65490\n2018  New York       184539\n      Westchester    107252\n      Nassau          89242\n      Saratoga        70010\n      Suffolk         69209\n2019  New York       187213\n      Westchester    112037\n      Nassau          92159\n      Saratoga        72219\n      Suffolk         72180\n2020  New York       191220\n      Westchester    115386\n      Nassau          96253\n      Saratoga        77398\n      Suffolk         76713\nName: pincp, dtype: int64\n\n\n\n\nQ1e\n\nVisualize the yearly trend of the mean level of pincp.\n\n\n# Groups the DataFrame 'df_ny' by 'year' and selects the 'pincp' column of each group.\n# Calculates the mean of the 'pincp' column for each group.\nq1e = df_ny.groupby('year')['pincp'].mean()\n\n# Plots the resulting Series using the default line plot.\nq1e.plot()\n\n<AxesSubplot:xlabel='year'>"
  }
]