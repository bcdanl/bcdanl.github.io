[
  {
    "objectID": "DANL200_midterm-spring-2022-q.html",
    "href": "DANL200_midterm-spring-2022-q.html",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q1a.",
    "href": "DANL200_midterm-spring-2022-q.html#q1a.",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1a.",
    "text": "Q1a.\nDownload dominick_oj_q1a.csv from the Midterm Exam in the Assignments or the Files sections in our Canvas.\nThen import the dominick_oj_q1a.csv using the following lines:\n\noj_q1a <- read_csv('ABSOLUTE_PATH_NAME_FOR_THE_FILE_dominick_oj_q1a.csv')\ntable(oj_q1a$brand)\n\n\nYou need to provide the absolute path name for the file, dominick_oj_q1a.csv to the above read_csv() function to read the file.\n\nVariable Description\n\nsales: the number of orange juice (OJ) cartons sold in a week\nprice: price of OJ carton\nbrand: OJ brand\nfeat: Advertisement status— 1 if advertised; 0 if not advertised.\nReport (1) minimum, (2) median, (3) maximum, (4) mean, and (5) standard deviation of variable price for the brand, Dominick’s OJ."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q1b",
    "href": "DANL200_midterm-spring-2022-q.html#q1b",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1b",
    "text": "Q1b\nFor Question 1b, run the following function to read the dominick_oj.csv file:\n\noj_q1b <- read_csv(\n  'https://bcdanl.github.io/data/dominick_oj.csv'\n)\n\n\nThe description of variables in oj_q1b is the same as oj_q1a.\n\nDescribe the relationship between the log of price and the log of sales by brand using ggplot. Make a simple comment on your ggplot figure."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q2a",
    "href": "DANL200_midterm-spring-2022-q.html#q2a",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2a",
    "text": "Q2a\nDescribe the distribution of animal_gender using ggplot. Make a simple comment on your ggplot figure."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q2b",
    "href": "DANL200_midterm-spring-2022-q.html#q2b",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2b",
    "text": "Q2b\nFind the five most popular breeds in NYC."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q2c",
    "href": "DANL200_midterm-spring-2022-q.html#q2c",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2c",
    "text": "Q2c\nDescribe the relationship between the five popular breeds and borough using ggplot. Make a simple comment on your ggplot figure."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q2d",
    "href": "DANL200_midterm-spring-2022-q.html#q2d",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2d",
    "text": "Q2d\nFind the five most popular breeds for each borough in NYC."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q2e",
    "href": "DANL200_midterm-spring-2022-q.html#q2e",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2e",
    "text": "Q2e\nFind the five most popular dog names for each gender in NYC."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q2f",
    "href": "DANL200_midterm-spring-2022-q.html#q2f",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2f",
    "text": "Q2f\nFind the five most popular dog names for each gender for each borough in NYC."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q2g",
    "href": "DANL200_midterm-spring-2022-q.html#q2g",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2g",
    "text": "Q2g\nAssume that all dogs in the nyc_dogs data frame are alive as of today.\nDescribe the distribution of age for each borough using ggplot. Make a simple comment on your ggplot."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q3a.",
    "href": "DANL200_midterm-spring-2022-q.html#q3a.",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3a.",
    "text": "Q3a.\nCreate a variable, payroll, which is defined as:\n\\[\n\\begin{align}\n\\texttt{payroll} = \\texttt{regular\\_gross\\_paid} + \\texttt{total\\_ot\\_paid}\n\\end{align}\n\\]"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q3b.",
    "href": "DANL200_midterm-spring-2022-q.html#q3b.",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3b.",
    "text": "Q3b.\nCalculate the mean of payroll by title_description."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#q3c.",
    "href": "DANL200_midterm-spring-2022-q.html#q3c.",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3c.",
    "text": "Q3c.\nCalculate the mean of payroll by work_location_borough."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-q.html#variable-description-1",
    "href": "DANL200_midterm-spring-2022-q.html#variable-description-1",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Variable Description",
    "text": "Variable Description\n\nFiscal Year: Fiscal Year\nPayroll Number: Payroll Number\nAgency Name: The Payroll agency that the employee works for\nLast Name: Last name of employee\nFirst Name: First name of employee\nMid Init: Middle initial of employee\nAgency Start Date: Date which employee began working for their current agency Date & Time\nWork Location Borough: Borough of employee’s primary work location\nTitle Description: Civil service title description of the employee\nLeave Status as of June 30: Status of employee as of the close of the relevant fiscal year: Active, Ceased, or On Leave\nBase Salary: Base Salary assigned to the employee\nPay Basis: Lists whether the employee is paid on an hourly, per diem or annual basis\nRegular Hours: Number of regular hours employee worked in the fiscal year\nRegular Gross Paid: The amount paid to the employee for base salary during the fiscal year\nOT Hours: Overtime Hours worked by employee in the fiscal year\nTotal OT Paid: Total overtime pay paid to the employee in the fiscal year\nTotal Other Pay: Includes any compensation in addition to gross salary and overtime pay, ie Differentials, lump sums, uniform allowance, meal allowance, retroactive pay increases, settlement amounts, and bonus pay, if applicable."
  },
  {
    "objectID": "DANL210_lab2a.html",
    "href": "DANL210_lab2a.html",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab2a.html#load-dataframe",
    "href": "DANL210_lab2a.html#load-dataframe",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbeer_mkt = pd.read_csv('https://bcdanl.github.io/data/beer_markets.csv')\nbeer_mkt.head(10)\n\n\n\n\n\n  \n    \n      \n      hh\n      _purchase_desc\n      quantity\n      brand\n      dollar_spent\n      beer_floz\n      price_per_floz\n      container\n      promo\n      market\n      ...\n      age\n      employment\n      degree\n      cow\n      race\n      microwave\n      dishwasher\n      tvcable\n      singlefamilyhome\n      npeople\n    \n  \n  \n    \n      0\n      2000946\n      BUD LT BR CN 12P\n      1\n      BUD LIGHT\n      8.14\n      144.0\n      0.056528\n      CAN\n      False\n      RURAL ILLINOIS\n      ...\n      50+\n      none\n      Grad\n      none/retired/student\n      white\n      True\n      True\n      premium\n      False\n      1\n    \n    \n      1\n      2003036\n      BUD LT BR CN 24P\n      1\n      BUD LIGHT\n      17.48\n      288.0\n      0.060694\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      2\n      2003036\n      BUD LT BR CN 24P\n      2\n      BUD LIGHT\n      33.92\n      576.0\n      0.058889\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      3\n      2003036\n      BUD LT BR CN 30P\n      2\n      BUD LIGHT\n      34.74\n      720.0\n      0.048250\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      4\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      40.48\n      864.0\n      0.046852\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      5\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      42.96\n      864.0\n      0.049722\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      6\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      40.96\n      864.0\n      0.047407\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      7\n      2001521\n      BUD LT BR CN 6P\n      5\n      BUD LIGHT\n      30.60\n      480.0\n      0.063750\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n    \n      8\n      2001521\n      BUD LT BR CN 6P\n      1\n      BUD LIGHT\n      9.99\n      96.0\n      0.104063\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n    \n      9\n      2001521\n      BUD LT BR CN 6P\n      5\n      BUD LIGHT\n      30.70\n      480.0\n      0.063958\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n  \n\n10 rows × 24 columns\n\n\n\n\nVariable Description\n\nhh: An identifier of the purchasing household;\n_purchase_desc: Details on the purchased item;\nquantity: Number of items purchased;\nbrand: BUD LIGHT, BUSCH LIGHT, COORS LIGHT, MILLER LITE, or NATURAL LIGHT;\nspent: Total dollar value of purchase;\nbeer_floz: Total volume of beer, in fluid ounces;\nprice_per_floz: Price per fl.oz. (i.e., spent/beer_floz);\ncontainer: Type of container;\npromo: Whether the item was promoted (coupon or something else);\nmarket: Scan-track market (or state if rural);\nvarious demographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher.\n\nSummarize DataFrame beer_mkt.\n\n\nskim(beer_mkt)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 73115  │ │ string      │ 13    │                                                          │\n│ │ Number of columns │ 24     │ │ bool        │ 6     │                                                          │\n│ └───────────────────┴────────┘ │ float64     │ 3     │                                                          │\n│                                │ int64       │ 2     │                                                          │\n│                                └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name      ┃ NA  ┃ NA %  ┃ mean      ┃ sd        ┃ p0       ┃ p25     ┃ p75      ┃ p100     ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━┩  │\n│ │ hh               │   0 │     0 │  17000000 │  12000000 │  2000000 │ 8200000 │ 30000000 │ 30000000 │ ▂█   █ │  │\n│ │ quantity         │   0 │     0 │       1.3 │       1.1 │        1 │       1 │        1 │       48 │   █    │  │\n│ │ dollar_spent     │   0 │     0 │        14 │       8.7 │     0.51 │       9 │       16 │      160 │   █▁   │  │\n│ │ beer_floz        │   0 │     0 │       270 │       200 │       12 │     140 │      360 │     9200 │   █    │  │\n│ │ price_per_floz   │   0 │     0 │     0.056 │     0.013 │   0.0013 │   0.046 │    0.064 │     0.23 │   ▁█   │  │\n│ └──────────────────┴─────┴───────┴───────────┴───────────┴──────────┴─────────┴──────────┴──────────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                   ┃ NA     ┃ NA %       ┃ words per row               ┃ total words            ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ _purchase_desc                │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ brand                         │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ container                     │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ market                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ buyertype                     │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ income                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ age                           │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ employment                    │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ degree                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ cow                           │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ race                          │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ tvcable                       │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ npeople                       │      0 │          0 │                         5.3 │                 390000 │  │\n│ └───────────────────────────────┴────────┴────────────┴─────────────────────────────┴────────────────────────┘  │\n│                                                      bool                                                       │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                               ┃ true            ┃ true rate                 ┃ hist             ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩  │\n│ │ promo                                     │           15000 │                       0.2 │      █    ▂      │  │\n│ │ childrenUnder6                            │            5000 │                     0.068 │      █    ▁      │  │\n│ │ children6to17                             │           15000 │                       0.2 │      █    ▂      │  │\n│ │ microwave                                 │           73000 │                      0.99 │           █      │  │\n│ │ dishwasher                                │           53000 │                      0.73 │      ▃    █      │  │\n│ │ singlefamilyhome                          │           59000 │                      0.81 │      ▂    █      │  │\n│ └───────────────────────────────────────────┴─────────────────┴───────────────────────────┴──────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nbeer_mkt.describe()\n\n\n\n\n\n  \n    \n      \n      hh\n      quantity\n      dollar_spent\n      beer_floz\n      price_per_floz\n    \n  \n  \n    \n      count\n      7.311500e+04\n      73115.000000\n      73115.000000\n      73115.000000\n      73115.000000\n    \n    \n      mean\n      1.740772e+07\n      1.317527\n      13.777683\n      265.926853\n      0.055951\n    \n    \n      std\n      1.158215e+07\n      1.149649\n      8.722942\n      199.522488\n      0.013417\n    \n    \n      min\n      2.000235e+06\n      1.000000\n      0.510000\n      12.000000\n      0.001315\n    \n    \n      25%\n      8.223438e+06\n      1.000000\n      8.970000\n      144.000000\n      0.046306\n    \n    \n      50%\n      8.413624e+06\n      1.000000\n      12.990000\n      216.000000\n      0.055509\n    \n    \n      75%\n      3.017132e+07\n      1.000000\n      16.380000\n      360.000000\n      0.063750\n    \n    \n      max\n      3.044072e+07\n      48.000000\n      159.130000\n      9216.000000\n      0.234063\n    \n  \n\n\n\n\n\nbeer_mkt.groupby('brand').describe()\n\n\n\n\n\n  \n    \n      \n      hh\n      quantity\n      ...\n      beer_floz\n      price_per_floz\n    \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n      count\n      mean\n      ...\n      75%\n      max\n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      brand\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      BUD LIGHT\n      21592.0\n      1.728292e+07\n      1.151146e+07\n      2000235.0\n      8212165.0\n      8403502.0\n      30173350.0\n      30439375.0\n      21592.0\n      1.326834\n      ...\n      288.0\n      2880.0\n      21592.0\n      0.061601\n      0.012090\n      0.002593\n      0.055417\n      0.060708\n      0.067604\n      0.187500\n    \n    \n      BUSCH LIGHT\n      8674.0\n      1.950770e+07\n      1.154444e+07\n      2001531.0\n      8274694.0\n      30022041.0\n      30199810.0\n      30440718.0\n      8674.0\n      1.326954\n      ...\n      360.0\n      2304.0\n      8674.0\n      0.044922\n      0.009065\n      0.003734\n      0.039972\n      0.044340\n      0.048542\n      0.234063\n    \n    \n      COORS LIGHT\n      13074.0\n      1.777439e+07\n      1.167094e+07\n      2000417.0\n      8207890.0\n      9000465.0\n      30173465.0\n      30440718.0\n      13074.0\n      1.267171\n      ...\n      360.0\n      1984.0\n      13074.0\n      0.060638\n      0.012364\n      0.004583\n      0.052833\n      0.060000\n      0.067361\n      0.180417\n    \n    \n      MILLER LITE\n      17159.0\n      1.743478e+07\n      1.160809e+07\n      2000946.0\n      8239785.0\n      8425198.0\n      30181493.0\n      30440718.0\n      17159.0\n      1.266915\n      ...\n      288.0\n      9216.0\n      17159.0\n      0.059676\n      0.011804\n      0.001315\n      0.052500\n      0.059028\n      0.065903\n      0.173333\n    \n    \n      NATURAL LIGHT\n      12616.0\n      1.576071e+07\n      1.134515e+07\n      2001715.0\n      8199800.0\n      8354289.0\n      30130924.0\n      30440718.0\n      12616.0\n      1.416138\n      ...\n      360.0\n      2160.0\n      12616.0\n      0.043939\n      0.008032\n      0.003467\n      0.039444\n      0.043368\n      0.048542\n      0.123750\n    \n  \n\n5 rows × 40 columns"
  },
  {
    "objectID": "DANL210_lab2a.html#q1a",
    "href": "DANL210_lab2a.html#q1a",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1a",
    "text": "Q1a\n\nSort the DataFrame beer_mkt by hh in ascending order.\n\n\nbeer_mkt = beer_mkt.sort_values('hh')"
  },
  {
    "objectID": "DANL210_lab2a.html#q1b",
    "href": "DANL210_lab2a.html#q1b",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1b",
    "text": "Q1b\n\nFind the top 5 beer markets in terms of the number of households that purchased beer.\n\n\n# value_counts()\nq1b = (\n       beer_mkt[['market', 'hh']]\n       .value_counts()\n       .sort_index()\n       .groupby('market')\n       .count()\n       .sort_values(ascending = False)\n       )\n       \n# size()\nq1b = (\n       beer_mkt.groupby(['market', 'hh'])\n       .size()\n       .count(level='market')\n       .sort_values(ascending = False)\n       )\n\n\n# adding a ranking variable to the DataFrame\nq1b = pd.DataFrame(q1b, columns=['n_hh'])\nq1b['ranking'] = q1b['n_hh'].rank(method = 'dense', ascending = False)\nq1b = q1b.query('ranking <= 5')"
  },
  {
    "objectID": "DANL210_lab2a.html#q1c",
    "href": "DANL210_lab2a.html#q1c",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1c",
    "text": "Q1c\n\nFind the top 5 beer markets in terms of the amount of total beer consumption.\n\n\nq1c = (\n       beer_mkt\n       .groupby('market')[['beer_floz']]\n       .sum()\n       .sort_values(by = 'beer_floz', ascending = False)\n       )\n\n# adding a ranking variable to the DataFrame\nq1c['ranking'] = q1c['beer_floz'].rank(method = 'dense', ascending = False)\nq1c = q1c.query('ranking <= 5')"
  },
  {
    "objectID": "DANL210_lab2a.html#q1d",
    "href": "DANL210_lab2a.html#q1d",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1d",
    "text": "Q1d\n\nProvide (1) seaborn code and (2) a simple comment to describe how the distribution of price_per_floz varies by brand.\n\n\nsns.displot(data = beer_mkt,\n            x = 'price_per_floz', bins = 200,\n            hue = 'brand', # for colorful histogram\n            row = 'brand')\n\n<seaborn.axisgrid.FacetGrid at 0x7ff3d00a98b0>"
  },
  {
    "objectID": "DANL210_lab2a.html#q1e",
    "href": "DANL210_lab2a.html#q1e",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1e",
    "text": "Q1e\n\nProvide (1) seaborn code and (2) a simple comment to describe how the relationship between price_per_floz and beer_floz varies by brand.\n\n\nsns.lmplot(data = beer_mkt,\n           x = \"beer_floz\",\n           y = \"price_per_floz\",\n           scatter_kws = {'alpha' : 0.1},\n           hue = 'brand',\n           row = 'brand')\n\n<seaborn.axisgrid.FacetGrid at 0x7ff3d133d970>\n\n\n\n\n\n\n# adding log transformed variables\nbeer_mkt['log_beer_floz'] = np.log(beer_mkt['beer_floz'])\nbeer_mkt['log_price_per_floz'] = np.log(beer_mkt['price_per_floz'])\n\n\nsns.lmplot(data = beer_mkt,\n           x = \"log_beer_floz\",\n           y = \"log_price_per_floz\",\n           scatter_kws = {'alpha' : 0.1},\n           hue = 'brand',\n           row = 'brand')\n\n<seaborn.axisgrid.FacetGrid at 0x7ff3d00fdd00>"
  },
  {
    "objectID": "mba-ch1-logit.html#logistic-regression-model",
    "href": "mba-ch1-logit.html#logistic-regression-model",
    "title": "Logistic Regression",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\nIn a regression model, we model the conditional mean for \\(y\\) given \\(x\\) as\n\n\\[\n\\begin{align}\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] &= \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}\\\\\n{ }\\\\\ny_{i} &=   \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1, i} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p, i} + \\epsilon_{i} \\quad \\text{for } i = 1, 2, ..., n\n\\end{align}\n\\] - Logistic regression is used to model a binary response:\n\n\\(y\\) is either 1 or 0 (e.g., True or False).\n\\(\\epsilon_{i}\\) is the random noise from logistic distribution whose cumulative density function is as follows:\n\n\nknitr::include_graphics('lec_figs/mba-1-11.png')\n\n\n\n\n\n\n\n\nwhere \\(z\\) is a linear combination of explanatory variables \\(\\mathbf{X}\\).\n\\[\nf(z) = \\frac{e^{z}}{1 + e^{z}}\n\\] - Since \\(y\\) is either 0 or 1, the conditional mean value of \\(y\\) is the probability:\n\\[\n\\begin{align}\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] &= \\text{Pr}(y = 1 | \\mathbf{X}) \\times 1 \\,+\\,\\text{Pr}(y = 0 | \\mathbf{X}) \\times 0\\\\ &= \\text{Pr}(y = 1 | \\mathbf{X}) \\\\\n{ }\\\\\n\\text{Pr}(y = 1 | \\mathbf{X}) &= f(\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p})\\\\\n&= \\frac{e^{\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}}}{1 + e^{\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}}} \\\\\n\\end{align}\n\\]\n\nThe logistic regression finds the beta coefficients, \\(b_{0}, b_{1}, \\cdots\\), such that the logistic function ranging from 0 to 1\n\n\\[\n\\begin{align}\nf( b_{0} + b_{1}*x_{i,1} + b_{2}*x_{i,2} + \\cdots )\\notag\n\\end{align}\n\\]\nis the best possible estimate of the binary outcome \\(y_{i}\\).\n\n\nInterpretation of Beta Estimates\n\nIn logistic regression, the effect of \\(x_{1}\\) on \\(Pr(y_{i} = 1 | \\mathbf{X})\\) is different across observations \\(i = 1, 2, \\cdots\\):\n\n\nknitr::include_graphics('lec_figs/effect-linear-logit.png')\n\n\n\n\n\n\n\n\n\n\n\nThe Goals of Logistic Regression:\n\nModeling for prediction (\\(\\text{Pr}({y} | \\mathbf{X})\\)): When we want to predict an outcome variable \\(y = 0 ,1\\) based on the information contained in a set of predictor variables \\(\\mathbf{X}\\).\n\n\nWe are estimating the conditional expectation (mean) for \\(y\\): \\[\n\\text{Pr}(\\, y \\,|\\, \\mathbf{X} \\,) = f(\\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}).\n\\]\nwhich is the probability that \\(y = 1\\) given the value for \\(X\\) from the logistic function.\nPrediction from the logistic regression with a threshold on the probabilities can be used as a classifier.\n\nIf the probability that the newborn baby i is at risk is greater than the threshold \\(\\theta\\in (0, 1)\\) (\\(\\text{Pr}(y_{i} = 1 | \\mathbf{X}) > \\theta\\)), the baby i is classified as at-risk.\n\nWe can discuss the performance of classifiers later.\n\nAccuracy: When the classifier says this newborn baby is at risk or is not at risk, what is the probability that the model is correct?\nPrecision: If the classifier says this newborn baby is at risk, what’s the probability that the baby is really at risk?\nRecall: Of all the babies at risk, what fraction did the classifier detect?\nThere is a trade-off between recall and precision.\n\n\n\n\nModeling for explanation (\\(\\hat{\\beta}\\)): When we want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(\\mathbf{X}\\).\n\n\nWe can average the marginal effects across the training data (average marginal effect, or AME).\nWe can obtain the marginal effect at an average observation or representative observations in the training data (marginal effect at the mean or at representative values).\nWe can also consider:\nThe AME for subgroup of the data\nThe AME at the mean value of VARIABLE.\n\n\n\n\nExample: Newbron Babies at risk\n\nWe’ll use a sample dataset from the 2010 CDC natality public-use data.\n\nThe data set records information about all US births, including risk factors about the mother and father, and about the delivery.\nNewborn babies are assessed at one and five minutes after birth to determine if a baby needs immediate emergency care or extra medical attention.\n\n\nTask 1. Identify the effects of several risk factors on the probability of atRisk == TRUE. Task 2. Classify ahead of time babies with a higher probability of atRisk == TRUE.\n\n\nLoad Packages and Data\n\n# install.packages(\"margins\")\nlibrary(tidyverse)\nlibrary(margins) # for AME\nlibrary(hrbrthemes) # for ggplot theme, theme_ipsum()\nlibrary(stargazer)\n\ntheme_set(theme_ipsum()) # setting theme_ipsum() default\nload(url(\"https://bcdanl.github.io/data/NatalRiskData.rData\"))\n\n# 50:50 split between training and testing data\ntrain <- filter(sdata,\n                ORIGRANDGROUP <= 5)\ntest <- filter(sdata,\n                ORIGRANDGROUP > 5)\nskim(train)\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n14212\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nlogical\n9\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGESTREC3\n0\n1\nFALSE\n2\n>= : 12651, < 3: 1561\n\n\nDPLURAL\n0\n1\nFALSE\n3\nsin: 13761, twi: 424, tri: 27\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nCIG_REC\n0\n1\n0.09\nFAL: 12913, TRU: 1299\n\n\nULD_MECO\n0\n1\n0.05\nFAL: 13542, TRU: 670\n\n\nULD_PRECIP\n0\n1\n0.03\nFAL: 13854, TRU: 358\n\n\nULD_BREECH\n0\n1\n0.06\nFAL: 13316, TRU: 896\n\n\nURF_DIAB\n0\n1\n0.05\nFAL: 13450, TRU: 762\n\n\nURF_CHYPER\n0\n1\n0.01\nFAL: 14046, TRU: 166\n\n\nURF_PHYPER\n0\n1\n0.04\nFAL: 13595, TRU: 617\n\n\nURF_ECLAM\n0\n1\n0.00\nFAL: 14181, TRU: 31\n\n\natRisk\n0\n1\n0.02\nFAL: 13939, TRU: 273\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPWGT\n0\n1\n153.28\n38.87\n74\n125\n145\n172\n375\n▆▇▂▁▁\n\n\nUPREVIS\n0\n1\n11.17\n4.02\n0\n9\n11\n13\n49\n▃▇▁▁▁\n\n\nDBWT\n0\n1\n3276.02\n582.91\n265\n2985\n3317\n3632\n6165\n▁▁▇▂▁\n\n\nORIGRANDGROUP\n0\n1\n2.53\n1.70\n0\n1\n3\n4\n5\n▇▅▅▅▅\n\n\n\n\n\n\n\n\nLinear Probability Model (LPM)\n\n# linear probability model\nlpm <- lm(atRisk ~ CIG_REC + GESTREC3 + DPLURAL + \n               ULD_MECO + ULD_PRECIP + ULD_BREECH + \n               URF_DIAB + URF_CHYPER + URF_PHYPER + URF_ECLAM, \n             data = train)\n\n\nLPM often works well when it comes to identifying AME.\nCaveats\n\nProbability of \\(y = 1\\) can be beyond [0, 1].\nThe error can’t be distributed Normal with \\(y_{i} \\in \\{0 , 1\\}\\).\nWhen using LPM, we should make standard errors of beta estimates robust to heteroskedasticity—variances of errors are non-constant across observations.\n\n\n\n\n\nLogistic Regression via glm( family = binomial(link = \"logit\") )\nmodel <- glm(atRisk ~ PWGT + UPREVIS + CIG_REC + GESTREC3 + DPLURAL + \n               ULD_MECO + ULD_PRECIP + ULD_BREECH + \n               URF_DIAB + URF_CHYPER + URF_PHYPER + URF_ECLAM, \n             data = train, \n             family = binomial(link = \"logit\") )\nstargazer(model, type = 'html')\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\natRisk\n\n\n\n\n\n\n\n\nPWGT\n\n\n0.004**\n\n\n\n\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\nUPREVIS\n\n\n-0.063***\n\n\n\n\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\nCIG_REC\n\n\n0.313*\n\n\n\n\n\n\n(0.187)\n\n\n\n\n\n\n\n\n\n\nGESTREC3< 37 weeks\n\n\n1.545***\n\n\n\n\n\n\n(0.141)\n\n\n\n\n\n\n\n\n\n\nDPLURALtriplet or higher\n\n\n1.394***\n\n\n\n\n\n\n(0.499)\n\n\n\n\n\n\n\n\n\n\nDPLURALtwin\n\n\n0.312\n\n\n\n\n\n\n(0.241)\n\n\n\n\n\n\n\n\n\n\nULD_MECO\n\n\n0.818***\n\n\n\n\n\n\n(0.236)\n\n\n\n\n\n\n\n\n\n\nULD_PRECIP\n\n\n0.192\n\n\n\n\n\n\n(0.358)\n\n\n\n\n\n\n\n\n\n\nULD_BREECH\n\n\n0.749***\n\n\n\n\n\n\n(0.178)\n\n\n\n\n\n\n\n\n\n\nURF_DIAB\n\n\n-0.346\n\n\n\n\n\n\n(0.288)\n\n\n\n\n\n\n\n\n\n\nURF_CHYPER\n\n\n0.560\n\n\n\n\n\n\n(0.390)\n\n\n\n\n\n\n\n\n\n\nURF_PHYPER\n\n\n0.162\n\n\n\n\n\n\n(0.250)\n\n\n\n\n\n\n\n\n\n\nURF_ECLAM\n\n\n0.498\n\n\n\n\n\n\n(0.777)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-4.412***\n\n\n\n\n\n\n(0.289)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n14,212\n\n\n\n\nLog Likelihood\n\n\n-1,231.496\n\n\n\n\nAkaike Inf. Crit.\n\n\n2,490.992\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nsummary(model)\n\n\nCall:\nglm(formula = atRisk ~ PWGT + UPREVIS + CIG_REC + GESTREC3 + \n    DPLURAL + ULD_MECO + ULD_PRECIP + ULD_BREECH + URF_DIAB + \n    URF_CHYPER + URF_PHYPER + URF_ECLAM, family = binomial(link = \"logit\"), \n    data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.9732  -0.1818  -0.1511  -0.1358   3.2641  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -4.412189   0.289352 -15.249  < 2e-16 ***\nPWGT                      0.003762   0.001487   2.530 0.011417 *  \nUPREVIS                  -0.063289   0.015252  -4.150 3.33e-05 ***\nCIG_RECTRUE               0.313169   0.187230   1.673 0.094398 .  \nGESTREC3< 37 weeks        1.545183   0.140795  10.975  < 2e-16 ***\nDPLURALtriplet or higher  1.394193   0.498866   2.795 0.005194 ** \nDPLURALtwin               0.312319   0.241088   1.295 0.195163    \nULD_MECOTRUE              0.818426   0.235798   3.471 0.000519 ***\nULD_PRECIPTRUE            0.191720   0.357680   0.536 0.591951    \nULD_BREECHTRUE            0.749237   0.178129   4.206 2.60e-05 ***\nURF_DIABTRUE             -0.346467   0.287514  -1.205 0.228187    \nURF_CHYPERTRUE            0.560025   0.389678   1.437 0.150676    \nURF_PHYPERTRUE            0.161599   0.250003   0.646 0.518029    \nURF_ECLAMTRUE             0.498064   0.776948   0.641 0.521489    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2698.7  on 14211  degrees of freedom\nResidual deviance: 2463.0  on 14198  degrees of freedom\nAIC: 2491\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nLogistic regression finds the beta parameters that maximize the log likelihood of the data, given the model, which is equivalent to minimizing the sum of the residual deviance.\n\nWe want to make the likelihood as big as possible.\nWe want to make the deviance as small as possible.\n\n\n\n\n\nLikelihood Function\n\nLikelihood is the probability of our data given the model.\n\n\nknitr::include_graphics('lec_figs/logistic_likelihood.png')\n\n\n\n\n\n\n\n\n\nThe probability that the seven data points would be observed: \\(L = (1-P1)*(1-P2)* P3*(1-P4)*P5*P6*P7\\).\n\nThe log of the likelihood: \\(\\log(L) = \\log(1-P1) + \\log(1-P2) + \\log(P3) + \\log(1-P4) + \\log(P5) + \\log(P6) + \\log(P7)\\)\n\nIn statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data.\n\nThis is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable.\n\n\n\n\n\nDeviance\n\nDeviance measures to the distance between data and fit\nThe null deviance is similar to the variance of the data around the average rate of positive examples.\n\n\n# likelihood function for logistic regression\n## y: the outcome in numeric form, either 1 or 0\n## py: the predicted probability that y == 1\n\nloglikelihood <- function(y, py) {\n  sum( y * log(py) + (1-y)*log(1 - py) )\n}\n\n# the rate of positive example in the dataset\npnull <- mean( as.numeric(train$atRisk) )\n\n# the null deviance\nnull.dev <- -2 *loglikelihood(as.numeric(train$atRisk), pnull)\n\n# the null deviance from summary(model)\nmodel$null.deviance\n\n[1] 2698.716\n\n# the predicted probability for the training data\npred <- predict(model, newdata=train, type = \"response\")\n\n# the residual deviance\nresid.dev <- -2 * loglikelihood(as.numeric(train$atRisk), pred)\n\n# the residual deviance from summary(model)\nmodel$deviance\n\n[1] 2462.992\n\n\n\n\n\nAIC, AICc, BIC and pseudo R-squared\n\nThe AIC, or the Akaike information criterion, is the log likelihood adjusted for the number of coefficients.\n\nThe corrected AIC, AICc, is the AIC corrected by the sample size and the degree of freedom, which is superior to the AIC.\n\nThe BIC, or Bayesian information criterion, attempts to approximate the posterior probability that each model is best.\n\nThe BIC can be useful only when in small sample settings.\n\nThe pseudo R-squared is a goodness-of-fit measure of how much of the deviance is “explained” by the model.\n\n\n# the AIC\nAIC <- 2 * ( length( model$coefficients ) -\n             loglikelihood( as.numeric(train$atRisk), pred) )\nAIC\n\n[1] 2490.992\n\nmodel$aic\n\n[1] 2490.992\n\n# the pseudo R-squared\npseudo_r2 <- 1 - (resid.dev / null.dev)\n\n\nRegression preserves the probabilities:\n\n\ntrain$pred <- predict(model, newdata=train, type = \"response\")\ntest$pred <- predict(model, newdata=test, type=\"response\")\n\nsum(train$atRisk == TRUE)\n\n[1] 273\n\nsum(train$pred)\n\n[1] 273\n\npremature <- subset(train, GESTREC3 == \"< 37 weeks\")\nsum(premature$atRisk == TRUE)\n\n[1] 112\n\nsum(premature$pred)\n\n[1] 112\n\n\n\n\n\nAverage Marginal Effects\n\nm <- margins(model)\name_result <- summary(m)\name_result\n\n                   factor     AME     SE       z      p   lower   upper\n                  CIG_REC  0.0064 0.0043  1.5000 0.1336 -0.0020  0.0148\n DPLURALtriplet or higher  0.0484 0.0290  1.6677 0.0954 -0.0085  0.1052\n              DPLURALtwin  0.0064 0.0056  1.1480 0.2510 -0.0045  0.0173\n       GESTREC3< 37 weeks  0.0450 0.0062  7.2235 0.0000  0.0328  0.0571\n                     PWGT  0.0001 0.0000  2.5096 0.0121  0.0000  0.0001\n               ULD_BREECH  0.0181 0.0056  3.2510 0.0012  0.0072  0.0290\n                 ULD_MECO  0.0211 0.0082  2.5718 0.0101  0.0050  0.0371\n               ULD_PRECIP  0.0038 0.0077  0.4946 0.6209 -0.0113  0.0189\n                  UPREVIS -0.0012 0.0003 -4.0631 0.0000 -0.0017 -0.0006\n               URF_CHYPER  0.0131 0.0115  1.1437 0.2528 -0.0094  0.0356\n                 URF_DIAB -0.0055 0.0040 -1.3887 0.1649 -0.0133  0.0023\n                URF_ECLAM  0.0114 0.0220  0.5196 0.6033 -0.0317  0.0545\n               URF_PHYPER  0.0031 0.0052  0.6066 0.5441 -0.0070  0.0133\n\nggplot(data = ame_result) +\n  geom_point( aes(factor, AME) ) +\n  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper), \n                width = .5) +\n  geom_hline(yintercept = 0) +\n  coord_flip()"
  },
  {
    "objectID": "mba-ch1-logit.html#references",
    "href": "mba-ch1-logit.html#references",
    "title": "Logistic Regression",
    "section": "References",
    "text": "References\n\n\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\nPractical Data Science with R by Nina Zumel and John Mount."
  },
  {
    "objectID": "DANL210_lab3q.html",
    "href": "DANL210_lab3q.html",
    "title": "Python Lab 3 - Tidy Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab3q.html#load-dataframe",
    "href": "DANL210_lab3q.html#load-dataframe",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbillboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')\nny_pincp = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_wide.csv')\ncovid = pd.read_csv('https://bcdanl.github.io/data/covid19_cases.csv')"
  },
  {
    "objectID": "DANL210_lab3q.html#q1a",
    "href": "DANL210_lab3q.html#q1a",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1a",
    "text": "Q1a\n\nDescribe how the distribution of rating varies across week 1, week 2, and week 3 using the faceted histogram."
  },
  {
    "objectID": "DANL210_lab3q.html#q1b",
    "href": "DANL210_lab3q.html#q1b",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1b",
    "text": "Q1b\n\nWhich artist(s) have the most number of tracks in billboard DataFrame?"
  },
  {
    "objectID": "DANL210_lab3q.html#q1c",
    "href": "DANL210_lab3q.html#q1c",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1c",
    "text": "Q1c\n\nMake ny_pincp longer."
  },
  {
    "objectID": "DANL210_lab3q.html#q1d",
    "href": "DANL210_lab3q.html#q1d",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1d",
    "text": "Q1d\n\nMake a wide-form DataFrame of covid whose variable names are from countriesAndTerritories and values are from cases."
  },
  {
    "objectID": "DANL210_lab3q.html#q1e",
    "href": "DANL210_lab3q.html#q1e",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1e",
    "text": "Q1e\n\nUse the wide-form DataFrame of covid to find the top 10 countries for which their cases are highly correlated with USA’s cases using DataFrame.corr()"
  },
  {
    "objectID": "DANL210_lab3q.html#load-dataframe-for-q2a-and-q2b",
    "href": "DANL210_lab3q.html#load-dataframe-for-q2a-and-q2b",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Load DataFrame for Q2a and Q2b",
    "text": "Load DataFrame for Q2a and Q2b\n\npaidsearch = pd.read_csv('https://bcdanl.github.io/data/paidsearch.csv')\n\n\nVariable description\n\ndma: an identification number of a designated market (DM) area i (e.g., Boston, Los Angeles)\ntreatment_period: 0 if date is before May 22, 2012 and 1 after.\nsearch_stays_on: 1 if the paid-search goes off in dma i, 0 otherwise.\nrevenue: eBay’s sales revenue for dma i and date t"
  },
  {
    "objectID": "DANL210_lab3q.html#q2a",
    "href": "DANL210_lab3q.html#q2a",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2a",
    "text": "Q2a\nSummarize the mean value of revenue for each group of search_stays_on for each date."
  },
  {
    "objectID": "DANL210_lab3q.html#q2b",
    "href": "DANL210_lab3q.html#q2b",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2b",
    "text": "Q2b\nCalculate the log difference between mean revenues in each group of search_stays_on. (This is the log of the average revenue in group of search_stays_on == 1 minus the log of the average revenue in group of search_stays_on == 0.)\n\nFor example, consider the following two observations:\n\n\n# date        the daily mean vale of `revenue`   search_stays_on\n# 1-Apr-12    93650.68                           0\n# 1-Apr-12    120277.57                          1\n\n\nThe log difference of daily mean revenues between the two group of search_stays_on for date 1-Apr-12 is log(120277.57) - log(93650.68)."
  },
  {
    "objectID": "DANL210_lab3q.html#load-dataframe-for-q2c-q2d-and-q2e",
    "href": "DANL210_lab3q.html#load-dataframe-for-q2c-q2d-and-q2e",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Load DataFrame for Q2c, Q2d, and Q2e",
    "text": "Load DataFrame for Q2c, Q2d, and Q2e\n\npaid_search = pd.read_csv('https://bcdanl.github.io/data/paid_search.csv')\n\n\nVariable description\n\nMay22_2012: 0 if date is before May 22, 2012; 1 otherwise.\nno_paid_search: same as search_stays_on.\nlog_revenue: natural log of eBay’s sales revenue for DM i before and after May 22, 2012"
  },
  {
    "objectID": "DANL210_lab3q.html#q2c",
    "href": "DANL210_lab3q.html#q2c",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2c",
    "text": "Q2c\nSort paid_search by DM and May22_2012 in ascending order."
  },
  {
    "objectID": "DANL210_lab3q.html#q2d",
    "href": "DANL210_lab3q.html#q2d",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2d",
    "text": "Q2d\nFor each DM, calculate the difference between log_revenue before and after May22_2012."
  },
  {
    "objectID": "DANL210_lab3q.html#q2e",
    "href": "DANL210_lab3q.html#q2e",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2e",
    "text": "Q2e\n\nConsider the DataFrame from Q2d.\nCalculate the mean value of the difference between log_revenue before and after May22_2012 for each group of no_paid_search.\nWhat is the difference in the mean values?"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html",
    "href": "DANL210_midterm-spring-2023-a.html",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#q1a",
    "href": "DANL210_midterm-spring-2023-a.html#q1a",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q1a",
    "text": "Q1a\nProvide both pandas/seaborn code and a simple comment to describe the trend of population for each borough.\n\nnyc_pop.columns\nnyc_pop_long = nyc_pop.melt(id_vars = 'Borough', var_name = 'Year', value_name = 'Pop')\n\n\n\n\n\n\n\n  \n    \n      \n      Borough\n      Year\n      Pop\n    \n  \n  \n    \n      0\n      Bronx\n      1950\n      1451277.0\n    \n    \n      1\n      Brooklyn\n      1950\n      2738175.0\n    \n    \n      2\n      Manhattan\n      1950\n      1960101.0\n    \n    \n      3\n      Queens\n      1950\n      1550849.0\n    \n    \n      4\n      Staten Island\n      1950\n      191555.0\n    \n    \n      5\n      Bronx\n      1960\n      1424815.0\n    \n    \n      6\n      Brooklyn\n      1960\n      2627319.0\n    \n    \n      7\n      Manhattan\n      1960\n      1698281.0\n    \n    \n      8\n      Queens\n      1960\n      1809578.0\n    \n    \n      9\n      Staten Island\n      1960\n      221991.0\n    \n    \n      10\n      Bronx\n      1970\n      1471701.0\n    \n    \n      11\n      Brooklyn\n      1970\n      2602012.0\n    \n    \n      12\n      Manhattan\n      1970\n      1539233.0\n    \n    \n      13\n      Queens\n      1970\n      1986473.0\n    \n    \n      14\n      Staten Island\n      1970\n      295443.0\n    \n    \n      15\n      Bronx\n      1980\n      1168972.0\n    \n    \n      16\n      Brooklyn\n      1980\n      2230936.0\n    \n    \n      17\n      Manhattan\n      1980\n      1428285.0\n    \n    \n      18\n      Queens\n      1980\n      1891325.0\n    \n    \n      19\n      Staten Island\n      1980\n      352121.0\n    \n    \n      20\n      Bronx\n      1990\n      1203789.0\n    \n    \n      21\n      Brooklyn\n      1990\n      2300664.0\n    \n    \n      22\n      Manhattan\n      1990\n      1487536.0\n    \n    \n      23\n      Queens\n      1990\n      1951598.0\n    \n    \n      24\n      Staten Island\n      1990\n      378977.0\n    \n    \n      25\n      Bronx\n      2000\n      1332650.0\n    \n    \n      26\n      Brooklyn\n      2000\n      2465326.0\n    \n    \n      27\n      Manhattan\n      2000\n      1537195.0\n    \n    \n      28\n      Queens\n      2000\n      2229379.0\n    \n    \n      29\n      Staten Island\n      2000\n      443728.0\n    \n    \n      30\n      Bronx\n      2010\n      1385108.0\n    \n    \n      31\n      Brooklyn\n      2010\n      2552911.0\n    \n    \n      32\n      Manhattan\n      2010\n      1585873.0\n    \n    \n      33\n      Queens\n      2010\n      2250002.0\n    \n    \n      34\n      Staten Island\n      2010\n      468730.0\n    \n    \n      35\n      Bronx\n      2020\n      1446788.0\n    \n    \n      36\n      Brooklyn\n      2020\n      2648452.0\n    \n    \n      37\n      Manhattan\n      2020\n      1638281.0\n    \n    \n      38\n      Queens\n      2020\n      2330295.0\n    \n    \n      39\n      Staten Island\n      2020\n      487155.0\n    \n    \n      40\n      Bronx\n      2030\n      1518998.0\n    \n    \n      41\n      Brooklyn\n      2030\n      2754009.0\n    \n    \n      42\n      Manhattan\n      2030\n      1676720.0\n    \n    \n      43\n      Queens\n      2030\n      2373551.0\n    \n    \n      44\n      Staten Island\n      2030\n      497749.0\n    \n    \n      45\n      Bronx\n      2040\n      1579245.0\n    \n    \n      46\n      Brooklyn\n      2040\n      2840525.0\n    \n    \n      47\n      Manhattan\n      2040\n      1691617.0\n    \n    \n      48\n      Queens\n      2040\n      2412649.0\n    \n    \n      49\n      Staten Island\n      2040\n      501109.0\n    \n  \n\n\n\n\n\nsns.lineplot(nyc_pop_long,\n             x = 'Year',\n             y = 'Pop',\n             hue = 'Borough')\n\n<AxesSubplot:xlabel='Year', ylabel='Pop'>"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#q1b",
    "href": "DANL210_midterm-spring-2023-a.html#q1b",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q1b",
    "text": "Q1b\nProvide both pandas/seaborn code and a simple comment to describe the trend of the proportion of population for each borough.\n\nNote: The proportion of population for each borough is each borough’s share of total population in NYC in each year.\n\n\nq1b = (\n       nyc_pop_long\n       .assign(tot = lambda x: x.groupby('Year')['Pop'].transform('sum'),\n               prop = lambda x: 100 * x['Pop'] / x['tot'],\n               chk = lambda x: x.groupby('Year')['prop'].transform('sum') )\n       )\n\n\n\n\n\n\n\n  \n    \n      \n      Borough\n      Year\n      Pop\n      tot\n      prop\n      chk\n    \n  \n  \n    \n      0\n      Bronx\n      1950\n      1451277.0\n      7891957.0\n      18.389317\n      100.0\n    \n    \n      1\n      Brooklyn\n      1950\n      2738175.0\n      7891957.0\n      34.695767\n      100.0\n    \n    \n      2\n      Manhattan\n      1950\n      1960101.0\n      7891957.0\n      24.836691\n      100.0\n    \n    \n      3\n      Queens\n      1950\n      1550849.0\n      7891957.0\n      19.651007\n      100.0\n    \n    \n      4\n      Staten Island\n      1950\n      191555.0\n      7891957.0\n      2.427218\n      100.0\n    \n    \n      5\n      Bronx\n      1960\n      1424815.0\n      7781984.0\n      18.309148\n      100.0\n    \n    \n      6\n      Brooklyn\n      1960\n      2627319.0\n      7781984.0\n      33.761557\n      100.0\n    \n    \n      7\n      Manhattan\n      1960\n      1698281.0\n      7781984.0\n      21.823239\n      100.0\n    \n    \n      8\n      Queens\n      1960\n      1809578.0\n      7781984.0\n      23.253427\n      100.0\n    \n    \n      9\n      Staten Island\n      1960\n      221991.0\n      7781984.0\n      2.852627\n      100.0\n    \n    \n      10\n      Bronx\n      1970\n      1471701.0\n      7894862.0\n      18.641250\n      100.0\n    \n    \n      11\n      Brooklyn\n      1970\n      2602012.0\n      7894862.0\n      32.958296\n      100.0\n    \n    \n      12\n      Manhattan\n      1970\n      1539233.0\n      7894862.0\n      19.496642\n      100.0\n    \n    \n      13\n      Queens\n      1970\n      1986473.0\n      7894862.0\n      25.161592\n      100.0\n    \n    \n      14\n      Staten Island\n      1970\n      295443.0\n      7894862.0\n      3.742219\n      100.0\n    \n    \n      15\n      Bronx\n      1980\n      1168972.0\n      7071639.0\n      16.530425\n      100.0\n    \n    \n      16\n      Brooklyn\n      1980\n      2230936.0\n      7071639.0\n      31.547651\n      100.0\n    \n    \n      17\n      Manhattan\n      1980\n      1428285.0\n      7071639.0\n      20.197369\n      100.0\n    \n    \n      18\n      Queens\n      1980\n      1891325.0\n      7071639.0\n      26.745214\n      100.0\n    \n    \n      19\n      Staten Island\n      1980\n      352121.0\n      7071639.0\n      4.979341\n      100.0\n    \n    \n      20\n      Bronx\n      1990\n      1203789.0\n      7322564.0\n      16.439447\n      100.0\n    \n    \n      21\n      Brooklyn\n      1990\n      2300664.0\n      7322564.0\n      31.418831\n      100.0\n    \n    \n      22\n      Manhattan\n      1990\n      1487536.0\n      7322564.0\n      20.314414\n      100.0\n    \n    \n      23\n      Queens\n      1990\n      1951598.0\n      7322564.0\n      26.651839\n      100.0\n    \n    \n      24\n      Staten Island\n      1990\n      378977.0\n      7322564.0\n      5.175469\n      100.0\n    \n    \n      25\n      Bronx\n      2000\n      1332650.0\n      8008278.0\n      16.640906\n      100.0\n    \n    \n      26\n      Brooklyn\n      2000\n      2465326.0\n      8008278.0\n      30.784721\n      100.0\n    \n    \n      27\n      Manhattan\n      2000\n      1537195.0\n      8008278.0\n      19.195075\n      100.0\n    \n    \n      28\n      Queens\n      2000\n      2229379.0\n      8008278.0\n      27.838432\n      100.0\n    \n    \n      29\n      Staten Island\n      2000\n      443728.0\n      8008278.0\n      5.540867\n      100.0\n    \n    \n      30\n      Bronx\n      2010\n      1385108.0\n      8242624.0\n      16.804212\n      100.0\n    \n    \n      31\n      Brooklyn\n      2010\n      2552911.0\n      8242624.0\n      30.972067\n      100.0\n    \n    \n      32\n      Manhattan\n      2010\n      1585873.0\n      8242624.0\n      19.239905\n      100.0\n    \n    \n      33\n      Queens\n      2010\n      2250002.0\n      8242624.0\n      27.297157\n      100.0\n    \n    \n      34\n      Staten Island\n      2010\n      468730.0\n      8242624.0\n      5.686660\n      100.0\n    \n    \n      35\n      Bronx\n      2020\n      1446788.0\n      8550971.0\n      16.919576\n      100.0\n    \n    \n      36\n      Brooklyn\n      2020\n      2648452.0\n      8550971.0\n      30.972529\n      100.0\n    \n    \n      37\n      Manhattan\n      2020\n      1638281.0\n      8550971.0\n      19.159005\n      100.0\n    \n    \n      38\n      Queens\n      2020\n      2330295.0\n      8550971.0\n      27.251817\n      100.0\n    \n    \n      39\n      Staten Island\n      2020\n      487155.0\n      8550971.0\n      5.697072\n      100.0\n    \n    \n      40\n      Bronx\n      2030\n      1518998.0\n      8821027.0\n      17.220194\n      100.0\n    \n    \n      41\n      Brooklyn\n      2030\n      2754009.0\n      8821027.0\n      31.220956\n      100.0\n    \n    \n      42\n      Manhattan\n      2030\n      1676720.0\n      8821027.0\n      19.008218\n      100.0\n    \n    \n      43\n      Queens\n      2030\n      2373551.0\n      8821027.0\n      26.907876\n      100.0\n    \n    \n      44\n      Staten Island\n      2030\n      497749.0\n      8821027.0\n      5.642756\n      100.0\n    \n    \n      45\n      Bronx\n      2040\n      1579245.0\n      9025145.0\n      17.498278\n      100.0\n    \n    \n      46\n      Brooklyn\n      2040\n      2840525.0\n      9025145.0\n      31.473456\n      100.0\n    \n    \n      47\n      Manhattan\n      2040\n      1691617.0\n      9025145.0\n      18.743378\n      100.0\n    \n    \n      48\n      Queens\n      2040\n      2412649.0\n      9025145.0\n      26.732523\n      100.0\n    \n    \n      49\n      Staten Island\n      2040\n      501109.0\n      9025145.0\n      5.552365\n      100.0\n    \n  \n\n\n\n\n\nsns.lineplot(q1b,\n             x = 'Year',\n             y = 'prop',\n             hue = 'Borough')\n\n<AxesSubplot:xlabel='Year', ylabel='prop'>"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#load-dataframe-for-q2a-and-q2b",
    "href": "DANL210_midterm-spring-2023-a.html#load-dataframe-for-q2a-and-q2b",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Load DataFrame for Q2a and Q2b",
    "text": "Load DataFrame for Q2a and Q2b\n\nrestaurant = pd.read_csv('https://bcdanl.github.io/data/DOHMH_NYC_Restaurant_Inspection.csv')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCAMIS\n\n\n\nDBA\n\n\n\nBORO\n\n\n\nSTREET\n\n\n\nCUISINE DESCRIPTION\n\n\n\nINSPECTION DATE\n\n\n\nACTION\n\n\n\nVIOLATION CODE\n\n\n\nVIOLATION DESCRIPTION\n\n\n\nCRITICAL FLAG\n\n\n\nSCORE\n\n\n\nGRADE\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n30191841\n\n\n\ndj reynolds pub and restaurant\n\n\n\nManhattan\n\n\n\nWEST 57 STREET\n\n\n\nIrish\n\n\n\n01/04/2022\n\n\n\nViolations were cited in the following area(s).\n\n\n\n10F\n\n\n\nNon-food contact surface improperly constructed. Unacceptable material used. Non-food contact surface or equipment improperly maintained and/or not properly sealed, raised, spaced or movable to allow accessibility for cleaning on all sides, above and underneath the unit.\n\n\n\nNot Critical\n\n\n\n12\n\n\n\nA\n\n\n\n\n\n\n\n1\n\n\n\n40356018\n\n\n\nriviera caterers\n\n\n\nBrooklyn\n\n\n\nSTILLWELL AVENUE\n\n\n\nAmerican\n\n\n\n02/01/2022\n\n\n\nViolations were cited in the following area(s).\n\n\n\n02G\n\n\n\nCold food item held above 41º F (smoked fish and reduced oxygen packaged foods above 38 ºF) except during necessary preparation.\n\n\n\nCritical\n\n\n\n7\n\n\n\nA\n\n\n\n\n\n\n\n2\n\n\n\n40356483\n\n\n\nwilken's fine food\n\n\n\nBrooklyn\n\n\n\nAVENUE U\n\n\n\nSandwiches\n\n\n\n08/19/2022\n\n\n\nViolations were cited in the following area(s).\n\n\n\n10F\n\n\n\nNon-food contact surface or equipment made of unacceptable material, not kept clean, or not properly sealed, raised, spaced or movable to allow accessibility for cleaning on all sides, above and underneath the unit.\n\n\n\nNot Critical\n\n\n\n2\n\n\n\nA\n\n\n\n\n\n\n\n3\n\n\n\n40356731\n\n\n\ntaste the tropics ice cream\n\n\n\nBrooklyn\n\n\n\nNOSTRAND AVENUE\n\n\n\nFrozen Desserts\n\n\n\n01/17/2023\n\n\n\nViolations were cited in the following area(s).\n\n\n\n08A\n\n\n\nEstablishment is not free of harborage or conditions conducive to rodents, insects or other pests.\n\n\n\nNot Critical\n\n\n\n9\n\n\n\nA\n\n\n\n\n\n\n\n4\n\n\n\n40357217\n\n\n\nwild asia\n\n\n\nBronx\n\n\n\nSOUTHERN BOULEVARD\n\n\n\nAmerican\n\n\n\n07/28/2021\n\n\n\nViolations were cited in the following area(s).\n\n\n\n02G\n\n\n\nCold food item held above 41º F (smoked fish and reduced oxygen packaged foods above 38 ºF) except during necessary preparation.\n\n\n\nCritical\n\n\n\n10\n\n\n\nA\n\n\n\n\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n…\n\n\n\n\n\n\n\n17628\n\n\n\n50133218\n\n\n\nsades southern cafe llc\n\n\n\nBronx\n\n\n\nEAST 233 STREET\n\n\n\nCaribbean\n\n\n\n03/06/2023\n\n\n\nViolations were cited in the following area(s).\n\n\n\n06A\n\n\n\nPersonal cleanliness is inadequate. Outer garment soiled with possible contaminant. Effective hair restraint not worn where required. Jewelry worn on hands or arms. Fingernail polish worn or fingernails not kept clean and trimmed.\n\n\n\nCritical\n\n\n\n12\n\n\n\nA\n\n\n\n\n\n\n\n17629\n\n\n\n50133250\n\n\n\nred brick\n\n\n\nBrooklyn\n\n\n\nNOSTRAND AVENUE\n\n\n\nCaribbean\n\n\n\n03/22/2023\n\n\n\nViolations were cited in the following area(s).\n\n\n\n08A\n\n\n\nEstablishment is not free of harborage or conditions conducive to rodents, insects or other pests.\n\n\n\nNot Critical\n\n\n\n4\n\n\n\nA\n\n\n\n\n\n\n\n17630\n\n\n\n50133302\n\n\n\npotbelly\n\n\n\nManhattan\n\n\n\nEAST 17 STREET\n\n\n\nSandwiches\n\n\n\n03/20/2023\n\n\n\nViolations were cited in the following area(s).\n\n\n\n06D\n\n\n\nFood contact surface not properly washed, rinsed and sanitized after each use and following any activity when contamination may have occurred.\n\n\n\nCritical\n\n\n\n7\n\n\n\nA\n\n\n\n\n\n\n\n17631\n\n\n\n50133309\n\n\n\npotbelly\n\n\n\nManhattan\n\n\n\nBROADWAY\n\n\n\nSoups/Salads/Sandwiches\n\n\n\n03/27/2023\n\n\n\nViolations were cited in the following area(s).\n\n\n\n10F\n\n\n\nNon-food contact surface or equipment made of unacceptable material, not kept clean, or not properly sealed, raised, spaced or movable to allow accessibility for cleaning on all sides, above and underneath the unit.\n\n\n\nNot Critical\n\n\n\n4\n\n\n\nA\n\n\n\n\n\n\n\n17632\n\n\n\n50133690\n\n\n\nisland shack\n\n\n\nBrooklyn\n\n\n\nFRANKLIN AVENUE\n\n\n\nCaribbean\n\n\n\n03/23/2023\n\n\n\nViolations were cited in the following area(s).\n\n\n\n10F\n\n\n\nNon-food contact surface or equipment made of unacceptable material, not kept clean, or not properly sealed, raised, spaced or movable to allow accessibility for cleaning on all sides, above and underneath the unit.\n\n\n\nNot Critical\n\n\n\n12\n\n\n\nA"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#variable-description",
    "href": "DANL210_midterm-spring-2023-a.html#variable-description",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Variable Description",
    "text": "Variable Description\n\nCAMIS:\n\nThis is an unique identifier for the entity (restaurant);\n10-digit integer\n\nDBA:\n\nThis field represents the name (doing business as) of the entity (restaurant);\nPublic business name, may change at discretion of restaurant owner\n\nBORO:\n\nBorough in which the entity (restaurant) is located.;\n• 1 = MANHATTAN\n• 2 = BRONX\n• 3 = BROOKLYN\n• 4 = QUEENS\n• 5 = STATEN ISLAND\n• 0 = Missing;\n\nCUISINE DESCRIPTION:\n\nThis field describes the entity (restaurant) cuisine.\n\nACTION:\n\nThis field represents the actions that is associated with each restaurant inspection. ;\n• Violations were cited in the following area(s).\n• No violations were recorded at the time of this inspection.\n• Establishment re-opened by DOHMH\n• Establishment re-closed by DOHMH\n• Establishment Closed by DOHMH.\n• Violations were cited in the following area(s) and those requiring immediate action were addressed.\n\nVIOLATION CODE:\n\nViolation code associated with an establishment (restaurant) inspection\n\nVIOLATION DESCRIPTION:\n\nViolation description associated with an establishment (restaurant) inspection\n\nCRITICAL FLAG:\n\nIndicator of critical violation;\n• Critical\n• Not Critical\n• Not Applicable;\nCritical violations are those most likely to contribute to food-borne illness\n\nSCORE:\n\nTotal score for a particular inspection;\n\nGRADE:\n\nGrade associated with the inspection;\n• N = Not Yet Graded\n• A = Grade A\n• B = Grade B\n• C = Grade C\n• Z = Grade Pending\n• P = Grade Pending issued on re-opening following an initial inspection that resulted in a closure"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#q2a.",
    "href": "DANL210_midterm-spring-2023-a.html#q2a.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2a.",
    "text": "Q2a.\nWhat are the mean, standard deviation, first quartile, median, third quartile, and maximum of SCORE for each GRADE of restaurants?\n\n\nq2a = restaurant.groupby('GRADE')['SCORE'].describe()\n\n\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      GRADE\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      A\n      16305.0\n      9.261515\n      3.415952\n      0.0\n      7.0\n      10.0\n      12.0\n      13.0\n    \n    \n      B\n      1034.0\n      21.027079\n      4.156573\n      0.0\n      18.0\n      21.0\n      24.0\n      36.0\n    \n    \n      C\n      294.0\n      38.561224\n      10.834844\n      0.0\n      31.0\n      36.0\n      44.0\n      86.0"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#q2b.",
    "href": "DANL210_midterm-spring-2023-a.html#q2b.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2b.",
    "text": "Q2b.\nProvide both (1) pandas/seaborn code and (2) a simple comment to describe how the distribution of SCORE varies by CRITICAL FLAG when GRADE is A.\n\nsns.displot(data = restaurant.query('GRADE == \"A\"'), x='SCORE', kind='kde', row='CRITICAL FLAG')\n\n<seaborn.axisgrid.FacetGrid at 0x7fe355f64190>\n\n\n\n\n\n\nsns.displot(data = restaurant.query('GRADE == \"A\"'), x='SCORE', row='CRITICAL FLAG')\n\n<seaborn.axisgrid.FacetGrid at 0x7fe35500d220>\n\n\n\n\n\n\nFor Not Critical type, the two SCORE values around 1 and 12 are most common, while 12 is the single most common SCORE value for Critical type."
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#q2c.",
    "href": "DANL210_midterm-spring-2023-a.html#q2c.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2c.",
    "text": "Q2c.\n\nFor each pair of BORO and GRADE, calculate\n\n\nthe proportion of Critical violation and\n\n\nthe proportion of Not Critical violation.\n\n\nMake a simple comment on how the proportions vary by BORO and GRADE.\n\n\n\n.assign() with .transform()\n\n\nq2c_ = (\n        restaurant\n        .groupby(['BORO', 'GRADE', 'CRITICAL FLAG'])\n        .size()\n        .reset_index(name='n')\n        .assign(tot = lambda x: x.groupby(['BORO', 'GRADE'])['n'].transform('sum'),\n                prop = lambda x: 100 * x['n'] / x['tot'])\n)\n\n\n.apply() with .assign()\n\n\nq2c = (\n       restaurant\n       .groupby(['BORO', 'GRADE', 'CRITICAL FLAG'])\n       .size()\n       .reset_index(name='n')\n       .groupby(['BORO', 'GRADE'])\n       .apply(lambda x: x.assign( prop = 100 * x['n'] / x['n'].sum() ) )\n       )\n\n\nvalue_counts(normalize = True) can be useful:\n\n\nq2c__ = (\n  restaurant\n  .groupby(['BORO', 'GRADE'])['CRITICAL FLAG']\n  .value_counts(normalize = True)\n)\n\n\n\n\n\n\n\n  \n    \n      \n      BORO\n      GRADE\n      CRITICAL FLAG\n      n\n      prop\n    \n  \n  \n    \n      0\n      Bronx\n      A\n      Critical\n      637\n      48.404255\n    \n    \n      1\n      Bronx\n      A\n      Not Critical\n      679\n      51.595745\n    \n    \n      2\n      Bronx\n      B\n      Critical\n      73\n      66.972477\n    \n    \n      3\n      Bronx\n      B\n      Not Critical\n      36\n      33.027523\n    \n    \n      4\n      Bronx\n      C\n      Critical\n      22\n      88.000000\n    \n    \n      5\n      Bronx\n      C\n      Not Critical\n      3\n      12.000000\n    \n    \n      6\n      Brooklyn\n      A\n      Critical\n      2082\n      49.313122\n    \n    \n      7\n      Brooklyn\n      A\n      Not Critical\n      2140\n      50.686878\n    \n    \n      8\n      Brooklyn\n      B\n      Critical\n      197\n      67.006803\n    \n    \n      9\n      Brooklyn\n      B\n      Not Critical\n      97\n      32.993197\n    \n    \n      10\n      Brooklyn\n      C\n      Critical\n      54\n      64.285714\n    \n    \n      11\n      Brooklyn\n      C\n      Not Critical\n      30\n      35.714286\n    \n    \n      12\n      Manhattan\n      A\n      Critical\n      3174\n      49.733626\n    \n    \n      13\n      Manhattan\n      A\n      Not Critical\n      3208\n      50.266374\n    \n    \n      14\n      Manhattan\n      B\n      Critical\n      226\n      64.942529\n    \n    \n      15\n      Manhattan\n      B\n      Not Critical\n      122\n      35.057471\n    \n    \n      16\n      Manhattan\n      C\n      Critical\n      64\n      65.979381\n    \n    \n      17\n      Manhattan\n      C\n      Not Critical\n      33\n      34.020619\n    \n    \n      18\n      Queens\n      A\n      Critical\n      1800\n      48.128342\n    \n    \n      19\n      Queens\n      A\n      Not Critical\n      1940\n      51.871658\n    \n    \n      20\n      Queens\n      B\n      Critical\n      147\n      60.743802\n    \n    \n      21\n      Queens\n      B\n      Not Critical\n      95\n      39.256198\n    \n    \n      22\n      Queens\n      C\n      Critical\n      54\n      65.060241\n    \n    \n      23\n      Queens\n      C\n      Not Critical\n      29\n      34.939759\n    \n    \n      24\n      Staten Island\n      A\n      Critical\n      299\n      46.356589\n    \n    \n      25\n      Staten Island\n      A\n      Not Critical\n      346\n      53.643411\n    \n    \n      26\n      Staten Island\n      B\n      Critical\n      31\n      75.609756\n    \n    \n      27\n      Staten Island\n      B\n      Not Critical\n      10\n      24.390244\n    \n    \n      28\n      Staten Island\n      C\n      Critical\n      3\n      60.000000\n    \n    \n      29\n      Staten Island\n      C\n      Not Critical\n      2\n      40.000000\n    \n  \n\n\n\n\n\nVisualization helps us see variations in data.\n\n\n(\n sns.FacetGrid(\n       data = q2c,\n       col='BORO',\n       row = 'GRADE')\n .map(sns.scatterplot, 'CRITICAL FLAG', 'prop' )\n )\n\n<seaborn.axisgrid.FacetGrid at 0x7fe3574ba910>\n\n\n\n\n\n\nsns.barplot would be better.\n\n\n(\n sns.FacetGrid(\n       data = q2c,\n       col='BORO',\n       row = 'GRADE')\n .map(sns.barplot, 'CRITICAL FLAG', 'prop' )\n )\n\n/Users/byeong-hakchoe/opt/anaconda3/lib/python3.9/site-packages/seaborn/axisgrid.py:712: UserWarning:\n\nUsing the barplot function without specifying `order` is likely to produce an incorrect plot.\n\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fe356b02ee0>\n\n\n\n\n\n\nFor GRADE A, the probability distribution of CRITICAL FLAG are similar across BOROs.\nFor GRADE B, the restaurants in Staten Island are more likely to be Critical than in other BOROs.\nFor GRADE C, the restaurants in Bronx are more likely to be Critical than in other BOROs."
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#q2d.",
    "href": "DANL210_midterm-spring-2023-a.html#q2d.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2d.",
    "text": "Q2d.\nFor the 10 most common CUISINE DESCRIPTION values, find the CUISINE DESCRIPTION value that has the highest proportion of GRADE A.\n\n\nq2d = (\n       restaurant\n       .groupby('CUISINE DESCRIPTION')\n       .apply(lambda x: x.assign( n = x.shape[0] ) )\n       .reset_index(drop = True)\n       .query('n.rank(method=\"dense\", ascending=False) <= 10')\n       .groupby(['CUISINE DESCRIPTION', 'GRADE'])\n       .size()\n       .reset_index(name='n')\n       .groupby('CUISINE DESCRIPTION')\n       .apply(lambda x: x.assign( prop_A = x['n'] / x['n'].sum() ) )\n       .query('GRADE == \"A\"')\n       .sort_values(by = 'prop_A', ascending = False)\n       .reset_index(drop = True)\n       )\n\n\n\n\n\n\n\n  \n    \n      \n      CUISINE DESCRIPTION\n      GRADE\n      n\n      prop_A\n    \n  \n  \n    \n      0\n      Donuts\n      A\n      499\n      0.988119\n    \n    \n      1\n      Coffee/Tea\n      A\n      1356\n      0.958982\n    \n    \n      2\n      American\n      A\n      3505\n      0.952964\n    \n    \n      3\n      Italian\n      A\n      653\n      0.947750\n    \n    \n      4\n      Pizza\n      A\n      961\n      0.915238\n    \n    \n      5\n      Bakery Products/Desserts\n      A\n      560\n      0.910569\n    \n    \n      6\n      Japanese\n      A\n      529\n      0.905822\n    \n    \n      7\n      Mexican\n      A\n      515\n      0.884880\n    \n    \n      8\n      Chinese\n      A\n      1205\n      0.878280\n    \n    \n      9\n      Latin American\n      A\n      464\n      0.852941"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#q2e.",
    "href": "DANL210_midterm-spring-2023-a.html#q2e.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2e.",
    "text": "Q2e.\n\nFind the 3 most common names of restaurants (DBA) in each BORO.\n\nIf the third most common DBA values are multiple, please include all the DBA values.\n\nOverall, which DBA value is most common in NYC?\n\n\nq2e = (\n       restaurant[['DBA', 'BORO']]\n       .groupby(['BORO', 'DBA'])\n       .size()\n       .reset_index(name='n')\n       )\n\nq2e['ranking'] = q2e.groupby(['BORO'])['n'].rank(method='dense', ascending=False)\n\nq2e = (\n       q2e\n       .query('ranking <= 3')\n       .sort_values(by=['BORO', 'ranking'])\n       .reset_index(drop = True)\n       .drop_duplicates()\n       )\n\nq2e_ = (\n       restaurant[['DBA']]\n       .groupby(['DBA'])\n       .value_counts()\n       .sort_values(ascending = False)\n)\n\n\n\n\n\n\n\n  \n    \n      \n      BORO\n      DBA\n      n\n      ranking\n    \n  \n  \n    \n      0\n      Bronx\n      dunkin\n      54\n      1.0\n    \n    \n      1\n      Bronx\n      mcdonald's\n      33\n      2.0\n    \n    \n      2\n      Bronx\n      kennedy fried chicken\n      32\n      3.0\n    \n    \n      3\n      Brooklyn\n      dunkin\n      93\n      1.0\n    \n    \n      4\n      Brooklyn\n      mcdonald's\n      46\n      2.0\n    \n    \n      5\n      Brooklyn\n      starbucks\n      43\n      3.0\n    \n    \n      6\n      Manhattan\n      starbucks\n      178\n      1.0\n    \n    \n      7\n      Manhattan\n      dunkin\n      108\n      2.0\n    \n    \n      8\n      Manhattan\n      chipotle mexican grill\n      52\n      3.0\n    \n    \n      9\n      Manhattan\n      subway\n      52\n      3.0\n    \n    \n      10\n      Queens\n      dunkin\n      113\n      1.0\n    \n    \n      11\n      Queens\n      subway\n      50\n      2.0\n    \n    \n      12\n      Queens\n      dunkin', baskin robbins\n      47\n      3.0\n    \n    \n      13\n      Staten Island\n      dunkin\n      34\n      1.0\n    \n    \n      14\n      Staten Island\n      subway\n      13\n      2.0\n    \n    \n      15\n      Staten Island\n      starbucks\n      9\n      3.0\n    \n  \n\n\n\n\nDBA\ndunkin                     402\nstarbucks                  286\nmcdonald's                 170\nsubway                     166\ndunkin', baskin robbins    107\n                          ... \nganda sushi                  1\nganni's pizzeria             1\ngantry bar & kitchen         1\ngaoming bakery               1\nzz's pizza & grill           1\nLength: 13914, dtype: int64\n\n\n\nNote that chipotle mexican grill and subway are both the third most popular franchise/chain in Manhattan.\nOverall, dunkin is the most popular franchise/chain in NYC."
  },
  {
    "objectID": "DANL210_midterm-spring-2023-a.html#q2f.",
    "href": "DANL210_midterm-spring-2023-a.html#q2f.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2f.",
    "text": "Q2f.\nFor all the DBA values that appear in the result of Q2f, find the DBA value that is most likely to commit critical violation.\n\nq2f_ = (\n        restaurant\n        .loc[(restaurant['DBA'].isin(q2e['DBA'])) & (restaurant['CRITICAL FLAG'] == 'Critical'), ['DBA']]\n        .groupby('DBA')\n        .size()\n        .reset_index(name='n_crit') \n        )\n\nq2f__ = (\n        restaurant\n        .loc[(restaurant['DBA'].isin(q2e['DBA'])) & (restaurant['CRITICAL FLAG'] != 'Critical'), ['DBA']]\n        .groupby('DBA')\n        .size()\n        .reset_index(name='n_not_crit') \n        )\n\n\nq2f__['n_crit'] = q2f_['n_crit']\n\nq2f__['prop'] = q2f__['n_crit'] / (q2f__['n_crit'] + q2f__['n_not_crit'])\n\nq2f__ = q2f__.sort_values('prop', ascending=False)\n\n\n.shift(1) can be useful:\n\n\nq2f = (\n       restaurant[restaurant['DBA'].isin(q2e['DBA'])]\n           .groupby(['DBA', 'CRITICAL FLAG'])\n           .size()\n           .reset_index(name='n')\n           .groupby('DBA')\n           .apply(lambda x: x.assign(lag_n=x['n'].shift(1),\n                                      tot=x['n'].sum(),\n                                      prop_crit=x['n'].shift(1)/x['n'].sum()))\n           .reset_index()\n           .loc[:, ['DBA', 'prop_crit']]\n           .sort_values(by='prop_crit', ascending=False)\n           .dropna(subset=['prop_crit'])\n       )\n\n\n\n\n\n\n\n  \n    \n      \n      DBA\n      prop_crit\n    \n  \n  \n    \n      13\n      subway\n      0.475904\n    \n    \n      3\n      dunkin\n      0.415423\n    \n    \n      5\n      dunkin', baskin robbins\n      0.411215\n    \n    \n      7\n      kennedy fried chicken\n      0.403846\n    \n    \n      1\n      chipotle mexican grill\n      0.384615\n    \n    \n      9\n      mcdonald's\n      0.370588\n    \n    \n      11\n      starbucks\n      0.325175\n    \n  \n\n\n\n\n\nAmong popular franchises/chains, subway is most likely to commit Critical violation in NYC."
  },
  {
    "objectID": "DANL210_hw1a.html",
    "href": "DANL210_hw1a.html",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files."
  },
  {
    "objectID": "DANL210_hw1a.html#q1a",
    "href": "DANL210_hw1a.html#q1a",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q1a",
    "text": "Q1a\n\nCalculate the simple difference between the probability of survival when passengers are first-class and the probability of survival when they are not.\n\n\n# Count the number of passengers in each class and return the count in descending order\ntitanic_1[['pclass']].value_counts()\n\n# Count the number of passengers who survived and who didn't and return the count in descending order\ntitanic_1[['survived']].value_counts()\n\n# Add a new column 'd' to the titanic_1 dataframe and set all values to 0\ntitanic_1['d'] = 0\n\n# For rows where the 'pclass' column is '1st class', set the value of the 'd' column to 1\ntitanic_1.loc[titanic_1['pclass']=='1st class', 'd'] = 1\n\n# Add a new column 'survived_d' to the titanic_1 DataFrame and set all values to 0\ntitanic_1['survived_d'] = 0\n\n# For rows where the 'survived' column is 'yes', set the value of the 'survived_d' column to 1\ntitanic_1.loc[titanic_1['survived']=='yes', 'survived_d'] = 1\n\n# Compute the mean of 'survived_d' for rows where 'd' is 0\ne_y0 = titanic_1.loc[titanic_1['d']==0, 'survived_d'].mean()\n\n# Compute the mean of 'survived_d' for rows where 'd' is 1\ne_y1 = titanic_1.loc[titanic_1['d']==1, 'survived_d'].mean()\n\n# Compute the SDP (so called treatment effect) by subtracting the mean of 'survived_d' where 'd' is 0 from the mean where 'd' is 1\nSDP = e_y1 - e_y0\n\n# Return the value of SDP\nSDP\n\n0.3152436786584735"
  },
  {
    "objectID": "DANL210_hw1a.html#q1b",
    "href": "DANL210_hw1a.html#q1b",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q1b",
    "text": "Q1b\n\nHow much does the probability of survival increase for first-class passengers relative to those who are not first-class passengers?\nSDP tells us what would happen to the probability of survival if non-first-class passengers were first-class.\n\nIn other words, SDP means the effect of being the first-class on the probability of survival from the Titanic Disaster."
  },
  {
    "objectID": "DANL210_hw1a.html#q1c",
    "href": "DANL210_hw1a.html#q1c",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q1c",
    "text": "Q1c\n\nConsider the probability of survival in titanic_2.csv.\n\n\ntitanic_2 = pd.read_csv('https://bcdanl.github.io/data/titanic_2.csv')\n\n\ntitanic_2.head()\n\n\n\n\n\n  \n    \n      \n      pclass\n      survived\n      sex\n      age\n    \n  \n  \n    \n      0\n      1st class\n      yes\n      female\n      29.0000\n    \n    \n      1\n      1st class\n      yes\n      male\n      0.9167\n    \n    \n      2\n      1st class\n      no\n      female\n      2.0000\n    \n    \n      3\n      1st class\n      no\n      male\n      30.0000\n    \n    \n      4\n      1st class\n      no\n      female\n      25.0000\n    \n  \n\n\n\n\n\ntitanic_2.describe()\n\n\n\n\n\n  \n    \n      \n      age\n    \n  \n  \n    \n      count\n      1046.000000\n    \n    \n      mean\n      29.881135\n    \n    \n      std\n      14.413500\n    \n    \n      min\n      0.166700\n    \n    \n      25%\n      21.000000\n    \n    \n      50%\n      28.000000\n    \n    \n      75%\n      39.000000\n    \n    \n      max\n      80.000000\n    \n  \n\n\n\n\n\nAfter stratifying on gender and age, what happens to the difference in the probabilities of survival between first-class passengers and non-first-class passengers.\nExplain in your own words what stratifying on gender and age did for this difference in probabilities of survival between first-class passengers and non-first-class passengers.\n\n\n# Get count of passengers by pclass\ntitanic_2[['pclass']].value_counts()\n\n# Get count of passengers who survived or not\ntitanic_2[['survived']].value_counts()\n\n# Get count of passengers by gender\ntitanic_2[['sex']].value_counts()\n\n# Get count of passengers by age\ntitanic_2[['age']].value_counts()\n\nage    \n24.0000    47\n22.0000    43\n21.0000    41\n30.0000    40\n18.0000    39\n           ..\n20.5000     1\n11.5000     1\n0.6667      1\n0.4167      1\n80.0000     1\nLength: 98, dtype: int64\n\n\n\ntitanic_2.groupby('sex').describe()\n\n\n\n\n\n  \n    \n      \n      age\n    \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      sex\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      female\n      388.0\n      28.687071\n      14.576995\n      0.1667\n      19.0\n      27.0\n      38.0\n      76.0\n    \n    \n      male\n      658.0\n      30.585233\n      14.280571\n      0.3333\n      21.0\n      28.0\n      39.0\n      80.0\n    \n  \n\n\n\n\n\n# Create a new column 'd' and set its value to 0 for all rows\ntitanic_2['d'] = 0\n\n# Set the value of column 'd' to 1 for rows where pclass is '1st class'\ntitanic_2.loc[titanic_2['pclass']=='1st class', 'd'] = 1\n\n# Create a new column 'survived_d' and set its value to 0 for all rows\ntitanic_2['survived_d'] = 0\n\n# Set the value of column 'survived_d' to 1 for rows where survived is 'yes'\ntitanic_2.loc[titanic_2['survived']=='yes', 'survived_d'] = 1\n\n# Create a new column 'sex_d' and set its value to 0 for all rows\ntitanic_2['sex_d'] = 0\n\n# Set the value of column 'sex_d' to 1 for rows where sex is 'male'\ntitanic_2.loc[titanic_2['sex']=='male', 'sex_d'] = 1\n\n# Create a new column 'AgeGroup' and set its value to 0 for all rows\ntitanic_2['AgeGroup'] = 0\n\n# Set the value of column 'AgeGroup' to 1 for rows where age is greater than or equal to 18\ntitanic_2.loc[titanic_2['age'] >= 18, 'AgeGroup'] = 1\n\n\ntitanic_2.groupby('AgeGroup').describe()\n\n\n\n\n\n  \n    \n      \n      age\n      d\n      ...\n      survived_d\n      sex_d\n    \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n      count\n      mean\n      ...\n      75%\n      max\n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      AgeGroup\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      154.0\n      9.101732\n      6.015429\n      0.1667\n      3.0\n      9.0\n      15.75\n      17.0\n      418.0\n      0.129187\n      ...\n      1.0\n      1.0\n      418.0\n      0.638756\n      0.480937\n      0.0\n      0.0\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      892.0\n      33.468610\n      12.244544\n      18.0000\n      24.0\n      30.0\n      41.00\n      80.0\n      892.0\n      0.301570\n      ...\n      1.0\n      1.0\n      892.0\n      0.645740\n      0.478557\n      0.0\n      0.0\n      1.0\n      1.0\n      1.0\n    \n  \n\n2 rows × 32 columns\n\n\n\n\ntitanic_2.groupby(['sex', 'AgeGroup']).describe()\n\n\n\n\n\n  \n    \n      \n      \n      age\n      d\n      ...\n      survived_d\n      sex_d\n    \n    \n      \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n      count\n      mean\n      ...\n      75%\n      max\n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      sex\n      AgeGroup\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      female\n      0\n      72.0\n      9.015047\n      5.974978\n      0.1667\n      3.00\n      9.0\n      15.0\n      17.0\n      150.0\n      0.126667\n      ...\n      1.0\n      1.0\n      150.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      316.0\n      33.169304\n      12.016747\n      18.0000\n      23.75\n      30.0\n      40.0\n      76.0\n      316.0\n      0.395570\n      ...\n      1.0\n      1.0\n      316.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      male\n      0\n      82.0\n      9.177845\n      6.086437\n      0.3333\n      3.00\n      9.0\n      16.0\n      17.0\n      267.0\n      0.131086\n      ...\n      0.0\n      1.0\n      267.0\n      1.0\n      0.0\n      1.0\n      1.0\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      576.0\n      33.632812\n      12.375016\n      18.0000\n      24.00\n      30.0\n      41.0\n      80.0\n      576.0\n      0.250000\n      ...\n      0.0\n      1.0\n      576.0\n      1.0\n      0.0\n      1.0\n      1.0\n      1.0\n      1.0\n      1.0\n    \n  \n\n4 rows × 32 columns\n\n\n\n\n# Create a new column 's' and set its value to 0 for all rows\ntitanic_2['s'] = 0 \n\n# Set the value of column 's' based on gender and age group\ntitanic_2.loc[(titanic_2.sex_d == 0) & (titanic_2.AgeGroup == 1), 's'] = 1\ntitanic_2.loc[(titanic_2.sex_d == 0) & (titanic_2.AgeGroup == 0), 's'] = 2\ntitanic_2.loc[(titanic_2.sex_d == 1) & (titanic_2.AgeGroup == 1), 's'] = 3\ntitanic_2.loc[(titanic_2.sex_d == 1) & (titanic_2.AgeGroup == 0), 's'] = 4\n\n# Get the number of observations where d is 0\nobs = titanic_2.loc[titanic_2.d == 0].shape[0]\n\n# Define a function to calculate weighted average effect\ndef weighted_avg_effect(df):\n  \n  # Calculate the difference in survival rates between the treatment and control groups\n    diff = ( df[ df.d == 1 ].survived_d.mean() - \n             df[ df.d == 0 ].survived_d.mean() )\n  \n  # Calculate the weight assigned to the treatment group\n    weight = df[ df.d == 0 ].shape[0] / obs\n    \n  # Calculate the weighted average effect\n    return diff * weight\n\n\n# Apply the weighted_avg_effect function to each group in the data frame grouped by the s variable\nSDP2 = titanic_2.groupby('s').apply( weighted_avg_effect )\n\n# Calculate the weighted average effect of treatment for the entire population\nWSDP = SDP2.sum()\n\nWSDP\n\n0.24695099450754543\n\n\n\nThe probability of survival for the first-class passengers can be different across gender and age groups.\n\nIn other words, the effect of being the first-class on the probability of survival from the Titanic Disaster can be different across genders and age groups.\n\nWSDP takes into account the difference in the effect of being first-class across gender and age groups by weighting.\n\n\nWSDP - SDP\n\n-0.0682926841509281\n\n\n\nThe probability of survival for first-class after taking into account gender and age (WSDP) is less than that (SDP) which does presumably assume that characteristics of passengers such as gender and ages is related with the probability of survival."
  },
  {
    "objectID": "DANL210_hw1a.html#q2a",
    "href": "DANL210_hw1a.html#q2a",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2a",
    "text": "Q2a\n\nHow many players have been recorded?\n\n\n# the number of unique players\nq2a = nhl1617['id_player'].nunique()  \n\nq2a\n\n888"
  },
  {
    "objectID": "DANL210_hw1a.html#q2b.",
    "href": "DANL210_hw1a.html#q2b.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2b.",
    "text": "Q2b.\n\nA column points (“P”) is missing in the data. The number of points of a player is defined as the sum of his goals (“G”) and assists (“A”).\nAdd the point column “P” to your DataFrame.\n\n\n# create a new column called 'P' in the nhl1617 dataframe that is the sum of the 'G' and 'A' columns\nq2b = nhl1617.assign(P = nhl1617['G'] + nhl1617['A'])\n\nq2b\n\n\n\n\n\n  \n    \n      \n      id_player\n      Born\n      City\n      Cntry\n      Nat\n      Ht\n      Wt\n      Last_Name\n      First_Name\n      Position\n      Team\n      GP\n      G\n      A\n      TOI\n      TOI_GP\n      P\n    \n  \n  \n    \n      0\n      1\n      30.04.1988\n      Hamilton\n      CAN\n      CAN\n      69\n      170\n      Abbott\n      Spencer\n      LW\n      CHI\n      1\n      0\n      0\n      514\n      8.57\n      0\n    \n    \n      1\n      2\n      25.02.1987\n      Muskegon\n      USA\n      USA\n      74\n      218\n      Abdelkader\n      Justin\n      LW/RW\n      DET\n      64\n      7\n      14\n      63969\n      16.65\n      21\n    \n    \n      2\n      3\n      23.09.1993\n      Stockholm\n      SWE\n      SWE\n      71\n      196\n      Aberg\n      Pontus\n      LW\n      NSH\n      15\n      1\n      1\n      11102\n      12.33\n      2\n    \n    \n      3\n      4\n      01.12.1991\n      Johnston\n      USA\n      USA\n      70\n      208\n      Acciari\n      Noel\n      C\n      BOS\n      29\n      2\n      3\n      18047\n      10.23\n      5\n    \n    \n      4\n      5\n      30.04.1992\n      Morristown\n      USA\n      USA\n      72\n      202\n      Agostino\n      Kenny\n      LW\n      STL\n      7\n      1\n      2\n      5366\n      12.78\n      3\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      883\n      884\n      18.04.1993\n      Huddinge\n      SWE\n      SWE\n      74\n      215\n      Zibanejad\n      Mika\n      C/RW\n      NYR\n      56\n      14\n      23\n      57362\n      17.07\n      37\n    \n    \n      884\n      885\n      01.09.1987\n      Toronto\n      CAN\n      CAN\n      71\n      180\n      Zolnierczyk\n      Harry\n      LW\n      NSH\n      24\n      2\n      2\n      12776\n      8.87\n      4\n    \n    \n      885\n      886\n      01.09.1987\n      Oslo\n      NOR\n      NOR\n      67\n      179\n      Zuccarello\n      Mats\n      RW/C/LW\n      NYR\n      80\n      15\n      44\n      90378\n      18.83\n      59\n    \n    \n      886\n      887\n      16.01.1992\n      Newport Beach\n      USA\n      USA\n      71\n      187\n      Zucker\n      Jason\n      LW/RW\n      MIN\n      79\n      22\n      25\n      72455\n      15.28\n      47\n    \n    \n      887\n      888\n      15.05.1995\n      St. Petersburg\n      RUS\n      RUS\n      73\n      224\n      Zykov\n      Valentin\n      LW\n      CAR\n      2\n      1\n      0\n      750\n      6.25\n      1\n    \n  \n\n888 rows × 17 columns"
  },
  {
    "objectID": "DANL210_hw1a.html#q2c.",
    "href": "DANL210_hw1a.html#q2c.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2c.",
    "text": "Q2c.\n\nWho is the top scorer in terms of points?\n\n\n# Sort the DataFrame q2b by the column 'P' in descending order\nq2c = q2b.sort_values(by= 'P', ascending=False)\n\nq2c\n\n\n\n\n\n  \n    \n      \n      id_player\n      Born\n      City\n      Cntry\n      Nat\n      Ht\n      Wt\n      Last_Name\n      First_Name\n      Position\n      Team\n      GP\n      G\n      A\n      TOI\n      TOI_GP\n      P\n    \n  \n  \n    \n      509\n      510\n      13.01.1997\n      Richmond Hill\n      CAN\n      CAN\n      73\n      200\n      McDavid\n      Connor\n      C\n      EDM\n      82\n      30\n      70\n      103967\n      21.13\n      100\n    \n    \n      149\n      150\n      07.08.1987\n      Cole Harbour\n      CAN\n      CAN\n      71\n      200\n      Crosby\n      Sidney\n      C\n      PIT\n      75\n      44\n      45\n      89450\n      19.88\n      89\n    \n    \n      389\n      390\n      19.11.1988\n      Buffalo\n      USA\n      USA\n      71\n      177\n      Kane\n      Patrick\n      RW/C\n      CHI\n      82\n      34\n      55\n      105263\n      21.40\n      89\n    \n    \n      21\n      22\n      23.11.1987\n      Gävle\n      SWE\n      SWE\n      73\n      213\n      Backstrom\n      Nicklas\n      C\n      WSH\n      82\n      23\n      63\n      89839\n      18.27\n      86\n    \n    \n      426\n      427\n      17.06.1993\n      Maykop\n      RUS\n      RUS\n      71\n      178\n      Kucherov\n      Nikita\n      RW\n      T.B\n      74\n      40\n      45\n      86320\n      19.43\n      85\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      512\n      513\n      22.05.1994\n      Hamilton\n      CAN\n      CAN\n      74\n      203\n      McEneny\n      Evan\n      D\n      VAN\n      1\n      0\n      0\n      908\n      15.13\n      0\n    \n    \n      520\n      521\n      22.02.1993\n      Langley\n      CAN\n      CAN\n      74\n      214\n      McNeill\n      Mark\n      C/RW\n      DAL\n      1\n      0\n      0\n      829\n      13.82\n      0\n    \n    \n      781\n      782\n      22.04.1994\n      Saskatoon\n      CAN\n      CAN\n      72\n      204\n      Stephenson\n      Chandler\n      C\n      WSH\n      4\n      0\n      0\n      2131\n      8.88\n      0\n    \n    \n      523\n      524\n      10.12.1992\n      Plantation\n      USA\n      USA\n      78\n      225\n      Megna\n      Jaycob\n      D\n      ANA\n      1\n      0\n      0\n      920\n      15.33\n      0\n    \n    \n      0\n      1\n      30.04.1988\n      Hamilton\n      CAN\n      CAN\n      69\n      170\n      Abbott\n      Spencer\n      LW\n      CHI\n      1\n      0\n      0\n      514\n      8.57\n      0\n    \n  \n\n888 rows × 17 columns"
  },
  {
    "objectID": "DANL210_hw1a.html#q2d.",
    "href": "DANL210_hw1a.html#q2d.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2d.",
    "text": "Q2d.\n\nHow many Russian (non-goalie) players had some ice time in there 2016/2017 regular season?\nHint: Nationality of a player can be found in “Nat”. Russians are indicated by “RUS”.\n\n\n# the number of rows in the 'nhl1617' dataframe where the value in the 'Nat' column is equal to 'RUS', and returns the count as output.\nq2d = len( nhl1617[nhl1617['Nat'] == 'RUS'] )\n\nq2d\n\n38"
  },
  {
    "objectID": "DANL210_hw1a.html#q2e.",
    "href": "DANL210_hw1a.html#q2e.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2e.",
    "text": "Q2e.\n\nWhat are their names?\n\n\n# Select rows where the 'Nat' column equals 'RUS', then select only the 'Last_Name' and 'First_Name' columns\nq2e = nhl1617[ nhl1617['Nat'] == 'RUS' ][ ['Last_Name', 'First_Name'] ]\n\nq2e\n\n\n\n\n\n  \n    \n      \n      Last_Name\n      First_Name\n    \n  \n  \n    \n      11\n      Anisimov\n      Artem\n    \n    \n      27\n      Barbashev\n      Ivan\n    \n    \n      87\n      Buchnevich\n      Pavel\n    \n    \n      90\n      Burmistrov\n      Alex\n    \n    \n      207\n      Emelin\n      Alexei\n    \n    \n      272\n      Goldobin\n      Nikolay\n    \n    \n      293\n      Grigorenko\n      Mikhail\n    \n    \n      303\n      Gurianov\n      Denis\n    \n    \n      385\n      Kalinin\n      Sergey\n    \n    \n      386\n      Kamenev\n      Vladislav\n    \n    \n      426\n      Kucherov\n      Nikita\n    \n    \n      429\n      Kulemin\n      Nikolay\n    \n    \n      430\n      Kulikov\n      Dmitry\n    \n    \n      433\n      Kuznetsov\n      Evgeny\n    \n    \n      473\n      Lyubimov\n      Roman\n    \n    \n      480\n      Malkin\n      Evgeni\n    \n    \n      486\n      Marchenko\n      Alexey\n    \n    \n      489\n      Markov\n      Andrei\n    \n    \n      557\n      Namestnikov\n      Vladislav\n    \n    \n      566\n      Nesterov\n      Nikita\n    \n    \n      595\n      Orlov\n      Dmitry\n    \n    \n      600\n      Ovechkin\n      Alex\n    \n    \n      607\n      Panarin\n      Artemi\n    \n    \n      646\n      Provorov\n      Ivan\n    \n    \n      657\n      Radulov\n      Alex\n    \n    \n      707\n      Scherbak\n      Nikita\n    \n    \n      724\n      Sergachev\n      Mikhail\n    \n    \n      749\n      Slepyshev\n      Anton\n    \n    \n      763\n      Soshnikov\n      Nikita\n    \n    \n      799\n      Svechnikov\n      Evgeny\n    \n    \n      802\n      Tarasenko\n      Vladimir\n    \n    \n      819\n      Tolchinsky\n      Sergey\n    \n    \n      824\n      Tryamkin\n      Nikita\n    \n    \n      828\n      Tyutin\n      Fedor\n    \n    \n      875\n      Yakupov\n      Nail\n    \n    \n      878\n      Zadorov\n      Nikita\n    \n    \n      879\n      Zaitsev\n      Nikita\n    \n    \n      887\n      Zykov\n      Valentin"
  },
  {
    "objectID": "DANL210_hw1a.html#q2f.",
    "href": "DANL210_hw1a.html#q2f.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2f.",
    "text": "Q2f.\n\nWho performed best among the Russian players in terms of points (“P”)?\n\n\n# Select only the rows where 'Nat' column is 'RUS'\n# Then sort these rows in descending order based on the values in the 'P' column\nq2f = q2b[q2b['Nat'] == 'RUS'].sort_values(by='P', ascending = False)\n\nq2f\n\n\n\n\n\n  \n    \n      \n      id_player\n      Born\n      City\n      Cntry\n      Nat\n      Ht\n      Wt\n      Last_Name\n      First_Name\n      Position\n      Team\n      GP\n      G\n      A\n      TOI\n      TOI_GP\n      P\n    \n  \n  \n    \n      426\n      427\n      17.06.1993\n      Maykop\n      RUS\n      RUS\n      71\n      178\n      Kucherov\n      Nikita\n      RW\n      T.B\n      74\n      40\n      45\n      86320\n      19.43\n      85\n    \n    \n      802\n      803\n      13.12.1991\n      Yaroslavl\n      RUS\n      RUS\n      72\n      219\n      Tarasenko\n      Vladimir\n      RW\n      STL\n      82\n      39\n      36\n      90872\n      18.47\n      75\n    \n    \n      607\n      608\n      30.10.1991\n      Korkino\n      RUS\n      RUS\n      71\n      170\n      Panarin\n      Artemi\n      LW/C\n      CHI\n      82\n      31\n      43\n      95798\n      19.47\n      74\n    \n    \n      480\n      481\n      31.07.1986\n      Magnitogorsk\n      RUS\n      RUS\n      75\n      195\n      Malkin\n      Evgeni\n      C/RW\n      PIT\n      62\n      33\n      39\n      69263\n      18.62\n      72\n    \n    \n      600\n      601\n      17.09.1985\n      Moscow\n      RUS\n      RUS\n      75\n      239\n      Ovechkin\n      Alex\n      LW/RW\n      WSH\n      82\n      33\n      36\n      90361\n      18.37\n      69\n    \n    \n      433\n      434\n      19.05.1992\n      Chelyabinsk\n      RUS\n      RUS\n      74\n      192\n      Kuznetsov\n      Evgeny\n      C/LW\n      WSH\n      82\n      19\n      40\n      83410\n      16.95\n      59\n    \n    \n      657\n      658\n      05.07.1986\n      Nizhny Tagil\n      RUS\n      RUS\n      74\n      205\n      Radulov\n      Alex\n      RW\n      MTL\n      76\n      18\n      36\n      83405\n      18.30\n      54\n    \n    \n      11\n      12\n      24.05.1988\n      Yaroslavl\n      RUS\n      RUS\n      76\n      198\n      Anisimov\n      Artem\n      C/LW\n      CHI\n      64\n      22\n      23\n      68529\n      17.85\n      45\n    \n    \n      489\n      490\n      20.12.1978\n      Voskresensk\n      RUS\n      RUS\n      72\n      200\n      Markov\n      Andrei\n      D\n      MTL\n      62\n      6\n      30\n      81230\n      21.83\n      36\n    \n    \n      879\n      880\n      29.10.1991\n      Moscow\n      RUS\n      RUS\n      74\n      195\n      Zaitsev\n      Nikita\n      D\n      TOR\n      82\n      4\n      32\n      108332\n      21.85\n      36\n    \n    \n      595\n      596\n      23.07.1991\n      Novokuznetsk\n      RUS\n      RUS\n      71\n      212\n      Orlov\n      Dmitry\n      D\n      WSH\n      82\n      6\n      27\n      96107\n      19.53\n      33\n    \n    \n      646\n      647\n      13.01.1997\n      Yaroslavl\n      RUS\n      RUS\n      73\n      201\n      Provorov\n      Ivan\n      D\n      PHI\n      82\n      6\n      24\n      108132\n      21.98\n      30\n    \n    \n      557\n      558\n      22.11.1992\n      Zhukovskiy\n      RUS\n      RUS\n      71\n      180\n      Namestnikov\n      Vladislav\n      C/LW\n      T.B\n      74\n      10\n      18\n      65645\n      14.78\n      28\n    \n    \n      293\n      294\n      16.05.1994\n      Khabarovsk\n      RUS\n      RUS\n      75\n      209\n      Grigorenko\n      Mikhail\n      C\n      COL\n      75\n      10\n      13\n      63401\n      14.08\n      23\n    \n    \n      429\n      430\n      14.07.1986\n      Magnitogorsk\n      RUS\n      RUS\n      73\n      225\n      Kulemin\n      Nikolay\n      LW/RW\n      NYI\n      72\n      12\n      11\n      59691\n      13.82\n      23\n    \n    \n      87\n      88\n      17.04.1995\n      Cherepovets\n      RUS\n      RUS\n      74\n      193\n      Buchnevich\n      Pavel\n      RW/LW\n      NYR\n      41\n      8\n      12\n      32616\n      13.25\n      20\n    \n    \n      566\n      567\n      28.03.1993\n      Chelyabinsk\n      RUS\n      RUS\n      71\n      191\n      Nesterov\n      Nikita\n      D\n      MTL/T.B\n      48\n      4\n      13\n      46901\n      16.28\n      17\n    \n    \n      90\n      91\n      21.10.1991\n      Kazan\n      RUS\n      RUS\n      73\n      180\n      Burmistrov\n      Alex\n      C/RW\n      ARI/WPG\n      49\n      5\n      11\n      39266\n      13.35\n      16\n    \n    \n      828\n      829\n      19.07.1983\n      Izhevsk\n      RUS\n      RUS\n      74\n      221\n      Tyutin\n      Fedor\n      D\n      COL\n      69\n      1\n      12\n      78405\n      18.93\n      13\n    \n    \n      27\n      28\n      14.12.1995\n      Moscow\n      RUS\n      RUS\n      72\n      180\n      Barbashev\n      Ivan\n      C\n      STL\n      30\n      5\n      7\n      21224\n      11.78\n      12\n    \n    \n      749\n      750\n      13.05.1994\n      Penza\n      RUS\n      RUS\n      74\n      218\n      Slepyshev\n      Anton\n      LW\n      EDM\n      41\n      4\n      6\n      27342\n      11.12\n      10\n    \n    \n      878\n      879\n      16.04.1995\n      Moscow\n      RUS\n      RUS\n      77\n      230\n      Zadorov\n      Nikita\n      D\n      COL\n      56\n      0\n      10\n      63960\n      19.03\n      10\n    \n    \n      207\n      208\n      25.04.1986\n      Togliatti\n      RUS\n      RUS\n      74\n      218\n      Emelin\n      Alexei\n      D\n      MTL\n      76\n      2\n      8\n      97227\n      21.32\n      10\n    \n    \n      763\n      764\n      14.10.1993\n      Nizhny Tagil\n      RUS\n      RUS\n      71\n      190\n      Soshnikov\n      Nikita\n      RW\n      TOR\n      56\n      5\n      4\n      36450\n      10.70\n      9\n    \n    \n      875\n      876\n      06.10.1993\n      Nizhnekamsk\n      RUS\n      RUS\n      71\n      195\n      Yakupov\n      Nail\n      RW/LW\n      STL\n      40\n      3\n      6\n      25553\n      10.67\n      9\n    \n    \n      824\n      825\n      30.08.1994\n      Yekaterinburg\n      RUS\n      RUS\n      79\n      265\n      Tryamkin\n      Nikita\n      D\n      VAN\n      66\n      2\n      7\n      66291\n      16.73\n      9\n    \n    \n      486\n      487\n      02.01.1992\n      Moscow\n      RUS\n      RUS\n      75\n      210\n      Marchenko\n      Alexey\n      D\n      DET/TOR\n      41\n      1\n      7\n      41700\n      16.95\n      8\n    \n    \n      473\n      474\n      06.01.1992\n      Tver\n      RUS\n      RUS\n      74\n      207\n      Lyubimov\n      Roman\n      LW\n      PHI\n      47\n      4\n      2\n      27011\n      9.58\n      6\n    \n    \n      430\n      431\n      29.10.1990\n      Lipetsk\n      RUS\n      RUS\n      73\n      204\n      Kulikov\n      Dmitry\n      D\n      BUF\n      47\n      2\n      3\n      61766\n      21.90\n      5\n    \n    \n      385\n      386\n      17.03.1991\n      Omsk\n      RUS\n      RUS\n      75\n      200\n      Kalinin\n      Sergey\n      C/RW\n      N.J\n      43\n      2\n      2\n      33008\n      12.63\n      4\n    \n    \n      272\n      273\n      07.10.1995\n      Moscow\n      RUS\n      RUS\n      71\n      185\n      Goldobin\n      Nikolay\n      RW/LW\n      S.J/VAN\n      14\n      3\n      0\n      9551\n      11.37\n      3\n    \n    \n      707\n      708\n      30.12.1995\n      Moscow\n      RUS\n      RUS\n      74\n      190\n      Scherbak\n      Nikita\n      RW\n      MTL\n      3\n      1\n      0\n      2048\n      11.38\n      1\n    \n    \n      819\n      820\n      03.02.1995\n      Moscow\n      RUS\n      RUS\n      68\n      170\n      Tolchinsky\n      Sergey\n      LW\n      CAR\n      2\n      0\n      1\n      1377\n      11.48\n      1\n    \n    \n      887\n      888\n      15.05.1995\n      St. Petersburg\n      RUS\n      RUS\n      73\n      224\n      Zykov\n      Valentin\n      LW\n      CAR\n      2\n      1\n      0\n      750\n      6.25\n      1\n    \n    \n      724\n      725\n      25.06.1998\n      Nizhnekamsk\n      RUS\n      RUS\n      75\n      215\n      Sergachev\n      Mikhail\n      D\n      MTL\n      4\n      0\n      0\n      2910\n      12.13\n      0\n    \n    \n      799\n      800\n      31.10.1996\n      Yuzhno-Sakhalinsk\n      RUS\n      RUS\n      75\n      205\n      Svechnikov\n      Evgeny\n      RW/LW\n      DET\n      2\n      0\n      0\n      1577\n      13.15\n      0\n    \n    \n      303\n      304\n      07.06.1997\n      Togliatti\n      RUS\n      RUS\n      75\n      200\n      Gurianov\n      Denis\n      RW\n      DAL\n      1\n      0\n      0\n      786\n      13.10\n      0\n    \n    \n      386\n      387\n      12.08.1996\n      Orsk\n      RUS\n      RUS\n      74\n      194\n      Kamenev\n      Vladislav\n      LW/C\n      NSH\n      2\n      0\n      0\n      1207\n      10.07\n      0"
  },
  {
    "objectID": "DANL210_hw1a.html#q2g.",
    "href": "DANL210_hw1a.html#q2g.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2g.",
    "text": "Q2g.\n\nHow many points (“P”) did he have?\n\n\nq2g = (\n  q2b[q2b['Nat'] == 'RUS'] \n  .sort_values(by='P', ascending=False)\n  .head(1)[['Last_Name', 'First_Name', 'P']]\n  )\n  \nq2g\n\n\n\n\n\n  \n    \n      \n      Last_Name\n      First_Name\n      P\n    \n  \n  \n    \n      426\n      Kucherov\n      Nikita\n      85\n    \n  \n\n\n\n\n\nThe above code creates a new DataFrame called q2g, which:\n\nFilters q2b for players whose nationality is ‘RUS’\nSorts the filtered DataFrame by the ‘P’ column in descending order\nSelects the top row of the sorted DataFrame using the .head(1) method\nFilters the columns ‘Last_Name’, ‘First_Name’, and ‘P’ from the selected row"
  },
  {
    "objectID": "DANL210_hw1a.html#q2h.",
    "href": "DANL210_hw1a.html#q2h.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2h.",
    "text": "Q2h.\n\nHow well did he perform in the entire league? Put differently, what was his rank in terms of points?\n\n\nq2h = (\n     q2b.assign(ranking = q2b['P'].rank(method = 'min', ascending=False))  # Create a new column 'ranking' based on the values in column 'P'\n        .sort_values(by='P', ascending=False)  # Sort the DataFrame by 'P' column in descending order\n        .loc[q2b['Nat'] == 'RUS']  # Filter the rows where the 'Nat' column is 'RUS'\n        .head(1)[['ranking', 'Last_Name', 'First_Name', 'P']]  # Select the top row and specific columns\n        )\n\nq2h\n\n\n\n\n\n  \n    \n      \n      ranking\n      Last_Name\n      First_Name\n      P\n    \n  \n  \n    \n      426\n      5.0\n      Kucherov\n      Nikita\n      85"
  },
  {
    "objectID": "DANL210_hw1a.html#q2i.",
    "href": "DANL210_hw1a.html#q2i.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2i.",
    "text": "Q2i.\n\nFind the top ten scorers (in terms of points) and print them including their number of point and their respective team.\n\n\nq2i = ( q2b.assign( ranking = q2b['P'].rank(ascending=False) )   # add a new column 'ranking' based on the 'P' column's rank\n           .sort_values(by='P', ascending=False)                  # sort by 'P' column in descending order\n           )\n\n( q2i.loc[q2i['ranking'] <= 10]                                    # select rows where 'ranking' is less than or equal to 10\n            [['ranking', 'Last_Name', 'First_Name', 'P', 'Team']]  # select specific columns\n            )\n\n\n\n\n\n  \n    \n      \n      ranking\n      Last_Name\n      First_Name\n      P\n      Team\n    \n  \n  \n    \n      509\n      1.0\n      McDavid\n      Connor\n      100\n      EDM\n    \n    \n      149\n      2.5\n      Crosby\n      Sidney\n      89\n      PIT\n    \n    \n      389\n      2.5\n      Kane\n      Patrick\n      89\n      CHI\n    \n    \n      21\n      4.0\n      Backstrom\n      Nicklas\n      86\n      WSH\n    \n    \n      426\n      5.5\n      Kucherov\n      Nikita\n      85\n      T.B\n    \n    \n      485\n      5.5\n      Marchand\n      Brad\n      85\n      BOS\n    \n    \n      704\n      7.0\n      Scheifele\n      Mark\n      82\n      WPG\n    \n    \n      184\n      8.0\n      Draisaitl\n      Leon\n      77\n      EDM\n    \n    \n      91\n      9.0\n      Burns\n      Brent\n      76\n      S.J\n    \n    \n      802\n      10.0\n      Tarasenko\n      Vladimir\n      75\n      STL"
  },
  {
    "objectID": "DANL210_hw1a.html#q2j.",
    "href": "DANL210_hw1a.html#q2j.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q2j.",
    "text": "Q2j.\n\nWhat are the three countries with the most players originating from?\n\n\n# Create a dataframe of count of players by nationality\nq2j = q2b[['Nat']].value_counts().reset_index()\n\n# Rename the columns of the dataframe\nq2j.columns = ['Nat', 'counts']\n\n# Create a new column 'ranking' with the ranking of each nationality by count\nq2j = ( q2j.assign( ranking = q2j['counts']\n                             .rank(method = 'dense', ascending=False) )\n           \n           # Filter the dataframe to only include the top 3 nationalities by count\n           .query('ranking <= 3' ) \n      )"
  },
  {
    "objectID": "DANL210_hw1a.html#q3a.",
    "href": "DANL210_hw1a.html#q3a.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q3a.",
    "text": "Q3a.\n\nFor each type of mine, calculate the total coal production for each pair of state-year.\n\n\n# Create a new column 'production' which is the sum of two other columns\ncoal['production'] = coal['production_underground'] + coal['production_surface']\n\n# Group the data by 'state' and 'year' columns and calculate the sum of 'production', 'production_underground', and 'production_surface'\nq3a = (\n       coal[ ['state', 'year', 'production',\n             'production_underground',\n             'production_surface'] ]\n       .groupby(['state', 'year'])\n       .sum()\n       )\n\nq3a\n\n\n\n\n\n  \n    \n      \n      \n      production\n      production_underground\n      production_surface\n    \n    \n      state\n      year\n      \n      \n      \n    \n  \n  \n    \n      Alabama\n      2011\n      19071\n      10878\n      8193\n    \n    \n      2012\n      19320\n      12569\n      6751\n    \n    \n      2013\n      18620\n      13515\n      5105\n    \n    \n      2014\n      16363\n      12517\n      3846\n    \n    \n      2015\n      13191\n      9897\n      3294\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Wyoming\n      2014\n      395665\n      3370\n      392295\n    \n    \n      2015\n      375773\n      3090\n      372683\n    \n    \n      2016\n      297218\n      1167\n      296051\n    \n    \n      2017\n      316454\n      1716\n      314738\n    \n    \n      2018\n      304188\n      2210\n      301978\n    \n  \n\n197 rows × 3 columns"
  },
  {
    "objectID": "DANL210_hw1a.html#q3b.",
    "href": "DANL210_hw1a.html#q3b.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q3b.",
    "text": "Q3b.\n\nFind the top 5 coal-producing states for each year.\n\n\n# Creates a new dataframe q3b\nq3b = (\n       # Reset the index of q3a\n       q3a.reset_index()\n       # Sort the values of q3a by the production column in descending order\n       .sort_values(['production'], ascending = False)\n       # Group q3a by year and get the head with the largest production for each year\n       .groupby('year')\n       .head()\n       # Sort q3b first by year in ascending order and then by production in descending order\n       .sort_values(['year','production'], ascending = [True, False])\n       )\n\nq3b\n\n\n\n\n\n  \n    \n      \n      state\n      year\n      production\n      production_underground\n      production_surface\n    \n  \n  \n    \n      189\n      Wyoming\n      2011\n      438673\n      3043\n      435630\n    \n    \n      181\n      West Virginia\n      2011\n      130186\n      80403\n      49783\n    \n    \n      61\n      Kentucky\n      2011\n      108768\n      65250\n      43518\n    \n    \n      141\n      Pennsylvania\n      2011\n      59183\n      47318\n      11865\n    \n    \n      157\n      Texas\n      2011\n      45903\n      0\n      45903\n    \n    \n      190\n      Wyoming\n      2012\n      401442\n      4637\n      396805\n    \n    \n      182\n      West Virginia\n      2012\n      115247\n      76281\n      38966\n    \n    \n      62\n      Kentucky\n      2012\n      90865\n      58200\n      32665\n    \n    \n      142\n      Pennsylvania\n      2012\n      54719\n      45042\n      9677\n    \n    \n      40\n      Illinois\n      2012\n      48485\n      42835\n      5650\n    \n    \n      191\n      Wyoming\n      2013\n      387924\n      4443\n      383481\n    \n    \n      183\n      West Virginia\n      2013\n      106868\n      74105\n      32763\n    \n    \n      63\n      Kentucky\n      2013\n      80379\n      54621\n      25758\n    \n    \n      143\n      Pennsylvania\n      2013\n      54007\n      45165\n      8842\n    \n    \n      41\n      Illinois\n      2013\n      52148\n      46447\n      5701\n    \n    \n      192\n      Wyoming\n      2014\n      395665\n      3370\n      392295\n    \n    \n      184\n      West Virginia\n      2014\n      107566\n      77074\n      30492\n    \n    \n      64\n      Kentucky\n      2014\n      77337\n      52809\n      24528\n    \n    \n      144\n      Pennsylvania\n      2014\n      60913\n      52913\n      8000\n    \n    \n      42\n      Illinois\n      2014\n      57969\n      52714\n      5255\n    \n    \n      193\n      Wyoming\n      2015\n      375773\n      3090\n      372683\n    \n    \n      185\n      West Virginia\n      2015\n      91174\n      71098\n      20076\n    \n    \n      65\n      Kentucky\n      2015\n      61426\n      43379\n      18047\n    \n    \n      43\n      Illinois\n      2015\n      56100\n      51973\n      4127\n    \n    \n      145\n      Pennsylvania\n      2015\n      50031\n      43894\n      6137\n    \n    \n      194\n      Wyoming\n      2016\n      297218\n      1167\n      296051\n    \n    \n      186\n      West Virginia\n      2016\n      76116\n      61316\n      14800\n    \n    \n      146\n      Pennsylvania\n      2016\n      45718\n      41385\n      4333\n    \n    \n      44\n      Illinois\n      2016\n      43423\n      41258\n      2165\n    \n    \n      66\n      Kentucky\n      2016\n      42868\n      32713\n      10155\n    \n    \n      195\n      Wyoming\n      2017\n      316454\n      1716\n      314738\n    \n    \n      187\n      West Virginia\n      2017\n      87800\n      69175\n      18625\n    \n    \n      147\n      Pennsylvania\n      2017\n      49084\n      43586\n      5498\n    \n    \n      45\n      Illinois\n      2017\n      48204\n      44905\n      3299\n    \n    \n      67\n      Kentucky\n      2017\n      41786\n      31512\n      10274\n    \n    \n      196\n      Wyoming\n      2018\n      304188\n      2210\n      301978\n    \n    \n      188\n      West Virginia\n      2018\n      89876\n      69366\n      20510\n    \n    \n      148\n      Pennsylvania\n      2018\n      49882\n      44633\n      5249\n    \n    \n      46\n      Illinois\n      2018\n      49564\n      46073\n      3491\n    \n    \n      68\n      Kentucky\n      2018\n      39569\n      30964\n      8605"
  },
  {
    "objectID": "DANL210_hw1a.html#q3c.",
    "href": "DANL210_hw1a.html#q3c.",
    "title": "DANL 210 - Homework Assignment 1",
    "section": "Q3c.",
    "text": "Q3c.\n\nVisualize the yearly trend of the total coal production from each type of mine.\n\n\n# selecting the columns 'year', 'production_underground', and 'production_surface' from the original DataFrame coal, grouping them by year, and summing the values of each group.\nq3c = (\n  coal[['year','production_underground', 'production_surface']]\n    .groupby(['year']).sum()\n    )\n\n\n# create a line plot for the 'production_underground' column of the q3c DataFrame.\nq3c['production_underground'].plot()\n\n<AxesSubplot:xlabel='year'>\n\n\n\n\n\n\n# create a line plot for the 'production_surface' column of the q3c DataFrame.\nq3c['production_surface'].plot()\n\n<AxesSubplot:xlabel='year'>\n\n\n\n\n\n\n# create a line plot for the 'production_underground' and 'production_surface' columns of the q3c DataFrame on the same graph.\nq3c.plot()\n\n<AxesSubplot:xlabel='year'>"
  },
  {
    "objectID": "mba-ch1-lm.html",
    "href": "mba-ch1-lm.html",
    "title": "Linear Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr) \nlibrary(ggfortify) # to create regression-related plots\nlibrary(ggcorrplot) # to create correlation heatmaps\nlibrary(fastDummies) # to create dummy variables\nlibrary(stargazer) # to create regression tables\n\noj <- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')"
  },
  {
    "objectID": "mba-ch1-lm.html#exploratory-data-analysis",
    "href": "mba-ch1-lm.html#exploratory-data-analysis",
    "title": "Linear Regression",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDescriptive Statistics\n\nskim(oj)\n\n\nData summary\n\n\nName\noj\n\n\nNumber of rows\n28947\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nbrand\n0\n1\n9\n11\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales\n0\n1\n17312.21\n27477.66\n64.00\n4864.00\n8384.00\n17408.00\n716416.00\n▇▁▁▁▁\n\n\nprice\n0\n1\n2.28\n0.65\n0.52\n1.79\n2.17\n2.73\n3.87\n▁▆▇▅▂\n\n\nad\n0\n1\n0.24\n0.43\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\n\n\noj %>% group_by(brand) %>% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n28947\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nbrand\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nbrand\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales\ndominicks\n0\n1\n19834.56\n32245.47\n64.00\n4416.00\n9152.00\n21056.00\n716416.00\n▇▁▁▁▁\n\n\nsales\nminute.maid\n0\n1\n18238.46\n29992.21\n320.00\n4800.00\n8320.00\n18560.00\n591360.00\n▇▁▁▁▁\n\n\nsales\ntropicana\n0\n1\n13863.62\n17515.82\n192.00\n5248.00\n8000.00\n13824.00\n288384.00\n▇▁▁▁▁\n\n\nprice\ndominicks\n0\n1\n1.74\n0.39\n0.52\n1.58\n1.59\n1.99\n2.69\n▁▂▇▃▂\n\n\nprice\nminute.maid\n0\n1\n2.24\n0.40\n0.88\n1.99\n2.17\n2.49\n3.17\n▁▂▇▆▂\n\n\nprice\ntropicana\n0\n1\n2.87\n0.55\n1.29\n2.49\n2.99\n3.19\n3.87\n▁▃▅▇▅\n\n\nad\ndominicks\n0\n1\n0.26\n0.44\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nad\nminute.maid\n0\n1\n0.29\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nad\ntropicana\n0\n1\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\n\n\noj %>% group_by(brand, ad) %>% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n28947\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nbrand, ad\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nbrand\nad\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales\ndominicks\n0\n0\n1\n11733.91\n15718.77\n64.00\n3584.00\n7040.00\n13312.00\n248000.00\n▇▁▁▁▁\n\n\nsales\ndominicks\n1\n0\n1\n43251.33\n50930.45\n64.00\n11888.00\n27744.00\n54160.00\n716416.00\n▇▁▁▁▁\n\n\nsales\nminute.maid\n0\n0\n1\n8372.65\n7701.69\n320.00\n4160.00\n6272.00\n9984.00\n231808.00\n▇▁▁▁▁\n\n\nsales\nminute.maid\n1\n0\n1\n42566.32\n46260.27\n1728.00\n16576.00\n29440.00\n49808.00\n591360.00\n▇▁▁▁▁\n\n\nsales\ntropicana\n0\n0\n1\n10038.86\n10246.02\n192.00\n4864.00\n7168.00\n11008.00\n175872.00\n▇▁▁▁▁\n\n\nsales\ntropicana\n1\n0\n1\n33047.02\n29632.94\n1408.00\n10816.00\n23808.00\n46896.00\n288384.00\n▇▂▁▁▁\n\n\nprice\ndominicks\n0\n0\n1\n1.80\n0.38\n0.52\n1.58\n1.69\n1.99\n2.69\n▁▂▇▅▂\n\n\nprice\ndominicks\n1\n0\n1\n1.56\n0.34\n0.89\n1.39\n1.58\n1.59\n2.69\n▂▇▂▁▁\n\n\nprice\nminute.maid\n0\n0\n1\n2.33\n0.41\n0.88\n1.99\n2.26\n2.62\n3.17\n▁▂▇▇▃\n\n\nprice\nminute.maid\n1\n0\n1\n2.02\n0.30\n0.99\n1.99\n1.99\n2.19\n2.81\n▁▂▇▂▂\n\n\nprice\ntropicana\n0\n0\n1\n2.97\n0.51\n1.32\n2.59\n2.99\n3.39\n3.87\n▁▂▃▇▅\n\n\nprice\ntropicana\n1\n0\n1\n2.39\n0.46\n1.29\n1.99\n2.39\n2.79\n3.59\n▁▇▆▅▂\n\n\n\n\n\n\n\n\nData Visualization\n\nCorrelation heatmap is a great tool to start identifying which input variables are strongly correlated with an outcome variable.\n\n\n# to convert a factor variable into indicators\noj_dummies <- dummy_cols(oj, select_columns = 'brand' ) %>% \n  select(-brand)\n\n# the matrix of the correlation test p-values\np.mat <- cor_pmat(oj_dummies) \n\n# correlation heatmap with correlation values\nggcorrplot( cor(oj_dummies), lab = T,\n            type = 'lower',\n            colors = c(\"#2E74C0\", \"white\", \"#CB454A\"),\n            p.mat = p.mat) # p.values\n\n\n\n# variation in log price\nggplot(oj, aes(x = log(price), fill = brand )) +\n  geom_histogram() +\n  facet_wrap(brand ~., ncol = 1)\n\n\n\n# variation in log sales\nggplot(oj, aes(x = log(sales), fill = brand )) +\n  geom_histogram() +\n  facet_wrap(brand ~., ncol = 1)\n\n\n\n# law of demand\np <- ggplot(oj, aes(x = log(sales), y = log(price),\n                    color = brand ))\n\np + geom_point( alpha = .025 ) +\n  geom_smooth(method = lm, se = F)\n\n\n\n# mosaic plot\nggplot(data = oj) +\n  geom_bar(aes(x = as.factor(ad), y = after_stat(prop),\n               group = brand, fill = brand), \n           position = \"fill\") +\n  labs(x = 'ad') +\n  theme(plot.title = element_text(size = rel(1.5)),\n        axis.title = element_text(size = 25),\n        axis.text.x = element_text(size = rel(1.5)),\n        axis.text.y = element_text(size = rel(1.5)))"
  },
  {
    "objectID": "mba-ch1-lm.html#linear-regression-model",
    "href": "mba-ch1-lm.html#linear-regression-model",
    "title": "Linear Regression",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\nA basic but powerful regression strategy is to deal in averages and lines.\n\nWe model the conditional mean for \\(y\\) given \\(x\\) as\n\n\n\\[\n\\begin{align}\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] &= \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}\\\\\n{ }\\\\\ny_{i} &=   \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1, i} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p, i} + \\epsilon_{i} \\quad \\text{for } i = 1, 2, ..., n\n\\end{align}\n\\] - Linear regression is used to model linear relationship between an outcome variable, \\(y\\), and a set of predictor variables \\(x_{1}, \\,\\cdots\\,, x_{p}\\).\n\nknitr::include_graphics('lec_figs/mba-1-2.png')\n\n\n\n\n\n\n\n\n\n\\(\\beta_{0}\\) is an intercept when \\(\\mathbf{X} = \\mathbf{0}\\).\n\\(\\beta_{1}\\) is a slope that describes a change in average value for \\(y\\) for each one-unit increase in \\(x_{1}\\).\n\\(\\epsilon_{i}\\) is the random noise.\n\nFor inference, we need to assume that \\(\\epsilon_{i}\\) is independent, identically distributed (iid) from Normal distribution.\n\n\n\\[\n\\epsilon_i \\overset{iid}{\\sim}N(0, \\sigma^2) \\quad \\text{ with constant variance } \\sigma^2\n\\]\n\nknitr::include_graphics('lec_figs/mba-1-3.png')\n\n\n\n\n\n\n\n\n\n\nFitted Line and Beta Estimates\n\nWe estimatede the best fitting line by ordinary least squares (OLS) - by minimizing the sum of squared errors (SSE)\n\n\\[\nS S E\\left(\\beta_{0}, \\beta_{1}\\right)=\\sum_{i=1}^{n}\\left[Y_{i}-\\left({\\beta}_{0}+{\\beta}_{1} X_{i}\\right)\\right]^{2}\n\\]\n\nknitr::include_graphics('lec_figs/mba-1-9.png')\n\n\n\n\n\n\n\n\nTherefore, the beta estimate has the following solution:\n\\[\n\\widehat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}} \\quad \\text{ and } \\quad \\widehat{\\beta}_{0}=\\bar{Y}-\\widehat{\\beta}_{1} \\bar{X}\n\\]\nwhere \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) and \\(\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\)\n\n\n\nConnection to covariance and correlation\n\nCovariance describes the joint variability of two variables\n\n\\[\n\\text{Cov}(X, Y) = \\sigma_{X,Y} = \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n\\]\n\nCorrelation is a normalized form of covariance, ranges from -1 to 1\n\n\\[\n\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}\n\\] - So, the beta coefficent can be represented by:\n$$ \\[\\begin{align}\n\\widehat{\\beta}_{1} &= \\, \\frac{\\widehat{\\text{Cov}}(X,Y)}{\\widehat{\\text{Var}}(X)} \\,=\\,  \\hat{\\rho}_{x,y} \\cdot \\frac{\\hat{\\sigma}_{Y}}{\\hat{\\sigma}_{X}}\n\n\\end{align}\\] $$\n\n\n\nInference with OLS\n\n\\(t\\)-statistics are coefficients Estimates / Std. Error, i.e., number of standard deviations from 0\n\np-values (i.e., Pr(>|t|)): estimated probability observing value as extreme as |t value| given the null hypothesis \\(\\beta = 0\\)\np-value \\(<\\) conventional threshold of \\(\\alpha = 0.05\\), sufficient evidence to reject the null hypothesis that the coefficient is zero,\nTypically |t values| \\(> 2\\) indicate significant relationship at \\(\\alpha = 0.05\\)\ni.e., there is a significant association between log(sales) and log(price)\n\nControlling the Type 1 error rate at \\(\\alpha = 0.05\\), i.e., the probability of a false positive mistake:\n\n5% chance that you’ll conclude there’s a significant association between \\(x\\) and \\(y\\) even when there is none\n\n\n\n\n\nR-squared\n\n\\(R^2\\) estimates the proportion of the variance of \\(Y\\) explained by \\(X\\).\n\n\n\n\nMean Squared Errors\n\nMean squared error (MSE) is a commonly used metric to evaluate the performance of a regression model.\n\nIt measures the average squared difference between the predicted values and the actual values in the dataset.\n\n\n\\[\nM S E \\,=\\, \\frac{\\sum_{i = 1}^{ n}\\,(\\, y_{i} - \\hat{y}_{i} \\,)^2}{n}\n\\] - Root mean squared error (RMSE) is the square root of the mean squared error (MSE).\n\\[\nR M S E \\,=\\, \\sqrt{M S E}\n\\] - RMSE shows how far predictions fall from true values.\n\n\n\nThe Goals of Linear Regression\n\nModeling for prediction (\\(\\hat{y}\\)): When we want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(\\mathbf{X}\\).\n\n\nWe are estimating the conditional expection (mean) for \\(y\\): \\[\n\\mathbb{E}[\\, y \\,|\\, \\mathbf{X} \\,] = \\beta_{0} \\,+\\, \\beta_{1}\\,x_{1} \\,+\\, \\cdots \\,+\\, \\beta_{p}\\,x_{p}.\n\\]\nwhich is the average value for \\(y\\) given the value for \\(X\\).\n\n\n\nModeling for explanation (\\(\\hat{\\beta}\\)): When we want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(\\mathbf{X}\\).\n\n\nCorrelation does not imply causation.\nWithout proper identification strategies, \\(\\beta_{1}\\) just means a correlation between \\(x_{1}\\) and \\(y\\).\nHowever, we can possibly identify a causal relationship between the explanatory variable and the outcome variable."
  },
  {
    "objectID": "mba-ch1-lm.html#linear-regression-and-controls",
    "href": "mba-ch1-lm.html#linear-regression-and-controls",
    "title": "Linear Regression",
    "section": "Linear Regression and Controls",
    "text": "Linear Regression and Controls\n\nSimple Linear Regression\nTo start, we can fit a simple model that regresses log price on log sales.\n\\[\n\\mathbb{E}[\\, \\log(\\, \\texttt{sales} \\,) \\,|\\, \\texttt{price}\\,] = \\alpha \\,+\\, \\beta\\, \\log(\\,\\texttt{price}\\,)\n\\] - The following model incorporates both brand and price:\n\\[\n\\mathbb{E}[\\, \\log(\\, \\texttt{sales} \\,) \\,|\\, \\texttt{price}\\,] = \\alpha_{\\texttt{brand}} \\,+\\, \\beta\\, \\log(\\,\\texttt{price}\\,)\n\\]\nformula_0 <- log(sales) ~ log(price)\nformula_1 <- log(sales) ~ brand + log(price)\n\nfit_0 <- lm( formula_0, data = oj )\nfit_1 <- lm( formula_1, data = oj )\n\nstargazer(fit_0, fit_1, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlog(sales)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nbrandminute.maid\n\n\n\n\n0.870***\n\n\n\n\n\n\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana\n\n\n\n\n1.530***\n\n\n\n\n\n\n\n\n(0.016)\n\n\n\n\n\n\n\n\n\n\n\n\nlog(price)\n\n\n-1.601***\n\n\n-3.139***\n\n\n\n\n\n\n(0.018)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n10.423***\n\n\n10.829***\n\n\n\n\n\n\n(0.015)\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n28,947\n\n\n28,947\n\n\n\n\nR2\n\n\n0.208\n\n\n0.394\n\n\n\n\nAdjusted R2\n\n\n0.208\n\n\n0.394\n\n\n\n\nResidual Std. Error\n\n\n0.907 (df = 28945)\n\n\n0.794 (df = 28943)\n\n\n\n\nF Statistic\n\n\n7,608.212*** (df = 1; 28945)\n\n\n6,275.074*** (df = 3; 28943)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nWe know that there are different brands of OJ here and some are more valuable than others.\n\nWhen we control for brand effect, the elasticity estimate nearly doubles to −3.14.\n\nThe premium brands, Minute Maid and Tropicana, had equivalent sales to Dominick’s at higher price points.\nSo if we don’t control for brand, it looks as though prices can rise without affecting sales for those observations.\n\nThis dampens the observable relationship between prices and sales and results in the (artificially) low elasticity estimate of −1.6.\n\nMore mechanically, how does this happen in regression?\n\n\nprice_fit <- lm(log(price) ~ brand, data = oj)\np_hat <- predict(price_fit, newdata = oj)\np_resid <- log(oj$price) - p_hat\n\n# regress log sales on p_resid\nresid_fit <- lm(log(sales) ~ p_resid, data = oj)\n\n# What is the beta coefficient for p_resid?!\nround( coef(resid_fit)[2], digit = 3 )\n\np_resid \n -3.139 \n\n\n\nThe coefficient on p_resid, the residuals from regression of log price on brand, is exactly the same as what we get on log(price) in the multiple linear regression for log sales onto this and brand!\nThis is one way that you can understand what OLS is doing:\n\nIt is finding the coefficients on the part of each input that is independent from the other inputs.\n\n\n\n\n\nControls in Linear Regression\n\nOmitted variable bias is a type of bias that can occur in linear regression when an important variable that is related to both the outcome variable and the input variable(s) is not included in the model.\n\nThis omission can lead to biased estimates of the beta coefficients of the included input variables.\n\nIn linear regression, a confounding variable is a variable that is related to both treatment and outcome variables, and that affects the relationship between them.\n\nA treatment variable is an input variable that the researcher believes has a causal effect on the outcome variable.\nWhen a confounding variable is not controlled in the regression model, it can lead to biased estimates of the relationship between the independent and treatment variables.\n\nBad controls in linear regression refer to the inclusion of variables in the model that do not actually control for the confounding factors they are intended to control for.\n\nThis can lead to biased estimates of the relationship between the independent and dependent variables.\n\n\n# simulation data\ntb <- tibble( \n  female = ifelse(runif(10000)>=0.5,1,0), # female indicator variable\n  ability = rnorm(10000), # e.g., talent, usually unobserved.\n  discrimination = female, # gender discrimination variable\n  occupation = 1 + 2*ability + 0*female - 2*discrimination + rnorm(10000), # true data generating process for occupation variable\n  wage = 1 - 1*discrimination + 1*occupation + 2*ability + rnorm(10000) # true data generating process for wage variable\n)\n\nlm_1 <- lm(wage ~ female, tb)\nlm_2 <- lm(wage ~ female + occupation, tb)\nlm_3 <- lm(wage ~ female + occupation + ability, tb)\n\nstargazer(lm_1,lm_2,lm_3, \n          column.labels = c(\"Biased Unconditional\", \n                            \"Biased\",\n                            \"Unbiased Conditional\"),\n          type = 'html')\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nwage\n\n\n\n\n\n\nBiased Unconditional\n\n\nBiased\n\n\nUnbiased Conditional\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nfemale\n\n\n-3.051***\n\n\n0.659***\n\n\n-1.008***\n\n\n\n\n\n\n(0.085)\n\n\n(0.030)\n\n\n(0.028)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noccupation\n\n\n\n\n1.814***\n\n\n0.987***\n\n\n\n\n\n\n\n\n(0.006)\n\n\n(0.010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nability\n\n\n\n\n\n\n2.048***\n\n\n\n\n\n\n\n\n\n\n(0.022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n2.077***\n\n\n0.200***\n\n\n1.018***\n\n\n\n\n\n\n(0.061)\n\n\n(0.020)\n\n\n(0.017)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n10,000\n\n\n10,000\n\n\n10,000\n\n\n\n\nR2\n\n\n0.114\n\n\n0.911\n\n\n0.952\n\n\n\n\nAdjusted R2\n\n\n0.114\n\n\n0.911\n\n\n0.952\n\n\n\n\nResidual Std. Error\n\n\n4.256 (df = 9998)\n\n\n1.346 (df = 9997)\n\n\n0.988 (df = 9996)\n\n\n\n\nF Statistic\n\n\n1,284.360*** (df = 1; 9998)\n\n\n51,365.840*** (df = 2; 9997)\n\n\n66,387.710*** (df = 3; 9996)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\noccupation is a bad control.\n\n\n\n\nResidual plot, QQ plot, and Residual vs Leverage plot\n\nResiduals should NOT display any systematic pattern.\nCheck the assumptions about normality with a QQ plot using ggfortify.\n\n\nlibrary(ggfortify)\nautoplot(fit_1, ncol = 1)\n\n\n\n\n\nStandardized residuals = residuals / sd(residuals)\nA QQ (Quantile-Quantile) plot is a graphical tool used to assess the normality of a distribution.\n\nIn the context of linear regression, a QQ plot can be used to assess whether the residuals are normally distributed.\nA QQ plot visualizes the relationship between the quantiles of the residuals and the quantiles of a theoretical normal distribution.\nQQ plots can be useful in identifying potential outliers or influential observations in a linear regression model, as well as in deciding whether to transform the dependent variable or use a different type of regression model altogether.\n\nA residual vs leverage plot is a graphical tool used to detect influential observations in linear regression.\n\nLeverage refers to how much an observation’s independent variables differ from the mean of the independent variables, and is a measure of how much influence that observation has on the regression line.\nIn a residual vs leverage plot, influential observations will typically appear as points that are far away from the center of the plot.\nIf an observation has high leverage but a small residual, it may not be influential.\nConversely, an observation with a large residual but low leverage may also not be influential."
  },
  {
    "objectID": "mba-ch1-lm.html#exercise",
    "href": "mba-ch1-lm.html#exercise",
    "title": "Linear Regression",
    "section": "Exercise",
    "text": "Exercise\nConsider the orange juice models:\n\nformula_0 <- log(sales) ~ log(price)\nformula_1 <- log(sales) ~ brand + log(price)\n\nfit_0 <- lm( formula_0, data = oj )\nfit_1 <- lm( formula_1, data = oj )\n\n\nDraw a residual plot for each model of fit_0 and fit_1.\nCalculate the RMSE for each model of fit_0 and fit_1.\nReview the interaction models:\n\nformula_0 <- log(sales) ~ log(price)\nformula_1 <- log(sales) ~ brand + log(price)\nformula_2 <- log(sales) ~ brand * log(price)\nformula_3 <- log(sales) ~ brand * ad * log(price)\n\nfit_0 <- lm( formula_0, data = oj )\nfit_1 <- lm( formula_1, data = oj )\nfit_2 <- lm( formula_2, data = oj )\nfit_3 <- lm( formula_3, data = oj )\n\nstargazer(fit_1, fit_2, fit_3, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlog(sales)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nbrandminute.maid\n\n\n0.870***\n\n\n0.888***\n\n\n0.047\n\n\n\n\n\n\n(0.013)\n\n\n(0.042)\n\n\n(0.047)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana\n\n\n1.530***\n\n\n0.962***\n\n\n0.708***\n\n\n\n\n\n\n(0.016)\n\n\n(0.046)\n\n\n(0.051)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nad\n\n\n\n\n\n\n1.094***\n\n\n\n\n\n\n\n\n\n\n(0.038)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(price)\n\n\n-3.139***\n\n\n-3.378***\n\n\n-2.774***\n\n\n\n\n\n\n(0.023)\n\n\n(0.036)\n\n\n(0.039)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandminute.maid:ad\n\n\n\n\n\n\n1.173***\n\n\n\n\n\n\n\n\n\n\n(0.082)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana:ad\n\n\n\n\n\n\n0.785***\n\n\n\n\n\n\n\n\n\n\n(0.099)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandminute.maid:log(price)\n\n\n\n\n0.057\n\n\n0.783***\n\n\n\n\n\n\n\n\n(0.057)\n\n\n(0.061)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana:log(price)\n\n\n\n\n0.666***\n\n\n0.736***\n\n\n\n\n\n\n\n\n(0.054)\n\n\n(0.057)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nad:log(price)\n\n\n\n\n\n\n-0.471***\n\n\n\n\n\n\n\n\n\n\n(0.074)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandminute.maid:ad:log(price)\n\n\n\n\n\n\n-1.109***\n\n\n\n\n\n\n\n\n\n\n(0.122)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrandtropicana:ad:log(price)\n\n\n\n\n\n\n-0.986***\n\n\n\n\n\n\n\n\n\n\n(0.124)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n10.829***\n\n\n10.955***\n\n\n10.407***\n\n\n\n\n\n\n(0.015)\n\n\n(0.021)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n28,947\n\n\n28,947\n\n\n28,947\n\n\n\n\nR2\n\n\n0.394\n\n\n0.398\n\n\n0.535\n\n\n\n\nAdjusted R2\n\n\n0.394\n\n\n0.398\n\n\n0.535\n\n\n\n\nResidual Std. Error\n\n\n0.794 (df = 28943)\n\n\n0.791 (df = 28941)\n\n\n0.695 (df = 28935)\n\n\n\n\nF Statistic\n\n\n6,275.074*** (df = 3; 28943)\n\n\n3,823.404*** (df = 5; 28941)\n\n\n3,031.232*** (df = 11; 28935)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01"
  },
  {
    "objectID": "mba-ch1-lm.html#references",
    "href": "mba-ch1-lm.html#references",
    "title": "Linear Regression",
    "section": "References",
    "text": "References\n\n\nCausal Inference: The Mixtape by Scott Cunningham.\nStatistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim.\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\n\n\n\nSummer Undergraduate Research Experience (SURE) 2022 in Statistics at Carnegie Mellon University by Ron Yurko."
  },
  {
    "objectID": "DANL210_lab2q.html",
    "href": "DANL210_lab2q.html",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "",
    "text": "import pandas as pd\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab2q.html#load-dataframe",
    "href": "DANL210_lab2q.html#load-dataframe",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbeer_mkt = pd.read_csv('https://bcdanl.github.io/data/beer_markets.csv')\nbeer_mkt.head(10)\n\n\n\n\n\n  \n    \n      \n      hh\n      _purchase_desc\n      quantity\n      brand\n      dollar_spent\n      beer_floz\n      price_per_floz\n      container\n      promo\n      market\n      ...\n      age\n      employment\n      degree\n      cow\n      race\n      microwave\n      dishwasher\n      tvcable\n      singlefamilyhome\n      npeople\n    \n  \n  \n    \n      0\n      2000946\n      BUD LT BR CN 12P\n      1\n      BUD LIGHT\n      8.14\n      144.0\n      0.056528\n      CAN\n      False\n      RURAL ILLINOIS\n      ...\n      50+\n      none\n      Grad\n      none/retired/student\n      white\n      True\n      True\n      premium\n      False\n      1\n    \n    \n      1\n      2003036\n      BUD LT BR CN 24P\n      1\n      BUD LIGHT\n      17.48\n      288.0\n      0.060694\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      2\n      2003036\n      BUD LT BR CN 24P\n      2\n      BUD LIGHT\n      33.92\n      576.0\n      0.058889\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      3\n      2003036\n      BUD LT BR CN 30P\n      2\n      BUD LIGHT\n      34.74\n      720.0\n      0.048250\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      4\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      40.48\n      864.0\n      0.046852\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      5\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      42.96\n      864.0\n      0.049722\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      6\n      2003036\n      BUD LT BR CN 36P\n      2\n      BUD LIGHT\n      40.96\n      864.0\n      0.047407\n      CAN\n      False\n      ATLANTA\n      ...\n      50+\n      full\n      College\n      clerical/sales/service\n      white\n      True\n      True\n      basic\n      True\n      2\n    \n    \n      7\n      2001521\n      BUD LT BR CN 6P\n      5\n      BUD LIGHT\n      30.60\n      480.0\n      0.063750\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n    \n      8\n      2001521\n      BUD LT BR CN 6P\n      1\n      BUD LIGHT\n      9.99\n      96.0\n      0.104063\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n    \n      9\n      2001521\n      BUD LT BR CN 6P\n      5\n      BUD LIGHT\n      30.70\n      480.0\n      0.063958\n      CAN\n      False\n      RURAL INDIANA\n      ...\n      50+\n      none\n      College\n      none/retired/student\n      white\n      True\n      True\n      none\n      False\n      1\n    \n  \n\n10 rows × 24 columns\n\n\n\n\nVariable Description\n\nhh: An identifier of the purchasing household;\n_purchase_desc: Details on the purchased item;\nquantity: Number of items purchased;\nbrand: BUD LIGHT, BUSCH LIGHT, COORS LIGHT, MILLER LITE, or NATURAL LIGHT;\nspent: Total dollar value of purchase;\nbeer_floz: Total volume of beer, in fluid ounces;\nprice_per_floz: Price per fl.oz. (i.e., spent/beer_floz);\ncontainer: Type of container;\npromo: Whether the item was promoted (coupon or something else);\nmarket: Scan-track market (or state if rural);\nvarious demographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher.\n\nSummarize DataFrame beer_mkt.\n\n\nskim(beer_mkt)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 73115  │ │ string      │ 13    │                                                          │\n│ │ Number of columns │ 24     │ │ bool        │ 6     │                                                          │\n│ └───────────────────┴────────┘ │ float64     │ 3     │                                                          │\n│                                │ int64       │ 2     │                                                          │\n│                                └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name      ┃ NA  ┃ NA %  ┃ mean      ┃ sd        ┃ p0       ┃ p25     ┃ p75      ┃ p100     ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━┩  │\n│ │ hh               │   0 │     0 │  17000000 │  12000000 │  2000000 │ 8200000 │ 30000000 │ 30000000 │ ▂█   █ │  │\n│ │ quantity         │   0 │     0 │       1.3 │       1.1 │        1 │       1 │        1 │       48 │   █    │  │\n│ │ dollar_spent     │   0 │     0 │        14 │       8.7 │     0.51 │       9 │       16 │      160 │   █▁   │  │\n│ │ beer_floz        │   0 │     0 │       270 │       200 │       12 │     140 │      360 │     9200 │   █    │  │\n│ │ price_per_floz   │   0 │     0 │     0.056 │     0.013 │   0.0013 │   0.046 │    0.064 │     0.23 │   ▁█   │  │\n│ └──────────────────┴─────┴───────┴───────────┴───────────┴──────────┴─────────┴──────────┴──────────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                   ┃ NA     ┃ NA %       ┃ words per row               ┃ total words            ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ _purchase_desc                │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ brand                         │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ container                     │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ market                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ buyertype                     │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ income                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ age                           │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ employment                    │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ degree                        │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ cow                           │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ race                          │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ tvcable                       │      0 │          0 │                         5.3 │                 390000 │  │\n│ │ npeople                       │      0 │          0 │                         5.3 │                 390000 │  │\n│ └───────────────────────────────┴────────┴────────────┴─────────────────────────────┴────────────────────────┘  │\n│                                                      bool                                                       │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                               ┃ true            ┃ true rate                 ┃ hist             ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩  │\n│ │ promo                                     │           15000 │                       0.2 │      █    ▂      │  │\n│ │ childrenUnder6                            │            5000 │                     0.068 │      █    ▁      │  │\n│ │ children6to17                             │           15000 │                       0.2 │      █    ▂      │  │\n│ │ microwave                                 │           73000 │                      0.99 │           █      │  │\n│ │ dishwasher                                │           53000 │                      0.73 │      ▃    █      │  │\n│ │ singlefamilyhome                          │           59000 │                      0.81 │      ▂    █      │  │\n│ └───────────────────────────────────────────┴─────────────────┴───────────────────────────┴──────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nbeer_mkt.describe()\n\n\n\n\n\n  \n    \n      \n      hh\n      quantity\n      dollar_spent\n      beer_floz\n      price_per_floz\n    \n  \n  \n    \n      count\n      7.311500e+04\n      73115.000000\n      73115.000000\n      73115.000000\n      73115.000000\n    \n    \n      mean\n      1.740772e+07\n      1.317527\n      13.777683\n      265.926853\n      0.055951\n    \n    \n      std\n      1.158215e+07\n      1.149649\n      8.722942\n      199.522488\n      0.013417\n    \n    \n      min\n      2.000235e+06\n      1.000000\n      0.510000\n      12.000000\n      0.001315\n    \n    \n      25%\n      8.223438e+06\n      1.000000\n      8.970000\n      144.000000\n      0.046306\n    \n    \n      50%\n      8.413624e+06\n      1.000000\n      12.990000\n      216.000000\n      0.055509\n    \n    \n      75%\n      3.017132e+07\n      1.000000\n      16.380000\n      360.000000\n      0.063750\n    \n    \n      max\n      3.044072e+07\n      48.000000\n      159.130000\n      9216.000000\n      0.234063"
  },
  {
    "objectID": "DANL210_lab2q.html#q1a",
    "href": "DANL210_lab2q.html#q1a",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1a",
    "text": "Q1a\n\nSort the DataFrame beer_mkt by hh in ascending order."
  },
  {
    "objectID": "DANL210_lab2q.html#q1b",
    "href": "DANL210_lab2q.html#q1b",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1b",
    "text": "Q1b\n\nFind the top 5 beer markets in terms of the number of households that purchased beer."
  },
  {
    "objectID": "DANL210_lab2q.html#q1c",
    "href": "DANL210_lab2q.html#q1c",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1c",
    "text": "Q1c\n\nFind the top 5 beer markets in terms of the amount of total beer consumption."
  },
  {
    "objectID": "DANL210_lab2q.html#q1d",
    "href": "DANL210_lab2q.html#q1d",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1d",
    "text": "Q1d\n\nProvide (1) seaborn code and (2) a simple comment to describe how the distribution of price_per_floz varies by brand."
  },
  {
    "objectID": "DANL210_lab2q.html#q1e",
    "href": "DANL210_lab2q.html#q1e",
    "title": "Python Lab 2 - EDA with pandas and seaborn",
    "section": "Q1e",
    "text": "Q1e\n\nProvide (1) seaborn code and (2) a simple comment to describe how the relationship between price_per_floz and beer_floz varies by brand."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html",
    "href": "DANL200_midterm-spring-2022-a.html",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q1a.",
    "href": "DANL200_midterm-spring-2022-a.html#q1a.",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1a.",
    "text": "Q1a.\nDownload dominick_oj_q1a.csv from the Midterm Exam in the Assignments or the Files sections in our Canvas.\nThen import the dominick_oj_q1a.csv using the following lines:\n\noj_q1a <- read_csv('ABSOLUTE_PATH_NAME_FOR_THE_FILE_dominick_oj_q1a.csv')\ntable(oj_q1a$brand)\n\n\nYou need to provide the absolute path name for the file, dominick_oj_q1a.csv to the above read_csv() function to read the file.\n\nVariable Description\n\nsales: the number of orange juice (OJ) cartons sold in a week\nprice: price of OJ carton\nbrand: OJ brand\nfeat: Advertisement status— 1 if advertised; 0 if not advertised.\nReport (1) minimum, (2) median, (3) maximum, (4) mean, and (5) standard deviation of variable price for the brand, Dominick’s OJ.\n\n\nlibrary(tidyverse)\noj_q1a <- read_csv('/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/dominick_oj_q1a.csv')\ntable(oj_q1a$brand)\n\n\n  dominicks minute.maid   tropicana \n       9602        9649        9649 \n\nlibrary(skimr)\noj_q1a_d <- filter(oj_q1a, brand == 'dominicks')\nskim(oj_q1a_d$price)\n\n\n\n\nData summary\n\n\n\n\nName\n\n\noj_q1a_d$price\n\n\n\n\nNumber of rows\n\n\n9602\n\n\n\n\nNumber of columns\n\n\n1\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n1\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\ndata\n\n\n0\n\n\n1\n\n\n1.74\n\n\n0.39\n\n\n0.52\n\n\n1.58\n\n\n1.59\n\n\n1.99\n\n\n2.69\n\n\n▁▂▇▃▂\n\n\n\n\n\n\n\n\nminimum, (2) median, (3) maximum, (4) mean, and (5) standard deviation of variable price for the brand, dominicks are ……"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q1b",
    "href": "DANL200_midterm-spring-2022-a.html#q1b",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1b",
    "text": "Q1b\nFor Question 1b, run the following function to read the dominick_oj.csv file:\n\noj_q1b <- read_csv(\n  'https://bcdanl.github.io/data/dominick_oj.csv'\n)\n\n\nThe description of variables in oj_q1b is the same as oj_q1a.\n\nDescribe the relationship between the log of price and the log of sales by brand using ggplot. Make a simple comment on your ggplot figure.\n\n\nsummary(oj_q1b)\n\n     sales            price          brand                 ad        \n Min.   :    64   Min.   :0.520   Length:28947       Min.   :0.0000  \n 1st Qu.:  4864   1st Qu.:1.790   Class :character   1st Qu.:0.0000  \n Median :  8384   Median :2.170   Mode  :character   Median :0.0000  \n Mean   : 17312   Mean   :2.282                      Mean   :0.2373  \n 3rd Qu.: 17408   3rd Qu.:2.730                      3rd Qu.:0.0000  \n Max.   :716416   Max.   :3.870                      Max.   :1.0000  \n\nggplot(data = oj_q1b, \n       mapping = aes(x = log(price), y = log(sales),\n                     color = brand)) +\n  geom_point(alpha = .05) +\n  geom_smooth(method = lm, se = F)\n\n\n\n\nHere we observe the downward sloping demand curve for each OJ brand.\nFor the same level of price, the demand for tropicana is the highest, followed by minute.maid and dominicks.\n\ntropicana is the luxurious OJ;\ndominicks OJ seems to be a budget option.\nminute.maid is somewhat between the two."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q2a",
    "href": "DANL200_midterm-spring-2022-a.html#q2a",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2a",
    "text": "Q2a\nDescribe the distribution of animal_gender using ggplot. Make a simple comment on your ggplot figure.\n\nggplot(nyc_dogs) +\n  geom_bar(aes(x = animal_gender))\n\n\n\nggplot(nyc_dogs) +\n  geom_density(aes(x = animal_gender))\n\n\n\nggplot(nyc_dogs) +\n  geom_boxplot(aes(x = animal_gender))\n\n\n\nggplot(nyc_dogs) +\n  geom_bar(aes(x = animal_gender)) +\n  facet_wrap(~borough)\n\n\n\n\n\nThere are more male dogs than female dogs.\nThere are more male dogs than female dogs in each borough."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q2b",
    "href": "DANL200_midterm-spring-2022-a.html#q2b",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2b",
    "text": "Q2b\nFind the five most popular breeds in NYC.\n\n# 0.\ntable(nyc_dogs$breed_rc)\n\n\n                      Affenpinscher                        Afghan Hound \n                                136                                  89 \n            Afghan Hound Crossbreed                    Airedale Terrier \n                                 19                                 227 \n                              Akita                    Akita Crossbreed \n                                491                                 151 \n                   Alaskan Klee Kai                    Alaskan Malamute \n                                113                                 287 \n                     American Bully          American English Coonhound \n                               1100                                 103 \n                American Eskimo Dog                   American Foxhound \n                               1145                                 265 \n          American Hairless Terrier              American Leopard Hound \n                                 15                                  11 \n     American Staffordshire Terrier              American Water Spaniel \n                               3052                                  15 \n             Anatolian Shepherd Dog               Australian Cattle Dog \n                                305                                 730 \n               Australian Cattledog                   Australian Kelpie \n                               1324                                 126 \n                Australian Shepherd            Australian Silky Terrier \n                               4212                                 149 \n                 Australian Terrier                             Azawakh \n                                107                                   3 \n                             Baladi                              Barbet \n                                  8                                  15 \n                            Basenji                        Basset Hound \n                                595                                 526 \n            Basset Hound Crossbreed                       Bassett Hound \n                                102                                 571 \n                             Beagle                   Beagle Crossbreed \n                               7822                                4224 \n                     Bearded Collie                           Beauceron \n                                 80                                  11 \n                 Bedlington Terrier                     Belgian Griffon \n                                 92                                  24 \n                  Belgian Laekenois                    Belgian Malinois \n                                  4                                 407 \n                   Belgian Sheepdog                    Belgian Tervuren \n                                 78                                  17 \n                      Berger Picard                Bernese Mountain Dog \n                                 23                                 923 \n                       Bichon Frise             Bichon Frise Crossbreed \n                               4310                                1620 \n                     Biewer Terrier             Black And Tan Coonhound \n                                 50                                 162 \n              Black Russian Terrier                          Bloodhound \n                                 33                                 108 \n              Bloodhound Crossbreed                  Bluetick Coonhound \n                                 58                                 134 \n                           Boerboel                           Bolognese \n                                 56                                  43 \n                      Border Collie            Border Collie Crossbreed \n                               1485                                 725 \n                     Border Terrier                              Borzoi \n                                800                                  64 \n                     Boston Terrier                Bouvier Des Flandres \n                               4670                                  71 \n                              Boxer                    Boxer Crossbreed \n                               3713                                 776 \n                     Boykin Spaniel                     Bracco Italiano \n                                 29                                   7 \n                             Briard                            Brittany \n                                 61                                  93 \n                   Brittany Spaniel                    Brussels Griffon \n                                358                                1137 \n        Brussels Griffon Crossbreed                  Bull Dog, American \n                                 35                                 327 \n                  Bull Dog, English                    Bull Dog, French \n                               3294                                2538 \n                       Bull Terrier                             Bulldog \n                                817                                 973 \n                        Bullmastiff                       Cairn Terrier \n                                100                                1764 \n                         Canaan Dog                          Cane Corso \n                                 83                                 854 \n               Cardigan Welsh Corgi               Catahoula Leopard Dog \n                                213                                 542 \n      Cavalier King Charles Spaniel                       Cesky Terrier \n                               5747                                   4 \n           Chesapeake Bay Retriever                           Chihuahua \n                                100                               21211 \n               Chihuahua Crossbreed                     Chinese Crested \n                               7149                                 188 \n                   Chinese Shar-Pei                             Chinook \n                                187                                  20 \n                          Chow Chow                        Cirneco Dell \n                                891                                  12 \n                    Clumber Spaniel                            Cockapoo \n                                 46                                 778 \n                     Cocker Spaniel           Cocker Spaniel Crossbreed \n                               5676                                1046 \n                             Collie                   Collie Crossbreed \n                                277                                 869 \n                    Collie, Bearded                      Collie, Border \n                                 86                                 841 \n                 Collie, Rough Coat                 Collie, Smooth Coat \n                                146                                 141 \n           Coonhound, Black And Tan                Coonhound, Blue Tick \n                                389                                 107 \n                 Coonhound, Redbone           Coonhound, Treeing Walker \n                                 18                                 290 \n                    Coton De Tulear                    Cotton De Tulear \n                                537                                 706 \n             Curly-Coated Retriever                           Dachshund \n                                 13                                3104 \n               Dachshund Crossbreed               Dachshund Smooth Coat \n                               1345                                2496 \n    Dachshund Smooth Coat Miniature              Dachshund, Long Haired \n                               1888                                 714 \n   Dachshund, Long Haired Miniature               Dachshund, Wirehaired \n                               1131                                 249 \n   Dachshund, Wirehaired, Miniature                           Dalmatian \n                                267                                 369 \n                      Dalmatian Mix              Dandie Dinmont Terrier \n                                 15                                  20 \n                  Doberman Pinscher                      Dogo Argentino \n                               1021                                 111 \n                  Dogue De Bordeaux                      Dutch Shepherd \n                                 56                                 164 \n             English Cocker Spaniel                    English Foxhound \n                                456                                  68 \n                     English Setter           English Setter Crossbreed \n                                 91                                   9 \n           English Springer Spaniel                 English Toy Spaniel \n                                317                                  33 \n           Entlebucher Mountain Dog                Estrela Mountain Dog \n                                 34                                   4 \n                           Eurasier                       Field Spaniel \n                                 11                                  41 \n                    Fila Brasileiro                    Finnish Lapphund \n                                 11                                   3 \n                      Finnish Spitz               Flat-Coated Retriever \n                                 44                                 144 \n                     French Bulldog                      French Spaniel \n                               6364                                  17 \n          German Longhaired Pointer                     German Pinscher \n                                 21                                  49 \n         German Shepherd Crossbreed                 German Shepherd Dog \n                               5683                                7129 \n         German Shorthaired Pointer                        German Spitz \n                                364                                  51 \n          German Wirehaired Pointer                     Giant Schnauzer \n                                 41                                  58 \n              Glen Of Imaal Terrier                    Golden Retriever \n                                 47                                7325 \n                       Goldendoodle                       Gordon Setter \n                               3849                                  32 \n       Grand Basset Griffon Vendeen                          Great Dane \n                                  4                                 546 \n                     Great Pyrenees          Greater Swiss Mountain Dog \n                                496                                  49 \n                     Greek Shephard                           Greyhound \n                                  2                                 860 \n                    Hamiltonstovare                             Harrier \n                                  3                                  44 \n                           Havanese                            Hovawart \n                               8606                                  15 \n                       Ibizan Hound                  Icelandic Sheepdog \n                                 47                                  11 \n         Irish Red And White Setter                        Irish Setter \n                                 14                                 150 \n                      Irish Terrier                 Irish Water Spaniel \n                                200                                   2 \n                    Irish Wolfhound                   Italian Greyhound \n                                 60                                1096 \n               Jack Russell Terrier     Jack Russell Terrier Crossbreed \n                               6880                                 697 \n                        Jagdterrier                       Japanese Chin \n                                  3                                 203 \n              Japanese Chin/Spaniel                      Japanese Spitz \n                                248                                  83 \n                              Jindo                    Jindo Dog, Korea \n                                299                                 937 \n                            Kai Ken                   Karelian Bear Dog \n                                 10                                   2 \n                           Keeshond                  Kerry Blue Terrier \n                                123                                  65 \n                          Kishu Ken                       Kooikerhondje \n                                  1                                  25 \n                     Kromfohrlander                              Kuvasz \n                                  8                                  18 \n                        Labradoodle            Labrador (or Crossbreed) \n                               3685                               28399 \n                  Lagotto Romagnolo                    Lakeland Terrier \n                                194                                  38 \n                  Lancashire Heeler                          Leonberger \n                                 10                                  26 \n                         Lhasa Apso                             Lowchen \n                               2486                                  31 \n                            Maltese                  Maltese Crossbreed \n                              15701                                4498 \n                           Maltipoo                  Manchester Terrier \n                               2358                                 158 \n                            Mastiff                       Mastiff, Bull \n                                294                                 192 \nMastiff, French (Dogue De Bordeaux)                 Mastiff, Neapolitan \n                                 51                                  40 \n               Mastiff, Old English                    Mastiff, Tibetan \n                                125                                  22 \n        Miniature American Shepherd       Miniature Australian Shepherd \n                                449                                 829 \n             Miniature Bull Terrier               Miniature Fox Terrier \n                                 53                                  15 \n                 Miniature Pinscher                 Miniature Schnauzer \n                               3242                                2298 \n                             Morkie                      Mountain Feist \n                               3049                                  13 \n                               Mudi                  Neapolitan Mastiff \n                                 11                                  12 \n                       Newfoundland                     Norfolk Terrier \n                                263                                 328 \n                   Norwegian Buhund                  Norwegian Elkhound \n                                  7                                 110 \n                    Norwich Terrier  Nova Scotia Duck Tolling Retriever \n                                399                                 101 \n               Old English Sheepdog              Olde English Bulldogge \n                                294                                  11 \n                         Otterhound                            Papillon \n                                  4                                1430 \n             Parson Russell Terrier                  Patterdale Terrier \n                                208                                   8 \n                          Pekingese                Pembroke Welsh Corgi \n                               2043                                1113 \n    Pembroke Welsh Corgi Crossbreed              Perro De Presa Canario \n                                130                                  24 \n               Peruvian Inca Orchid        Petit Basset Griffon Vendeen \n                                  5                                  99 \n                      Pharaoh Hound                        Pharoh Hound \n                                102                                  51 \n                  Pit Bull (or Mix)                               Plott \n                              24393                                 491 \n                            Pointer         Pointer, German Shorthaired \n                                861                                 227 \n         Pointer, German Wirehaired                        Polish Hound \n                                 59                                  19 \n            Polish Lowland Sheepdog                          Pomeranian \n                                 17                                9287 \n              Pomeranian Crossbreed                              Pomsky \n                               1355                                 381 \n                             Poodle                   Poodle Crossbreed \n                               4723                                6178 \n                  Poodle, Miniature                    Poodle, Standard \n                               4567                                3105 \n                        Poodle, Toy          Portuguese Podengo Pequeno \n                               4923                                  58 \n                Portuguese Sheepdog                Portuguese Water Dog \n                                  1                                 557 \n                                Pug                      Pug Crossbreed \n                               5203                                 534 \n                             Puggle                                Puli \n                               2889                                  72 \n                               Pumi                   Pyrenean Shepherd \n                                 10                                  13 \n                        Rat Terrier              Rat Terrier Crossbreed \n                               1181                                 240 \n                  Redbone Coonhound                 Rhodesian Ridgeback \n                                175                                 935 \n                         Rottweiler               Rottweiler Crossbreed \n                               2126                                 339 \n                    Russell Terrier                         Russian Toy \n                                 89                                  26 \n                  Russian Wolfhound                       Saint Bernard \n                                  2                                 118 \n                             Saluki                             Samoyed \n                                 48                                 437 \n                         Schipperke                         Schipperkee \n                                 83                                 135 \n               Schnauzer Crossbreed                    Schnauzer, Giant \n                                624                                 107 \n               Schnauzer, Miniature     Schnauzer, Miniature Crossbreed \n                               3398                                 310 \n                Schnauzer, Standard                  Scottish Deerhound \n                                659                                  12 \n                   Scottish Terrier                    Sealyham Terrier \n                                561                                  31 \n                  Shar-Pei, Chinese                  Shepard Crossbreed \n                                704                                 771 \n                  Shetland Sheepdog                           Shiba Inu \n                               1070                                6109 \n               Shiba Inu Crossbreed                            Shih Tzu \n                                288                               27407 \n                Shih Tzu Crossbreed                      Siberian Husky \n                               8098                                4999 \n          Siberian Husky Crossbreed                       Silky Terrier \n                                770                                 943 \n                       Skye Terrier         Small Munsterlander Pointer \n                                 31                                   5 \n                 Smooth Fox Terrier         Soft Coated Wheaten Terrier \n                                 71                                 363 \n                  Spanish Water Dog                    Spinone Italiano \n                                 27                                  66 \n                        St. Bernard                           Stabyhoun \n                                 74                                   1 \n         Staffordshire Bull Terrier                  Standard Schnauzer \n                               1230                                  31 \n                     Sussex Spaniel                    Swedish Lapphund \n                                 20                                   1 \n                   Swedish Vallhund                  Terrier Crossbreed \n                                 11                                2369 \n                        Terrier Mix                      Thai Ridgeback \n                               5293                                  45 \n                    Tibetan Mastiff                     Tibetan Spaniel \n                                 26                                 253 \n                    Tibetan Terrier                             Tornjak \n                                781                                   1 \n                               Tosa                     Toy Fox Terrier \n                                  3                                 194 \n          Treeing Tennessee Brindle            Treeing Walker Coonhound \n                                 34                                 200 \n                            Unknown                              Vizsla \n                              54586                                 877 \n                         Weimaraner               Welsh Corgi, Cardigan \n                                559                                 310 \n              Welsh Corgi, Pembroke              Welsh Springer Spaniel \n                               1123                                  75 \n                      Welsh Terrier             West High White Terrier \n                                118                                1619 \n        West Highland White Terrier                     Wheaton Terrier \n                                835                                1539 \n                            Whippet                    Wire Fox Terrier \n                                600                                 583 \n        Wirehaired Pointing Griffon                   Wirehaired Vizsla \n                                 78                                   8 \n                     Xoloitzcuintli                   Yorkshire Terrier \n                                 44                               30379 \n       Yorkshire Terrier Crossbreed \n                               4554 \n\n# 1.\nq2b <- group_by(nyc_dogs, \n                breed_rc)\nq2b <- summarise(q2b,\n                 n = n())\nq2b <- arrange(q2b,\n               -n)  # or desc(n)\nq2b\n\n# A tibble: 327 × 2\n   breed_rc                     n\n   <chr>                    <int>\n 1 Unknown                  54586\n 2 Yorkshire Terrier        30379\n 3 Labrador (or Crossbreed) 28399\n 4 Shih Tzu                 27407\n 5 Pit Bull (or Mix)        24393\n 6 Chihuahua                21211\n 7 Maltese                  15701\n 8 Pomeranian                9287\n 9 Havanese                  8606\n10 Shih Tzu Crossbreed       8098\n# … with 317 more rows\n\n# Or with pipes!\nq2b <- nyc_dogs %>% \n  group_by(breed_rc) %>% \n  summarise(n = n()) %>% \n  arrange(-n)\n\n\n# 2.\nq2b <- group_by(nyc_dogs, \n                breed_rc)\n\nq2b <- mutate(q2b,\n              n = n())\n\nq2b <- arrange(q2b,\n               desc(n))\n\nq2b <- select(q2b, \n              breed_rc, n)\n\nq2b <- distinct(q2b)\nq2b\n\n# A tibble: 327 × 2\n# Groups:   breed_rc [327]\n   breed_rc                     n\n   <chr>                    <int>\n 1 Unknown                  54586\n 2 Yorkshire Terrier        30379\n 3 Labrador (or Crossbreed) 28399\n 4 Shih Tzu                 27407\n 5 Pit Bull (or Mix)        24393\n 6 Chihuahua                21211\n 7 Maltese                  15701\n 8 Pomeranian                9287\n 9 Havanese                  8606\n10 Shih Tzu Crossbreed       8098\n# … with 317 more rows\n\n# Or with pipes!\nq2b <- nyc_dogs %>% \n  group_by(breed_rc) %>% \n  mutate(n = n()) %>% \n  arrange(-n) %>% \n  select(breed_rc, n) %>% \n  distinct()\n\n1 Unknown 54586 2 Yorkshire Terrier 30379 3 Labrador (or Crossbreed) 28399 4 Shih Tzu 27407 5 Pit Bull (or Mix) 24393 6 Chihuahua 21211"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q2c",
    "href": "DANL200_midterm-spring-2022-a.html#q2c",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2c",
    "text": "Q2c\nDescribe the relationship between the five popular breeds and borough using ggplot. Make a simple comment on your ggplot figure.\n\nq2c <- filter(nyc_dogs,\n              breed_rc %in% c(\"Yorkshire Terrier\", \n                              \"Labrador (or Crossbreed)\",\n                              \"Shih Tzu\", \n                              \"Pit Bull (or Mix)\",\n                              \"Chihuahua\"),\n              !is.na(borough)\n                         )\n\n# or | (or) operator. \n# IMHO, %in% (membership operator) is more convenient to use.\n\nq2c <- filter(nyc_dogs,\n              breed_rc == \"Yorkshire Terrier\" | \n              breed_rc == \"Labrador (or Crossbreed)\" | \n              breed_rc == \"Shih Tzu\" | \n              breed_rc == \"Pit Bull (or Mix)\" | \n              breed_rc == \"Chihuahua\",\n              !is.na(borough)\n)\n\n# 1. stacked bar charts\n# distribution of `breed_rc` by `borough` \nggplot(q2c) +\n  geom_bar(aes(x = breed_rc, fill = borough)) +\n  coord_flip()\n\n\n\n# distribution of `borough` by `breed_rc` \nggplot(q2c) +\n  geom_bar(aes(fill = breed_rc, x = borough)) +\n  coord_flip()\n\n\n\n# 2. stacked bar charts of relative frequencies\n# distribution of `breed_rc` by `borough` \nggplot(q2c) +\n  geom_bar(aes(x = breed_rc, fill = borough), \n           position = \"fill\") +\n  coord_flip()\n\n\n\n# distribution of `borough` by `breed_rc` \nggplot(q2c) +\n  geom_bar(aes(fill = breed_rc, x = borough), \n           position = \"fill\") +\n  coord_flip()\n\n\n\n# 3. side-by-side bar charts \n# distribution of `breed_rc` by `borough` \nggplot(q2c) +\n  geom_bar(aes(x = breed_rc, fill = borough), \n           position = \"dodge2\") +\n  coord_flip()\n\n\n\n# distribution of `borough` by `breed_rc` \nggplot(q2c) +\n  geom_bar(aes(fill = breed_rc, x = borough), \n           position = \"dodge2\") +\n  coord_flip()\n\n\n\n# 4. side-by-side bar charts of relative frequencies\n# distribution of `breed_rc` by `borough` \nggplot(q2c) +\n  geom_bar(aes(x = breed_rc, fill = borough, \n               y = ..prop.., group = borough), \n           position = \"dodge2\") +\n  coord_flip()\n\n\n\n# distribution of `borough` by `breed_rc` \nggplot(q2c) +\n  geom_bar(aes(x = borough, fill = breed_rc, \n               y = ..prop.., group = breed_rc), \n           position = \"dodge2\") +\n  coord_flip()\n\n\n\n\n\n# What if the filtered data could not be obtained ... \nggplot(data = nyc_dogs)+\n  geom_bar(mapping = aes(x=borough))+\n  facet_wrap(~breed_rc, ncol = 4) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  # optional"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q2d",
    "href": "DANL200_midterm-spring-2022-a.html#q2d",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2d",
    "text": "Q2d\nFind the five most popular breeds for each borough in NYC.\n\n# 1. \nq2d <- group_by(nyc_dogs,\n                breed_rc, borough)\nq2d <- summarize(q2d,\n                 n = n())\nq2d <- arrange(q2d,\n               borough, -n)\n\n# equivalently,\nq2d <- nyc_dogs %>% \n  group_by(breed_rc, borough) %>% \n  summarize(n = n()) %>% \n  arrange(borough, -n)\n\n\n# 2.\nq2d <- group_by(nyc_dogs,\n                breed_rc, borough)\nq2d <- mutate(q2d,\n              n = n())\nq2d <- arrange(q2d,\n               -n)\nq2d <- select(q2d,\n              breed_rc, n, borough)\nq2d <- distinct(q2d)\n\n# equivalently,\nq2d <- nyc_dogs %>% \n  group_by(breed_rc, borough) %>% \n  mutate(n = n()) %>% \n  arrange(-n) %>% \n  select(breed_rc, n, borough) %>% \n  distinct()\n\n\n# filter by borough\nq2d_bronx <- q2d %>% filter(borough == \"Bronx\")\nq2d_bronx\n\n# A tibble: 253 × 3\n# Groups:   breed_rc, borough [253]\n   breed_rc                     n borough\n   <chr>                    <int> <chr>  \n 1 Unknown                   5071 Bronx  \n 2 Yorkshire Terrier         4956 Bronx  \n 3 Pit Bull (or Mix)         4612 Bronx  \n 4 Shih Tzu                  4153 Bronx  \n 5 Chihuahua                 2934 Bronx  \n 6 Labrador (or Crossbreed)  1913 Bronx  \n 7 Maltese                   1889 Bronx  \n 8 Shih Tzu Crossbreed       1301 Bronx  \n 9 Pomeranian                1031 Bronx  \n10 Chihuahua Crossbreed       958 Bronx  \n# … with 243 more rows\n\nq2d_brooklyn <- q2d %>% filter(borough == \"Brooklyn\")\nq2d_brooklyn\n\n# A tibble: 305 × 3\n# Groups:   breed_rc, borough [305]\n   breed_rc                     n borough \n   <chr>                    <int> <chr>   \n 1 Unknown                  14007 Brooklyn\n 2 Yorkshire Terrier         7928 Brooklyn\n 3 Labrador (or Crossbreed)  7660 Brooklyn\n 4 Shih Tzu                  7463 Brooklyn\n 5 Pit Bull (or Mix)         7305 Brooklyn\n 6 Chihuahua                 5439 Brooklyn\n 7 Maltese                   3550 Brooklyn\n 8 Pomeranian                2440 Brooklyn\n 9 Shih Tzu Crossbreed       2129 Brooklyn\n10 Chihuahua Crossbreed      2108 Brooklyn\n# … with 295 more rows\n\nq2d_manhattan <- q2d %>% filter(borough == \"Manhattan\")\nq2d_manhattan\n\n# A tibble: 314 × 3\n# Groups:   breed_rc, borough [314]\n   breed_rc                     n borough  \n   <chr>                    <int> <chr>    \n 1 Unknown                  17421 Manhattan\n 2 Labrador (or Crossbreed) 10387 Manhattan\n 3 Yorkshire Terrier         7517 Manhattan\n 4 Chihuahua                 7118 Manhattan\n 5 Shih Tzu                  6482 Manhattan\n 6 Pit Bull (or Mix)         4919 Manhattan\n 7 Havanese                  4727 Manhattan\n 8 Maltese                   4269 Manhattan\n 9 French Bulldog            3565 Manhattan\n10 Golden Retriever          3280 Manhattan\n# … with 304 more rows\n\nq2d_queens <- q2d %>% filter(borough == \"Queens\")\nq2d_queens\n\n# A tibble: 291 × 3\n# Groups:   breed_rc, borough [291]\n   breed_rc                     n borough\n   <chr>                    <int> <chr>  \n 1 Unknown                  11807 Queens \n 2 Yorkshire Terrier         6713 Queens \n 3 Shih Tzu                  6039 Queens \n 4 Pit Bull (or Mix)         5108 Queens \n 5 Labrador (or Crossbreed)  4974 Queens \n 6 Maltese                   4297 Queens \n 7 Chihuahua                 4183 Queens \n 8 Pomeranian                2255 Queens \n 9 German Shepherd Dog       2111 Queens \n10 Beagle                    2080 Queens \n# … with 281 more rows\n\nq2d_staten_island <- q2d %>% filter(borough == \"Staten Island\")\nq2d_staten_island\n\n# A tibble: 248 × 3\n# Groups:   breed_rc, borough [248]\n   breed_rc                     n borough      \n   <chr>                    <int> <chr>        \n 1 Unknown                   5894 Staten Island\n 2 Labrador (or Crossbreed)  3202 Staten Island\n 3 Shih Tzu                  3058 Staten Island\n 4 Yorkshire Terrier         2987 Staten Island\n 5 Pit Bull (or Mix)         2289 Staten Island\n 6 Maltese                   1588 Staten Island\n 7 Chihuahua                 1375 Staten Island\n 8 German Shepherd Dog       1127 Staten Island\n 9 Beagle                     799 Staten Island\n10 Golden Retriever           739 Staten Island\n# … with 238 more rows\n\n# 3.\nq2d <- group_by(nyc_dogs,\n                breed_rc, borough)\n\nq2d <- mutate(q2d,\n              n = n())\n\nq2d <- group_by(q2d,\n                borough)\n\nq2d <- mutate(q2d,\n              rank = dense_rank(desc(n)) )\n\nq2d <- filter(q2d, rank <= 10 )\n\nq2d <- select(q2d,\n              breed_rc, n, rank, borough)\n\nq2d <- distinct(q2d)\n\nq2d <- arrange(q2d,\n               borough,-n)\n\n\n# equivalently\nq2d <- nyc_dogs %>% \n  group_by(breed_rc, borough) %>% \n  mutate(n = n()) %>% \n  group_by(borough) %>% \n  mutate(rank = dense_rank(desc(n)) ) %>% \n  filter(rank <= 10 ) %>% \n  select(breed_rc, n, rank, borough) %>% \n  distinct() %>% \n  arrange(borough,-n)"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q2e",
    "href": "DANL200_midterm-spring-2022-a.html#q2e",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2e",
    "text": "Q2e\nFind the five most popular dog names for each gender in NYC.\n\n# 1.\nq2e <- group_by(nyc_dogs,\n                animal_name, animal_gender)\nq2e <- summarise(q2e,\n                 n = n())\nq2e <- arrange(q2e,\n               animal_gender, -n)\n\n# equivalently\nq2e <- nyc_dogs %>% \n  group_by(animal_name, animal_gender) %>% \n  summarise(n = n()) %>% \n  arrange(animal_gender, -n)\n\n\n# 2. \nq2e <- group_by(nyc_dogs,\n                animal_name, animal_gender)\nq2e <- mutate(q2e,\n              n = n())\nq2e <- arrange(q2e,\n               -n)\nq2e <- select(q2e,\n              animal_name, animal_gender, n)\nq2e <- distinct(q2e)\n\n# equivalently\nq2e <- nyc_dogs %>% \n  group_by(animal_name, animal_gender) %>% \n  mutate(n = n()) %>% \n  arrange(-n) %>% \n  select(animal_name, animal_gender, n) %>% \n  distinct()\n\n\n\n# filter by animal_gender\nq2e_F <- q2e %>% filter(animal_gender == \"F\")\nq2e_F\n\n# A tibble: 14,602 × 3\n# Groups:   animal_name, animal_gender [14,602]\n   animal_name animal_gender     n\n   <chr>       <chr>         <int>\n 1 Bella       F              5493\n 2 Unknown     F              3884\n 3 Lola        F              3478\n 4 Luna        F              3204\n 5 Lucy        F              2994\n 6 Daisy       F              2677\n 7 Coco        F              2596\n 8 Princess    F              2222\n 9 Chloe       F              2041\n10 Molly       F              1954\n# … with 14,592 more rows\n\nq2e_M <- q2e %>% filter(animal_gender == \"M\")\nq2e_M\n\n# A tibble: 17,018 × 3\n# Groups:   animal_name, animal_gender [17,018]\n   animal_name       animal_gender     n\n   <chr>             <chr>         <int>\n 1 Unknown           M              5460\n 2 Max               M              4860\n 3 Charlie           M              3485\n 4 Rocky             M              3367\n 5 Name Not Provided M              3254\n 6 Buddy             M              2739\n 7 Teddy             M              2453\n 8 Lucky             M              2323\n 9 Toby              M              2140\n10 Milo              M              2096\n# … with 17,008 more rows"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q2f",
    "href": "DANL200_midterm-spring-2022-a.html#q2f",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2f",
    "text": "Q2f\nFind the five most popular dog names for each gender for each borough in NYC.\n\npopular_name <- nyc_dogs %>% \n  group_by(animal_name, animal_gender, borough) %>% \n  mutate(n = n()) %>% \n  arrange(borough, animal_gender, -n) %>% \n  select(animal_gender, animal_name, n) %>% \n  distinct()\n\n\n# 1.\nq2f <- group_by(nyc_dogs,\n                animal_name, animal_gender, borough)\nq2f <- summarise(q2f,\n                 n = n())\nq2f <- arrange(q2f,\n               animal_gender, -n)\n\n# equivalently,\nq2f <- nyc_dogs %>% \n  group_by(animal_name, animal_gender, borough) %>% \n  summarise(n = n()) %>% \n  arrange(animal_gender, -n)\n\n# 2. \nq2f <- group_by(nyc_dogs,\n                animal_name, animal_gender, borough)\nq2f <- mutate(q2f,\n              n = n())\nq2f <- arrange(q2f,\n               -n)\nq2f <- select(q2f,\n              animal_name, animal_gender, borough, n)\nq2f <- distinct(q2f)\n\n# equivalently\nq2f <- nyc_dogs %>% \n  group_by(animal_name, animal_gender, borough) %>% \n  mutate(n = n()) %>% \n  arrange(-n) %>% \n  select(animal_name, animal_gender, borough, n) %>% \n  distinct()\n\n\n# filter by animal_gender\nq2f_F <- q2f %>% filter(animal_gender == \"F\")\nq2f_M <- q2f %>% filter(animal_gender == \"M\")\n\n\n# filter `q2f_F` by borough\nq2f_F_bronx <- q2f_F %>% filter(borough == \"Bronx\")\nq2f_F_bronx\n\n# A tibble: 3,161 × 4\n# Groups:   animal_name, animal_gender, borough [3,161]\n   animal_name       animal_gender borough     n\n   <chr>             <chr>         <chr>   <int>\n 1 Bella             F             Bronx     774\n 2 Princess          F             Bronx     490\n 3 Luna              F             Bronx     436\n 4 Lola              F             Bronx     375\n 5 Coco              F             Bronx     322\n 6 Mia               F             Bronx     295\n 7 Daisy             F             Bronx     248\n 8 Chloe             F             Bronx     228\n 9 Lady              F             Bronx     201\n10 Name Not Provided F             Bronx     179\n# … with 3,151 more rows\n\nq2f_F_brooklyn <- q2f_F %>% filter(borough == \"Brooklyn\")\nq2f_F_brooklyn\n\n# A tibble: 6,428 × 4\n# Groups:   animal_name, animal_gender, borough [6,428]\n   animal_name animal_gender borough      n\n   <chr>       <chr>         <chr>    <int>\n 1 Unknown     F             Brooklyn  1480\n 2 Bella       F             Brooklyn  1331\n 3 Lola        F             Brooklyn   836\n 4 Luna        F             Brooklyn   792\n 5 Lucy        F             Brooklyn   733\n 6 Daisy       F             Brooklyn   636\n 7 Name        F             Brooklyn   635\n 8 Princess    F             Brooklyn   567\n 9 Chloe       F             Brooklyn   551\n10 Coco        F             Brooklyn   531\n# … with 6,418 more rows\n\nq2f_F_manhattan <- q2f_F %>% filter(borough == \"Manhattan\")\nq2f_F_manhattan\n\n# A tibble: 7,207 × 4\n# Groups:   animal_name, animal_gender, borough [7,207]\n   animal_name animal_gender borough       n\n   <chr>       <chr>         <chr>     <int>\n 1 Unknown     F             Manhattan  1555\n 2 Lucy        F             Manhattan  1358\n 3 Lola        F             Manhattan  1317\n 4 Bella       F             Manhattan  1262\n 5 Luna        F             Manhattan  1025\n 6 Daisy       F             Manhattan   884\n 7 Coco        F             Manhattan   783\n 8 Stella      F             Manhattan   698\n 9 Penny       F             Manhattan   685\n10 Sophie      F             Manhattan   678\n# … with 7,197 more rows\n\nq2f_F_queens <- q2f_F %>% filter(borough == \"Queens\")\nq2f_F_queens\n\n# A tibble: 4,788 × 4\n# Groups:   animal_name, animal_gender, borough [4,788]\n   animal_name       animal_gender borough     n\n   <chr>             <chr>         <chr>   <int>\n 1 Bella             F             Queens   1338\n 2 Name Not Provided F             Queens    780\n 3 Luna              F             Queens    678\n 4 Coco              F             Queens    605\n 5 Lola              F             Queens    580\n 6 Daisy             F             Queens    569\n 7 Princess          F             Queens    549\n 8 Molly             F             Queens    465\n 9 Lucy              F             Queens    455\n10 Unknown           F             Queens    429\n# … with 4,778 more rows\n\nq2f_F_staten_island <- q2f_F %>% filter(borough == \"Staten Island\")\nq2f_F_staten_island\n\n# A tibble: 2,391 × 4\n# Groups:   animal_name, animal_gender, borough [2,391]\n   animal_name animal_gender borough           n\n   <chr>       <chr>         <chr>         <int>\n 1 Bella       F             Staten Island   742\n 2 Molly       F             Staten Island   352\n 3 Coco        F             Staten Island   328\n 4 Daisy       F             Staten Island   328\n 5 Lola        F             Staten Island   315\n 6 Lucy        F             Staten Island   247\n 7 Luna        F             Staten Island   238\n 8 Chloe       F             Staten Island   234\n 9 Mia         F             Staten Island   221\n10 Unknown     F             Staten Island   207\n# … with 2,381 more rows\n\n# filter `q2f_M` by borough\nq2f_M_bronx <- q2f_M %>% filter(borough == \"Bronx\")\nq2f_M_bronx\n\n# A tibble: 4,057 × 4\n# Groups:   animal_name, animal_gender, borough [4,057]\n   animal_name       animal_gender borough     n\n   <chr>             <chr>         <chr>   <int>\n 1 Max               M             Bronx     683\n 2 Rocky             M             Bronx     495\n 3 Lucky             M             Bronx     372\n 4 Toby              M             Bronx     316\n 5 Teddy             M             Bronx     312\n 6 Prince            M             Bronx     295\n 7 Name Not Provided M             Bronx     289\n 8 Charlie           M             Bronx     288\n 9 Buddy             M             Bronx     282\n10 Milo              M             Bronx     249\n# … with 4,047 more rows\n\nq2f_M_brooklyn <- q2f_M %>% filter(borough == \"Brooklyn\")\nq2f_M_brooklyn\n\n# A tibble: 7,499 × 4\n# Groups:   animal_name, animal_gender, borough [7,499]\n   animal_name       animal_gender borough      n\n   <chr>             <chr>         <chr>    <int>\n 1 Unknown           M             Brooklyn  1937\n 2 Max               M             Brooklyn  1179\n 3 Rocky             M             Brooklyn   881\n 4 Name              M             Brooklyn   859\n 5 Charlie           M             Brooklyn   835\n 6 Name Not Provided M             Brooklyn   668\n 7 Lucky             M             Brooklyn   660\n 8 Buddy             M             Brooklyn   627\n 9 Teddy             M             Brooklyn   533\n10 Milo              M             Brooklyn   533\n# … with 7,489 more rows\n\nq2f_M_manhattan <- q2f_M %>% filter(borough == \"Manhattan\")\nq2f_M_manhattan\n\n# A tibble: 8,103 × 4\n# Groups:   animal_name, animal_gender, borough [8,103]\n   animal_name animal_gender borough       n\n   <chr>       <chr>         <chr>     <int>\n 1 Unknown     M             Manhattan  1888\n 2 Charlie     M             Manhattan  1290\n 3 Max         M             Manhattan  1186\n 4 Oliver      M             Manhattan   836\n 5 Teddy       M             Manhattan   798\n 6 Cooper      M             Manhattan   768\n 7 Buddy       M             Manhattan   729\n 8 Leo         M             Manhattan   665\n 9 Rocky       M             Manhattan   650\n10 Jack        M             Manhattan   629\n# … with 8,093 more rows\n\nq2f_M_queens <- q2f_M %>% filter(borough == \"Queens\")\nq2f_M_queens\n\n# A tibble: 6,048 × 4\n# Groups:   animal_name, animal_gender, borough [6,048]\n   animal_name       animal_gender borough     n\n   <chr>             <chr>         <chr>   <int>\n 1 Name Not Provided M             Queens   1595\n 2 Max               M             Queens   1258\n 3 Unknown           M             Queens   1012\n 4 Rocky             M             Queens    875\n 5 Charlie           M             Queens    733\n 6 Buddy             M             Queens    668\n 7 Lucky             M             Queens    649\n 8 Toby              M             Queens    562\n 9 Milo              M             Queens    541\n10 Teddy             M             Queens    536\n# … with 6,038 more rows\n\nq2f_M_staten_island <- q2f_M %>% filter(borough == \"Staten Island\")\nq2f_M_staten_island\n\n# A tibble: 2,890 × 4\n# Groups:   animal_name, animal_gender, borough [2,890]\n   animal_name       animal_gender borough           n\n   <chr>             <chr>         <chr>         <int>\n 1 Max               M             Staten Island   523\n 2 Rocky             M             Staten Island   451\n 3 Buddy             M             Staten Island   421\n 4 Charlie           M             Staten Island   311\n 5 Unknown           M             Staten Island   262\n 6 Teddy             M             Staten Island   256\n 7 Name Not Provided M             Staten Island   230\n 8 Bailey            M             Staten Island   205\n 9 Cody              M             Staten Island   188\n10 Toby              M             Staten Island   185\n# … with 2,880 more rows\n\n# 3.\nq2f <- group_by(nyc_dogs,\n                animal_name, animal_gender, borough)\n\nq2f <- mutate(q2f,\n              n = n())\n\nq2f <- group_by(q2f,\n                animal_gender, borough)\n\nq2f <- mutate(q2f,\n              rank = dense_rank(desc(n)) )\n\nq2f <- filter(q2f, \n              rank <= 10 )\n\nq2f <- select(q2f,\n              animal_name, animal_gender, n, rank, borough)\n\nq2f <- distinct(q2f)\n\nq2f <- arrange(q2f,\n               borough, animal_gender, -n)\n\n# equivalently\nq2f <- nyc_dogs %>% \n  group_by(animal_name, animal_gender, borough) %>% \n  mutate(n = n()) %>% \n  group_by(animal_gender, borough) %>% \n  mutate(rank = dense_rank(desc(n)) ) %>% \n  filter(rank <= 10 ) %>% \n  select(animal_name, animal_gender, n, rank, borough) %>% \n  distinct() %>% \n  arrange(borough, animal_gender, -n)"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q2g",
    "href": "DANL200_midterm-spring-2022-a.html#q2g",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2g",
    "text": "Q2g\nAssume that all dogs in the nyc_dogs data frame are alive as of today.\nDescribe the distribution of age for each borough using ggplot. Make a simple comment on your ggplot.\n\nnyc_dogs_age <- nyc_dogs %>% \n  mutate( age = 2022 - animal_birth_year )\n\nggplot( nyc_dogs_age ) +\n  geom_density(aes(x = age)) + \n  facet_wrap(~borough)\n\n\n\nggplot( filter(nyc_dogs_age, \n               age <= 25, !is.na(borough)) ) +\n  geom_density(aes(x = age)) + \n  facet_wrap(~borough)\n\n\n\nggplot( filter(nyc_dogs_age, \n               age <= 25, !is.na(borough)) ) +\n  geom_histogram(aes(x = age), \n                 binwidth = 1) + \n  facet_wrap(~borough)\n\n\n\n\nAdd some comments here …."
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q3a.",
    "href": "DANL200_midterm-spring-2022-a.html#q3a.",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3a.",
    "text": "Q3a.\nCreate a variable, payroll, which is defined as:\n\\[\n\\begin{align}\n\\texttt{payroll} = \\texttt{regular\\_gross\\_paid} + \\texttt{total\\_ot\\_paid}\n\\end{align}\n\\]\nwhere regular_gross_paid and total_ot_paid are variables in the nyc_payroll data frame.\n\nq3a <-  nyc_payroll %>% \n  mutate(payroll = regular_gross_paid + total_ot_paid)"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q3b.",
    "href": "DANL200_midterm-spring-2022-a.html#q3b.",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3b.",
    "text": "Q3b.\nCalculate the mean of payroll by title_description.\n\nq3b <- group_by(q3a, \n                title_description)\nq3b <-  summarise(q3b, \n                  avg_payroll = mean(payroll, na.rm = T))\nq3b <- arrange(q3b, \n               -avg_payroll)\n\n# equivalently\nq3b <- q3a %>% \n  group_by(title_description) %>% \n  summarise(avg_payroll = mean(payroll, na.rm = T)) %>% \n  arrange(-avg_payroll)"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#q3c.",
    "href": "DANL200_midterm-spring-2022-a.html#q3c.",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3c.",
    "text": "Q3c.\nCalculate the mean of payroll by work_location_borough.\n\nq3c <- group_by(q3a, \n                work_location_borough)\nq3c <-  summarise(q3c, \n                  avg_payroll = mean(payroll, na.rm = T))\nq3c <- arrange(q3c, \n               -avg_payroll)\n\n# equivalently\nq3c <- q3a %>% \n  group_by(work_location_borough) %>% \n  summarise(avg_payroll = mean(payroll, na.rm = T)) %>% \n  arrange(-avg_payroll)"
  },
  {
    "objectID": "DANL200_midterm-spring-2022-a.html#variable-description-1",
    "href": "DANL200_midterm-spring-2022-a.html#variable-description-1",
    "title": "Spring 2022, DANL 200: Introduction to Data Analytics",
    "section": "Variable Description",
    "text": "Variable Description\n\nFiscal Year: Fiscal Year\nPayroll Number: Payroll Number\nAgency Name: The Payroll agency that the employee works for\nLast Name: Last name of employee\nFirst Name: First name of employee\nMid Init: Middle initial of employee\nAgency Start Date: Date which employee began working for their current agency Date & Time\nWork Location Borough: Borough of employee’s primary work location\nTitle Description: Civil service title description of the employee\nLeave Status as of June 30: Status of employee as of the close of the relevant fiscal year: Active, Ceased, or On Leave\nBase Salary: Base Salary assigned to the employee\nPay Basis: Lists whether the employee is paid on an hourly, per diem or annual basis\nRegular Hours: Number of regular hours employee worked in the fiscal year\nRegular Gross Paid: The amount paid to the employee for base salary during the fiscal year\nOT Hours: Overtime Hours worked by employee in the fiscal year\nTotal OT Paid: Total overtime pay paid to the employee in the fiscal year\nTotal Other Pay: Includes any compensation in addition to gross salary and overtime pay, ie Differentials, lump sums, uniform allowance, meal allowance, retroactive pay increases, settlement amounts, and bonus pay, if applicable."
  },
  {
    "objectID": "DANL210_hw1q.html",
    "href": "DANL210_hw1q.html",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files.\nImport all the Python libraries you need here.\n\n\nimport pandas as pd"
  },
  {
    "objectID": "DANL210_hw1q.html#q1a",
    "href": "DANL210_hw1q.html#q1a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q1a",
    "text": "Q1a\nCalculate the simple difference between the probability of survival when passengers are first-class and the probability of survival when they are not."
  },
  {
    "objectID": "DANL210_hw1q.html#q1b",
    "href": "DANL210_hw1q.html#q1b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q1b",
    "text": "Q1b\nHow much does the probability of survival increase for first-class passengers relative to those who are not first-class passengers?"
  },
  {
    "objectID": "DANL210_hw1q.html#q1c",
    "href": "DANL210_hw1q.html#q1c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q1c",
    "text": "Q1c\nConsider the probability of survival in titanic_2.csv.\n\ntitanic_2 = pd.read_csv(\"data/titanic_2.csv\")\n\nAfter stratifying on gender and age, what happens to the difference in the probabilities of survival between first-class passengers and non-first-class passengers.\nExplain in your own words what stratifying on gender and age did for this difference in probabilities of survival between first-class passengers and non-first-class passengers."
  },
  {
    "objectID": "DANL210_hw1q.html#q2a",
    "href": "DANL210_hw1q.html#q2a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2a",
    "text": "Q2a\nHow many players have been recorded?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2b",
    "href": "DANL210_hw1q.html#q2b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2b",
    "text": "Q2b\nA column points (P) is missing in the data. The number of points of a player is defined as the sum of his goals (G) and assists (A). Add the point column P to your DataFrame."
  },
  {
    "objectID": "DANL210_hw1q.html#q2c",
    "href": "DANL210_hw1q.html#q2c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2c",
    "text": "Q2c\nWho is the top scorer in terms of points?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2d",
    "href": "DANL210_hw1q.html#q2d",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2d",
    "text": "Q2d\nHow many Russian (non-goalie) players had some ice time in there 2016/2017 regular season? Hint: Nationality of a player can be found in “Nat”. Russians are indicated by “RUS”."
  },
  {
    "objectID": "DANL210_hw1q.html#q2e",
    "href": "DANL210_hw1q.html#q2e",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2e",
    "text": "Q2e\nWhat are their names?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2f",
    "href": "DANL210_hw1q.html#q2f",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2f",
    "text": "Q2f\nWho performed best among the Russian players in terms of points (P)?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2g",
    "href": "DANL210_hw1q.html#q2g",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2g",
    "text": "Q2g\nHow many points (P) did he have?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2h",
    "href": "DANL210_hw1q.html#q2h",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2h",
    "text": "Q2h\nHow well did he perform in the entire league? Put differently, what was his rank in terms of points?"
  },
  {
    "objectID": "DANL210_hw1q.html#q2i",
    "href": "DANL210_hw1q.html#q2i",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2i",
    "text": "Q2i\nFind the top ten scorers (in terms of points) and print them including their number of point and their respective team."
  },
  {
    "objectID": "DANL210_hw1q.html#q2j",
    "href": "DANL210_hw1q.html#q2j",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q2j",
    "text": "Q2j\nWhat are the three countries with the most players originating from?"
  },
  {
    "objectID": "DANL210_hw1q.html#q3a",
    "href": "DANL210_hw1q.html#q3a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q3a",
    "text": "Q3a\nFor each type of mine, calculate the total coal production for each pair of state-year."
  },
  {
    "objectID": "DANL210_hw1q.html#q3b",
    "href": "DANL210_hw1q.html#q3b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q3b",
    "text": "Q3b\nFind the top 5 coal-producing states for each year."
  },
  {
    "objectID": "DANL210_hw1q.html#q3c",
    "href": "DANL210_hw1q.html#q3c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 1",
    "section": "Q3c",
    "text": "Q3c\nVisualize the yearly trend of the total coal production from each type of mine."
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html",
    "href": "DANL210_midterm-spring-2023-q.html",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#q1a",
    "href": "DANL210_midterm-spring-2023-q.html#q1a",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q1a",
    "text": "Q1a\nProvide both pandas/seaborn code and a simple comment to describe the trend of population for each borough."
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#q1b",
    "href": "DANL210_midterm-spring-2023-q.html#q1b",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q1b",
    "text": "Q1b\nProvide both pandas/seaborn code and a simple comment to describe the trend of the proportion of population for each borough.\n\nNote: The proportion of population for each borough is each borough’s share of total population in NYC in each year."
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#load-dataframe-for-question-2",
    "href": "DANL210_midterm-spring-2023-q.html#load-dataframe-for-question-2",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Load DataFrame for Question 2",
    "text": "Load DataFrame for Question 2\n\nrestaurant = pd.read_csv('https://bcdanl.github.io/data/DOHMH_NYC_Restaurant_Inspection.csv')\n\n\n\n\n\n\n\n  \n    \n      \n      CAMIS\n      DBA\n      BORO\n      STREET\n      CUISINE DESCRIPTION\n      INSPECTION DATE\n      ACTION\n      VIOLATION CODE\n      VIOLATION DESCRIPTION\n      CRITICAL FLAG\n      SCORE\n      GRADE\n    \n  \n  \n    \n      0\n      30191841\n      dj reynolds pub and restaurant\n      Manhattan\n      WEST   57 STREET\n      Irish\n      01/04/2022\n      Violations were cited in the following area(s).\n      10F\n      Non-food contact surface improperly constructe...\n      Not Critical\n      12\n      A\n    \n    \n      1\n      40356018\n      riviera caterers\n      Brooklyn\n      STILLWELL AVENUE\n      American\n      02/01/2022\n      Violations were cited in the following area(s).\n      02G\n      Cold food item held above 41º F (smoked fish a...\n      Critical\n      7\n      A\n    \n    \n      2\n      40356483\n      wilken's fine food\n      Brooklyn\n      AVENUE U\n      Sandwiches\n      08/19/2022\n      Violations were cited in the following area(s).\n      10F\n      Non-food contact surface or equipment made of ...\n      Not Critical\n      2\n      A\n    \n    \n      3\n      40356731\n      taste the tropics ice cream\n      Brooklyn\n      NOSTRAND AVENUE\n      Frozen Desserts\n      01/17/2023\n      Violations were cited in the following area(s).\n      08A\n      Establishment is not free of harborage or cond...\n      Not Critical\n      9\n      A\n    \n    \n      4\n      40357217\n      wild asia\n      Bronx\n      SOUTHERN BOULEVARD\n      American\n      07/28/2021\n      Violations were cited in the following area(s).\n      02G\n      Cold food item held above 41º F (smoked fish a...\n      Critical\n      10\n      A\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      17628\n      50133218\n      sades southern cafe llc\n      Bronx\n      EAST  233 STREET\n      Caribbean\n      03/06/2023\n      Violations were cited in the following area(s).\n      06A\n      Personal cleanliness is inadequate. Outer garm...\n      Critical\n      12\n      A\n    \n    \n      17629\n      50133250\n      red brick\n      Brooklyn\n      NOSTRAND AVENUE\n      Caribbean\n      03/22/2023\n      Violations were cited in the following area(s).\n      08A\n      Establishment is not free of harborage or cond...\n      Not Critical\n      4\n      A\n    \n    \n      17630\n      50133302\n      potbelly\n      Manhattan\n      EAST   17 STREET\n      Sandwiches\n      03/20/2023\n      Violations were cited in the following area(s).\n      06D\n      Food contact surface not properly washed, rins...\n      Critical\n      7\n      A\n    \n    \n      17631\n      50133309\n      potbelly\n      Manhattan\n      BROADWAY\n      Soups/Salads/Sandwiches\n      03/27/2023\n      Violations were cited in the following area(s).\n      10F\n      Non-food contact surface or equipment made of ...\n      Not Critical\n      4\n      A\n    \n    \n      17632\n      50133690\n      island shack\n      Brooklyn\n      FRANKLIN AVENUE\n      Caribbean\n      03/23/2023\n      Violations were cited in the following area(s).\n      10F\n      Non-food contact surface or equipment made of ...\n      Not Critical\n      12\n      A\n    \n  \n\n17633 rows × 12 columns"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#variable-description",
    "href": "DANL210_midterm-spring-2023-q.html#variable-description",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Variable Description",
    "text": "Variable Description\n\nCAMIS:\n\nThis is an unique identifier for the entity (restaurant);\n10-digit integer\n\nDBA:\n\nThis field represents the name (doing business as) of the entity (restaurant);\nPublic business name, may change at discretion of restaurant owner\n\nBORO:\n\nBorough in which the entity (restaurant) is located.;\n• 1 = MANHATTAN\n• 2 = BRONX\n• 3 = BROOKLYN\n• 4 = QUEENS\n• 5 = STATEN ISLAND\n• 0 = Missing;\n\nCUISINE DESCRIPTION:\n\nThis field describes the entity (restaurant) cuisine.\n\nACTION:\n\nThis field represents the actions that is associated with each restaurant inspection. ;\n• Violations were cited in the following area(s).\n• No violations were recorded at the time of this inspection.\n• Establishment re-opened by DOHMH\n• Establishment re-closed by DOHMH\n• Establishment Closed by DOHMH.\n• Violations were cited in the following area(s) and those requiring immediate action were addressed.\n\nVIOLATION CODE:\n\nViolation code associated with an establishment (restaurant) inspection\n\nVIOLATION DESCRIPTION:\n\nViolation description associated with an establishment (restaurant) inspection\n\nCRITICAL FLAG:\n\nIndicator of critical violation;\n• Critical\n• Not Critical\n• Not Applicable;\nCritical violations are those most likely to contribute to food-borne illness\n\nSCORE:\n\nTotal score for a particular inspection;\n\nGRADE:\n\nGrade associated with the inspection;\n• N = Not Yet Graded\n• A = Grade A\n• B = Grade B\n• C = Grade C\n• Z = Grade Pending\n• P = Grade Pending issued on re-opening following an initial inspection that resulted in a closure"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#q2a.",
    "href": "DANL210_midterm-spring-2023-q.html#q2a.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2a.",
    "text": "Q2a.\nWhat are the mean, standard deviation, first quartile, median, third quartile, and maximum of SCORE for each GRADE of restaurants?"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#q2b.",
    "href": "DANL210_midterm-spring-2023-q.html#q2b.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2b.",
    "text": "Q2b.\nProvide both (1) pandas/seaborn code and (2) a simple comment to describe how the distribution of SCORE varies by CRITICAL FLAG when GRADE is A."
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#q2c.",
    "href": "DANL210_midterm-spring-2023-q.html#q2c.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2c.",
    "text": "Q2c.\n\nFor each pair of BORO and GRADE, calculate\n\n\nthe proportion of Critical violation and\n\n\nthe proportion of Not Critical violation.\n\n\nMake a simple comment on how the proportions vary by BORO and GRADE."
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#q2d.",
    "href": "DANL210_midterm-spring-2023-q.html#q2d.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2d.",
    "text": "Q2d.\nFor the 10 most common CUISINE DESCRIPTION values, find the CUISINE DESCRIPTION value that has the highest proportion of GRADE A."
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#q2e.",
    "href": "DANL210_midterm-spring-2023-q.html#q2e.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2e.",
    "text": "Q2e.\n\nFind the 3 most common names of restaurants (DBA) in each BORO.\n\nIf the third most common DBA values are multiple, please include all the DBA values.\n\nOverall, which DBA value is most common in NYC?"
  },
  {
    "objectID": "DANL210_midterm-spring-2023-q.html#q2f.",
    "href": "DANL210_midterm-spring-2023-q.html#q2f.",
    "title": "Spring 2023, DANL 210: Data Preparation and Management",
    "section": "Q2f.",
    "text": "Q2f.\nFor all the DBA values that appear in the result of Q2f, find the DBA value that is most likely to commit critical violation."
  },
  {
    "objectID": "DANL210_lab3a.html",
    "href": "DANL210_lab3a.html",
    "title": "Python Lab 3 - Tidy Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab3a.html#load-dataframe",
    "href": "DANL210_lab3a.html#load-dataframe",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbillboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')\nny_pincp = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_wide.csv')\ncovid = pd.read_csv('https://bcdanl.github.io/data/covid19_cases.csv')"
  },
  {
    "objectID": "DANL210_lab3a.html#q1a",
    "href": "DANL210_lab3a.html#q1a",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1a",
    "text": "Q1a\n\nDescribe how the distribution of rating varies across week 1, week 2, and week 3 using the faceted histogram.\n\n\nbillboard_long = billboard.melt(\n  id_vars = [\"year\", \"artist\", \"track\", \"time\", \"date.entered\"],\n  var_name = \"week\",\n  value_name = \"rating\",\n)\n\n\n# The .isin() method is used to check whether each value in the \"week\" column is present in the given list of values. \nbillboard_wk1_2_3 = billboard_long.loc[\n    billboard_long['week'].isin(['wk1', 'wk2', 'wk3'])\n    ]\n\nsns.displot(billboard_wk1_2_3,\n             x = 'rating',\n             row = 'week'\n             )\n\n<seaborn.axisgrid.FacetGrid at 0x7faf5704a040>"
  },
  {
    "objectID": "DANL210_lab3a.html#q1b",
    "href": "DANL210_lab3a.html#q1b",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1b",
    "text": "Q1b\n\nWhich artist(s) have the most number of tracks in billboard DataFrame?\n\n\nbillboard_songs = (\n    billboard[[\"artist\", \"track\"]]\n    .drop_duplicates()        # drops duplicate observations.\n    .drop(\"track\", axis = 1)  # drops variable, track.\n    .value_counts()\n    )"
  },
  {
    "objectID": "DANL210_lab3a.html#q1c",
    "href": "DANL210_lab3a.html#q1c",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1c",
    "text": "Q1c\n\nMake ny_pincp longer.\n\n\nny_pincp_long = ny_pincp.melt(\n        id_vars = ['fips', 'geoname'],\n        var_name = 'year',\n        value_name = 'pincp'\n        )"
  },
  {
    "objectID": "DANL210_lab3a.html#q1d",
    "href": "DANL210_lab3a.html#q1d",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1d",
    "text": "Q1d\n\nMake a wide-form DataFrame of covid whose variable names are from countriesAndTerritories and values are from cases.\n\n\ncovid_wide = (\n    covid\n    .pivot_table(index = 'date', \n           columns = 'countriesAndTerritories', \n           values = 'cases')\n    )"
  },
  {
    "objectID": "DANL210_lab3a.html#q1e",
    "href": "DANL210_lab3a.html#q1e",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q1e",
    "text": "Q1e\n\nUse the wide-form DataFrame of covid to find the top 10 countries for which their cases are highly correlated with USA’s cases using DataFrame.corr()\n\n\ncorr_usa = (\n    covid_wide.corr()\n    .sort_values(by = 'USA', ascending = False)\n    .USA\n    )"
  },
  {
    "objectID": "DANL210_lab3a.html#load-dataframe-for-q2a-and-q2b",
    "href": "DANL210_lab3a.html#load-dataframe-for-q2a-and-q2b",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Load DataFrame for Q2a and Q2b",
    "text": "Load DataFrame for Q2a and Q2b\n\npaidsearch = pd.read_csv('https://bcdanl.github.io/data/paidsearch.csv')\n\n\nVariable description\n\ndma: an identification number of a designated market (DM) area i (e.g., Boston, Los Angeles)\ntreatment_period: 0 if date is before May 22, 2012 and 1 after.\nsearch_stays_on: 1 if the paid-search goes off in dma i, 0 otherwise.\nrevenue: eBay’s sales revenue for dma i and date t"
  },
  {
    "objectID": "DANL210_lab3a.html#q2a",
    "href": "DANL210_lab3a.html#q2a",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2a",
    "text": "Q2a\nSummarize the mean vale of revenue for each group of search_stays_on and for each date.\n\nq2a = (\n    paidsearch                                              # Use paidsearch dataframe\n    .groupby(['search_stays_on', 'date'])                    # Group by 'search_stays_on' and 'date' columns\n    .agg( {'revenue': 'mean'} )                        # Compute mean revenue for each group\n    .reset_index()                                          # Reset index to convert to DataFrame\n)"
  },
  {
    "objectID": "DANL210_lab3a.html#q2b",
    "href": "DANL210_lab3a.html#q2b",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2b",
    "text": "Q2b\nCalculate the log difference between mean revenues in each group of search_stays_on. (This is the log of the average revenue in group of search_stays_on == 1 minus the log of the average revenue in group of search_stays_on == 0.)\n\nFor example, consider the following two observations:\n\n\n# date        the daily mean vale of `revenue`   search_stays_on\n# 1-Apr-12    93650.68                           0\n# 1-Apr-12    120277.57                          1\n\n\nThe log difference of daily mean revenues between the two group of search_stays_on for date 1-Apr-12 is log(120277.57) - log(93650.68).\n\n\n# Compute mean revenue for each group of 'search_stays_on' and 'date' in Q1b DataFrame\nq2b = (\n    paidsearch\n    .groupby(['search_stays_on', 'date'])                             # Group by 'search_stays_on' and 'date' columns\n    .agg( {'revenue' : 'mean'} )                                 # Compute mean revenue for each group\n    .reset_index()\n    .sort_values(by=['date', 'search_stays_on'])                       # Sort by 'date' and 'search_stays_on' columns\n    .pivot(index='date', columns='search_stays_on', values='revenue')  # Pivot 'search_stays_on' column to create wide format\n)\n\n# rename columns of q1b\nq2b.columns = ['rev_control', 'rev_treat']\n\nq2b = q2b.assign(diff_log = \n                 np.log(q2b['rev_control']) - \n                 np.log(q2b['rev_treat']) ) # Compute 'diff_log' column"
  },
  {
    "objectID": "DANL210_lab3a.html#load-dataframe-for-q2c-q2d-and-q2e",
    "href": "DANL210_lab3a.html#load-dataframe-for-q2c-q2d-and-q2e",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Load DataFrame for Q2c, Q2d, and Q2e",
    "text": "Load DataFrame for Q2c, Q2d, and Q2e\n\npaid_search = pd.read_csv('https://bcdanl.github.io/data/paid_search.csv')"
  },
  {
    "objectID": "DANL210_lab3a.html#q2c",
    "href": "DANL210_lab3a.html#q2c",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2C",
    "text": "Q2C\nSort paid_search by DM and May22_2012 in ascending order.\n\npaid_search = paid_search.sort_values(\n    by = ['DM', 'May22_2012']\n    )"
  },
  {
    "objectID": "DANL210_lab3a.html#q2d",
    "href": "DANL210_lab3a.html#q2d",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2d",
    "text": "Q2d\nFor each DM, calculate the difference between log_revenue before and after May22_2012.\n\nq2d = (\n     paid_search\n    .set_index(['DM', 'May22_2012', 'no_paid_search'])\n    .assign(after = lambda x: x.groupby('DM')['log_revenue'].transform('last'),\n            before = lambda x: x.groupby('DM')['log_revenue'].transform('first'))\n    .reset_index()\n    .drop(columns=['log_revenue', 'May22_2012'])\n    .drop_duplicates()\n    .assign(diff = lambda x: x['after'] - x['before'])\n)"
  },
  {
    "objectID": "DANL210_lab3a.html#q2e",
    "href": "DANL210_lab3a.html#q2e",
    "title": "Python Lab 3 - Tidy Data",
    "section": "Q2e",
    "text": "Q2e\n\nConsider the DataFrame from Q2d.\nFor each group of no_paid_search, calculate the mean value of the difference between log_revenue before and after May22_2012 .\nWhat is the difference in the mean values?\n\n\nq2e = (\n     q2d\n    .drop(columns=['DM', 'after', 'before'])\n    .groupby('no_paid_search')\n    .mean()\n)\n\nq2e['diff'].iloc[1] - q2e['diff'].iloc[0]  # treatment effect of turning off search-engine marketing\n\n-0.006586851608119372\n\n\n\nAfter the paid-search went off, sales revenue decreased by 0.66%\n\nWas eBay’s paid search worth it?"
  },
  {
    "objectID": "DANL210_lab5q.html",
    "href": "DANL210_lab5q.html",
    "title": "Python Lab 5 - Dates and Times",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab5q.html#q1a",
    "href": "DANL210_lab5q.html#q1a",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q1a",
    "text": "Q1a\n\nAdd the new variables, closing_quarter and closing_year, to the DataFrame banks.\n\nclosing_quarter: the quarter in which the bank closed (1, 2, 3, or 4)\nclosing_year: the year in which the bank closed"
  },
  {
    "objectID": "DANL210_lab5q.html#q1b",
    "href": "DANL210_lab5q.html#q1b",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q1b",
    "text": "Q1b\nCount the number of banks that were closed for each pair of year-quarter."
  },
  {
    "objectID": "DANL210_lab5q.html#q1c",
    "href": "DANL210_lab5q.html#q1c",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q1c",
    "text": "Q1c\nProvide both visualization code and a simple comment to describe the quarterly trend of bank failure."
  },
  {
    "objectID": "DANL210_lab5q.html#q2a",
    "href": "DANL210_lab5q.html#q2a",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q2a",
    "text": "Q2a\nAdd a variable, date_dt, which is a datetime type of Date variable, to the stock DataFrame."
  },
  {
    "objectID": "DANL210_lab5q.html#q2b",
    "href": "DANL210_lab5q.html#q2b",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q2b",
    "text": "Q2b\n\nFor each year, find the two dates for which\n\nTSLA’s Close was the highest of the year.\nTSLA’s Close was the lowest of the year."
  },
  {
    "objectID": "DANL210_lab5q.html#q2c",
    "href": "DANL210_lab5q.html#q2c",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q2c",
    "text": "Q2c\n\nCalculate the gap between the two adjacent dates with the highest Close of the year.\nCalculate the gap between the two adjacent dates with the lowest Close of the year."
  },
  {
    "objectID": "DANL210_lab8a.html",
    "href": "DANL210_lab8a.html",
    "title": "Python Lab 8 - Web-scrapping 3 Example Answers",
    "section": "",
    "text": "Go to the following website for web scrapping\n\nhttp://quotes.toscrape.com\n\n\n\n\n\n\nProvide your Python Selenium code to scrape all the quotes.\n\nYou should create the two DataFrames with the following variables:\n\n\nDataFrame about each quote\n\n\nquote\nauthor\ntags\nabout, URL for description about each author.\n\n\nDataFrame about each author\n\n\nabout, URL for description about each author.\nborn_date\nborn_location\nauthor_description\nSave the two DataFrames in the CSV files.\n\n\n\n# %%\n# working directory\nimport os\nwd_path = '/Users/byeong-hakchoe/Google Drive/suny-geneseo/spring2023/lecture_codes/'\nos.chdir(wd_path)  \nos.getcwd()\n\n\n# %%\nimport pandas as pd\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\n\noptions = Options()\noptions.add_argument(\"window-size=1400,1200\")\ndriver = webdriver.Chrome(chrome_options=options, \n                          executable_path = \"chromedriver\")\n# driver.quit()\nurl = \"http://quotes.toscrape.com/\"\ndriver.get(url)\n\n\n# %%\ndf = pd.DataFrame()\nwhile True:\n    quotes = driver.find_elements(By.CLASS_NAME,'text')\n    authors = driver.find_elements(By.CLASS_NAME,'author')\n    tags = driver.find_elements(By.CLASS_NAME,'tags')\n    abouts = driver.find_elements(By.LINK_TEXT,'(about)')\n    next_btn = driver.find_elements(By.PARTIAL_LINK_TEXT, 'Next')\n    for j in range(0, len(quotes)):\n        quote = pd.DataFrame( [quotes[j].text] ) \n        author = pd.DataFrame( [authors[j].text] ) \n        tag = pd.DataFrame( [tags[j].text] )\n        about = pd.DataFrame( [abouts[j].get_attribute('href')] )\n        data = pd.concat( [quote, author, tag, about], axis=1 )\n        df = df.append(data)\n    if next_btn != []:\n        next_btn[0].click()\n    else:\n        break\n\ndf.columns = ['quote', 'author', 'tag', 'about']\ndf.to_csv(\"data/webscrapping_quotes.csv\")    \n\n\n# %%\n\n# drop duplicate authors\nauthor_url = df[['about']]\nauthor_url = author_url.drop_duplicates()\n\n\n# changing the index of author_url for the for-loop below\ns = pd.Series(range(0,len(author_url)))\nauthor_url = author_url.set_index([s])  \n\nauthor_info = pd.DataFrame()\nfor i in range(0, len(author_url)):\n    driver.get(author_url.at[i, 'about'])\n    born_date = driver.find_elements(By.CLASS_NAME,'author-born-date')\n    born_location = driver.find_elements(By.CLASS_NAME,'author-born-location')\n    author_description = driver.find_elements(By.CLASS_NAME,'author-description')\n    born_date = pd.DataFrame( [born_date[0].text] ) \n    born_location = pd.DataFrame( [born_location[0].text] ) \n    author_description = pd.DataFrame( [author_description[0].text] )\n    about = pd.DataFrame( [author_url.at[i, 'about']] )\n    data = pd.concat( [about, born_date, born_location, author_description], axis=1 )\n    author_info = author_info.append(data)\n    \nauthor_info.columns = ['about', 'born_date', 'born_location', 'author_description']\nauthor_info.to_csv(\"data/webscrapping_quotes_author_info.csv\")       \n\n\ndriver.quit()"
  },
  {
    "objectID": "DANL310_hw2a.html",
    "href": "DANL310_hw2a.html",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2 - Example Answers -",
    "section": "",
    "text": "Add a web-page of the team project proposal to your website. The tab menu to link the web-page must be provided."
  },
  {
    "objectID": "DANL310_hw2a.html#q2a.",
    "href": "DANL310_hw2a.html#q2a.",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2 - Example Answers -",
    "section": "Q2a.",
    "text": "Q2a.\nUse the following data.frame for Q2a, Q2b, and Q2c.\n\nhdi_corruption <- read_csv(\n  'https://bcdanl.github.io/data/hdi_corruption.csv')\n\n\n# Use geom_smooth(method = lm, formula = \"y ~ log(x)\", se = F) .\n# `box.padding` option would be useful for geom_text_repel().\n\n\ncountry_highlight <- c(\"Germany\", \"Norway\", \"United States\", \n                       \"Greece\", \"Singapore\", \n                       \"Argentina\", \"Senegal\",\n                       \"China\", \"Egypt\", \"South Africa\")\n\ncorruption <- hdi_corruption %>% \n  mutate(label = ifelse(country %in% country_highlight, country, NA))\n\n\nggplot(data = filter(corruption, year == 2014), aes(cpi, hdi)) + \n  geom_smooth(method = lm, formula = \"y ~ log(x)\", se = F) + \n  geom_point(\n    aes(color = region, fill = region),\n    size = 2.5, alpha = 0.5, shape = 21\n  ) + \n  geom_text_repel(\n    aes(label = label), color = \"black\", size = 4,\n    box.padding = .75\n  ) +\n  scale_y_continuous(\n    limits = c(0.3, 1.05), breaks = c(0.2, 0.4, 0.6, 0.8, 1.0),\n    name = \"Human Development Index, 2014\\n(1.0 = most developed)\"\n  ) +\n  scale_x_continuous(\n    limits = c(10, 95),\n    breaks = c(20, 40, 60, 80, 100),\n    name = \"Corruption Perceptions Index, 2014 (100 = least corrupt)\"\n  ) + \n  theme_minimal() + \n  theme(\n    plot.margin = unit( c(1.75, .75, .75, .5), \"cm\"),\n    legend.position = c(.5, 1.05),\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = 10)\n  ) +\n  labs( color = \"Region\", fill = \"Region\")"
  },
  {
    "objectID": "DANL310_hw2a.html#q2b",
    "href": "DANL310_hw2a.html#q2b",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2 - Example Answers -",
    "section": "Q2b",
    "text": "Q2b\n\nDownload the file labor_supply.zip from the Data folder in the Files section in Canvas. Then, extract labor_supply.zip, so that you can access the labor_supply.csv file.\nVariable description in labor_supply.csv\n\nSEX: 1 if Male; 2 if Female; 9 if NIU (Not in universe)\nNCHLT5: Number of own children under age 5 in a household; 9 if 9+\nLABFORCE: 0 if NIU or members of the armed forces; 1 if not in the labor force; 2 if in the labor force.\nASECWT: sample weight\n\nA sample weight of each observation means how much population each observation represents.\n\nIf you sum ASECWT for each year, you get the size of yearly population in the US.\n\nHouseholds with LABFORCE == 0 is not in labor force.\nLabor force participation rate can be calculated by:\n\n\\[\n(\\text{Labor Force Participation Rate}) \\, = \\, \\frac{(\\text{Size of population in labor force})}{(\\text{Size of civilian population that are not members of the armed force})}\n\\]\n\npath <- '/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/labor_supply.csv'\n\ncps_labor <- read_csv(path)\n\ncps_labor <- cps_labor %>% \n  filter(YEAR >= 1982) %>% \n  filter(LABFORCE != 0) %>% \n  mutate(LABFORCE = LABFORCE - 1) %>% \n  mutate(labor_supply = LABFORCE * ASECWT,\n         child = ifelse(NCHLT5 == 0, \n                        \"No Child Under Age 5 in Household\", \n                        \"Having Children Under Age 5 in Household\")) %>% \n  group_by(YEAR, SEX, child) %>% \n  summarize(pct = sum(labor_supply) / sum(ASECWT, na.rm= T) ) %>% \n  filter(!is.na(child))\n  \nlabel_df <- filter(cps_labor, YEAR == 2022) %>% \n  mutate(label = ifelse(SEX == 1, \"Male\", \"Female\"))\n\nggplot(data = cps_labor,\n       aes(x = YEAR, y = pct, \n           color = factor(SEX))) +\n  geom_line(size = 1.5) +\n  geom_label_repel(data = label_df,\n                  aes(x = YEAR, y = pct, label = label),\n                  color = 'black',\n                  nudge_y = .033, box.padding = .5) +\n  facet_grid( . ~ factor(child)) +\n  scale_x_continuous( breaks = seq(1982, 2022, 4) ) +\n  scale_y_continuous( labels = scales::percent) +  \n  scale_color_manual( labels = c(\"Male\", \"Female\"),\n                      values = c(\"#2E74C0\", \"#CB454A\") ) +\n  labs(x = \"Year\",\n       y = \"Labor Force Participation Rate\",\n       color = \"Gender\",\n       lty = \"Young Children\",\n       title = \"Fertility and Labor Supply in the U.S.\",\n       subtitle = \"1982-2022\",\n       caption = \"Data: IPUMS-CPS, University of Minnesota, www.ipums.org.\") +  guides(color = \"none\") +\n  theme_ipsum() +\n  theme(axis.title.y = element_text(size = rel(1.5),\n                                    face = 'bold'),\n        axis.text.x = element_text(angle = 45))"
  },
  {
    "objectID": "DANL310_hw2a.html#q2c",
    "href": "DANL310_hw2a.html#q2c",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2 - Example Answers -",
    "section": "Q2c",
    "text": "Q2c\n\nlibrary(ggcorrplot) # to create correlation heatmaps using ggcorrplot()\nbeer_mkt <- read_csv('https://bcdanl.github.io/data/beer_markets.csv')\n\n\nMake a correlation heat-map with variables that are either strongly correlated or promo-related.\nThe variables are selected by how high the mean value of the absolute value of correlations with the variable is (top 13-15 variables).\nYou can start with the following data.frame:\n\n\nbeer_dummies <- beer_mkt %>% select(-hh, -market) \nreg <- lm(data = beer_dummies,\n          beer_floz ~ .)\nbeer_dummies <-  as.data.frame(model.matrix(reg))[, -1]\nbeer_dummies <- cbind(beer_mkt$beer_floz ,beer_dummies)\nbeer_dummies <- beer_dummies %>% \n  rename(beer_floz = `beer_mkt$beer_floz`)\n\n\nTo calculate a correlation between numeric variables in data.frame, use cor(data.frame)\n\n\nbeer_dummies <- beer_dummies %>% \n  mutate(`brandBUD LIGHT` = \n           ifelse(`brandBUSCH LIGHT` == 1  |\n                     `brandCOORS LIGHT` == 1 |\n                     `brandMILLER LITE` == 1 |\n                     `brandNATURAL LIGHT` == 1, 0, 1)) %>% \n  select(dollar_spent, ends_with(\"floz\"), \n         quantity, `brandBUD LIGHT`, \n         starts_with(\"brand\"), everything())\n\n# the correlation matrix\ncor_beer <- cor(beer_dummies)\n\n# to select variables with high correlations\ncor_beer_abs <- abs(cor_beer)\ncor_beer_abs <- cor_beer_abs %>% as_tibble()\ncor_beer_mean <- cor_beer_abs %>% \n  summarise_if(is.numeric, mean)\n\ncor_beer_mean <- t(cor_beer_mean) # transpose\ncor_beer_mean <- cbind(cor_beer_mean, rownames(cor_beer_mean))\ncor_beer_mean <- cor_beer_mean %>% \n  as_tibble() \n\ncor_beer_mean <- cor_beer_mean %>% \n  mutate(V1 = as.numeric(V1)) %>% \n  arrange(-V1)\n\ncor_beer_mean <- cor_beer_mean %>% \n  rename(cor_mean = V1,\n         var_name = V2)\n\ncor_beer_mean <- cor_beer_mean %>% \n  mutate(ranking = dense_rank(-cor_mean))\n\ncor_beer_mean_selected <- cor_beer_mean %>% \n  filter(ranking <= 15 | str_detect(var_name, \"promo\") )\n\ndf_cor_beer <- as.data.frame(cor_beer)\ndf_cor_beer <- df_cor_beer %>% \n  mutate(var = rownames(df_cor_beer))\n\ncor_beer_selected <- df_cor_beer %>% \n  filter(var %in% cor_beer_mean_selected$var_name ) %>% \n  select(-var)\n\n\ncor_beer_selected2 <- t(cor_beer_selected)\n\ncor_beer_selected2 <- as.data.frame(cor_beer_selected2) %>% \n  mutate(var = rownames(cor_beer_selected2)) \n\ncor_beer_selected2 <- cor_beer_selected2 %>% \n  filter(var %in% cor_beer_mean_selected$var_name ) %>% \n  select(-var)\n\n# p values for correlation tests\n# p.mat <- cor_pmat(cor_beer_selected2) \n\n# Tentatively trying to make correlation heatmaps \n# ggcorrplot( cor_beer_selected2, lab = T,\n#             type = 'lower',\n#             colors = c(\"#2E74C0\", \"white\", \"#CB454A\"),\n#             ) \n\n# colnames(cor_beer_selected2)\n\n\n# finalizing variables\ncor_beer_selected3 <- cor_beer_selected2 %>% \n  select(-starts_with(\"cow\"))\n\ncor_beer_selected3 <- t(cor_beer_selected3) # transpose\n\ncor_beer_selected3 <- as.data.frame(cor_beer_selected3) %>% \n  select(-starts_with(\"cow\"))\n\n\n# rownames and colnames\nvar_list <- colnames(cor_beer_selected3)\nvarlist <- rownames(cor_beer_selected3) \nvarlist <- str_replace_all(varlist,\n                           \"brand\", \"\")\nvarlist <- str_replace_all(varlist,\n                           \"container\", \"\")\n\nrownames(cor_beer_selected3) <- varlist\nvarlist <- colnames(cor_beer_selected3) \nvarlist <- str_replace_all(varlist,\n                           \"brand\", \"\")\nvarlist <- str_replace_all(varlist,\n                           \"container\", \"\")\ncolnames(cor_beer_selected3) <- varlist\n\n# correlation heatmap with correlation values\np <- ggcorrplot( cor_beer_selected3, lab = T,\n            colors = c(\"#2E74C0\", \"white\", \"#CB454A\"),\n) +\n  theme(axis.text = element_text(size = rel(1.5),\n                                 face = 'bold'))  + \n  guides(fill = guide_colourbar(barheight = 38.5))\n\np\n\n# for NY ------------------------------------------------------------------\n\nbeer_dummies <- beer_mkt  %>% \n  filter(str_detect(market, \"NY\") | \n           str_detect(market, \"NEW YORK\") |\n           str_detect(market, \"ALBANY\") |\n           str_detect(market, \"BUFFALO-ROCHESTER\") |\n           str_detect(market, \"SYRACUSE\") \n           ) %>% \n  select(-hh, -market)\n\n\nreg <- lm(data = beer_dummies,\n          beer_floz ~ .)\n\nbeer_mkt_NY <- beer_mkt %>% \n  filter(str_detect(market, \"NY\") | \n           str_detect(market, \"NEW YORK\") |\n           str_detect(market, \"ALBANY\") |\n           str_detect(market, \"BUFFALO-ROCHESTER\") |\n           str_detect(market, \"SYRACUSE\") \n  ) %>% \n  select(beer_floz)\n\nbeer_dummies_tmp <-  as.data.frame(model.matrix(reg))[, -1]\n\nbeer_dummies <- cbind(beer_mkt_NY,\n                      beer_dummies_tmp) \n\nbeer_dummies <- beer_dummies %>% \n  mutate(`brandBUD LIGHT` = \n           ifelse(`brandBUSCH LIGHT` == 1  |\n                    `brandCOORS LIGHT` == 1 |\n                    `brandMILLER LITE` == 1 |\n                    `brandNATURAL LIGHT` == 1, 0, 1)) %>% \n  select(dollar_spent, ends_with(\"floz\"), \n         quantity, `brandBUD LIGHT`, \n         starts_with(\"brand\"), everything()) \n\n# the matrix of the correlation test p-values\ncor_beer <- cor(beer_dummies)\n\ncor_beer_abs <- abs(cor_beer)\ncor_beer_abs <- cor_beer_abs %>% as_tibble()\ncor_beer_mean <- cor_beer_abs %>% \n  summarise_if(is.numeric, mean)\n\ncor_beer_mean <- t(cor_beer_mean)\ncor_beer_mean <- cbind(cor_beer_mean, rownames(cor_beer_mean))\ncor_beer_mean <- cor_beer_mean %>% \n  as_tibble() \n\ncor_beer_mean <- cor_beer_mean %>% \n  mutate(V1 = as.numeric(V1)) %>% \n  arrange(-V1)\n\ncor_beer_mean <- cor_beer_mean %>% \n  rename(cor_mean = V1,\n         var_name = V2)\n\ncor_beer_mean <- cor_beer_mean %>% \n  mutate(ranking = dense_rank(-cor_mean))\n\ncor_beer_mean_selected <- cor_beer_mean %>% \n  filter(var_name %in% var_list )\n\ndf_cor_beer <- as.data.frame(cor_beer)\ndf_cor_beer <- df_cor_beer %>% \n  mutate(var = rownames(df_cor_beer))\n\ncor_beer_selected <- df_cor_beer %>% \n  filter(var %in% cor_beer_mean_selected$var_name ) %>% \n  select(-var)\n\n\ncor_beer_selected2 <- t(cor_beer_selected)\n\ncor_beer_selected2 <- as.data.frame(cor_beer_selected2) %>% \n  mutate(var = rownames(cor_beer_selected2)) \n\n\ncor_beer_selected2 <- cor_beer_selected2 %>% \n  filter(var %in% cor_beer_mean_selected$var_name ) %>% \n  select(-var)\n\n# p values for correlation test\n# p.mat <- cor_pmat(cor_beer_selected2) \n\n# temporary correlation heatmap with correlation values\n# ggcorrplot( cor_beer_selected2, lab = T,\n#             type = 'lower',\n#             colors = c(\"#2E74C0\", \"white\", \"#CB454A\"),\n# )   + \n#   guides(fill = guide_colourbar(barheight = 38.5))\n# \n# colnames(cor_beer_selected2)\n\ncor_beer_selected3 <- cor_beer_selected2 %>% \n  select(-starts_with(\"cow\"))\n\ncor_beer_selected3 <- t(cor_beer_selected3)\n\ncor_beer_selected3 <- as.data.frame(cor_beer_selected3) %>% \n  select(-starts_with(\"cow\"))\n\n\nvarlist <- rownames(cor_beer_selected3) \nvarlist <- str_replace_all(varlist,\n                           \"brand\", \"\")\nvarlist <- str_replace_all(varlist,\n                           \"container\", \"\")\n\nrownames(cor_beer_selected3) <- varlist\ncolnames(cor_beer_selected3)\nvarlist <- colnames(cor_beer_selected3) \nvarlist <- str_replace_all(varlist,\n                           \"brand\", \"\")\nvarlist <- str_replace_all(varlist,\n                           \"container\", \"\")\ncolnames(cor_beer_selected3) <- varlist\n\n# correlation heatmap with correlation values\np <- ggcorrplot( cor_beer_selected3, lab = T,\n                 colors = c(\"#2E74C0\", \"white\", \"#CB454A\"),\n) +\n  theme(axis.text = element_text(size = rel(1.5),\n                                 face = 'bold'))  + \n  guides(fill = guide_colourbar(barheight = 38.5))\n  \n  \n  # p.values\np\n\n\nknitr::include_graphics('lec_figs/beer_cor.jpg')\n\n\n\n\n\n\n\n\n\nThen, make a correlation heat-map for NY markets with the same selection of variables.\nNY markets are such that whose market value is either ALBANY, BUFFALO-ROCHESTER, URBAN NY, SUBURBAN NY, EXURBAN NY, RURAL NEW YORK, or SYRACUSE.\n\n\nknitr::include_graphics('lec_figs/beer_cor_ny.jpg')\n\n\n\n\n\n\n\n\nReferences:\n\nFundamentals of Data Visualization by Claus O. Wilke\nData Visualization: A practical introduction by Kieran Healy"
  },
  {
    "objectID": "DANL310_midterm-spring-2023-a.html",
    "href": "DANL310_midterm-spring-2023-a.html",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam Example Answers",
    "section": "",
    "text": "library(knitr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(socviz)\nlibrary(ggthemes)\nlibrary(hrbrthemes)\nlibrary(gapminder)\nlibrary(stargazer)\n\n\nI use theme_set(theme_ipsum() + theme(strip.background =element_rect(fill=\"lightgray\"))) in the setup R code chunk with the include = FALSE option"
  },
  {
    "objectID": "DANL310_midterm-spring-2023-a.html#q1a",
    "href": "DANL310_midterm-spring-2023-a.html#q1a",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam Example Answers",
    "section": "Q1a",
    "text": "Q1a\n\nReplicate the following ggplot.\n\nUse the color #0072B2 for dots.\n\n\n\n# Set the data and filter to include only observations from 2007 and exclude Oceania\nggplot(data = filter(gapminder, year == 2007,\n                     continent != 'Oceania'),\n\n       # Set the aesthetics (x-axis and y-axis) to life expectancy and reorder countries by life expectancy\n       aes(x = lifeExp,\n           y = fct_reorder(country, lifeExp))) +\n\n  # Add a layer of points to the plot, setting the color to blue and size to 1.75\n  geom_point(color = \"#0072B2\", size = 1.75) +\n\n  # Add a layer of text labels to the plot, setting the label to life expectancy, hjust to -.25, and size to 2\n  geom_text(aes(label = lifeExp), hjust = -.25,\n            size = 2) +\n\n  # Facet the plot by continent, with y-scales free\n  facet_wrap(.~continent, scales = \"free_y\") +\n\n  # Set the x-axis label to NULL (no name) and limit the x-axis to 35-90\n  scale_x_continuous(\n    name = NULL,\n    lim = c(35, 90)\n  ) +\n\n  # Set the y-axis label to NULL (no name)\n  scale_y_discrete(name = NULL) +\n\n  # Add a title to the plot\n  labs(title = 'Life expectancy in 2007') +\n\n  # Set the theme to minimal\n  theme_minimal() +\n\n  # Customize theme elements: set the y-axis text size, the plot title size and position, and the facet strip text size and font face\n  theme(\n    axis.text.y = element_text(size = rel(.75)),\n    plot.title = element_text(size = rel(1.5),\n                              hjust = 0.5,\n                              face = 'bold'),\n    strip.text = element_text(size = rel(1.25),\n                              face = 'bold')\n  )"
  },
  {
    "objectID": "DANL310_midterm-spring-2023-a.html#q1b",
    "href": "DANL310_midterm-spring-2023-a.html#q1b",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam Example Answers",
    "section": "Q1b",
    "text": "Q1b\n\nMake a simple comment on the visualization result.\n\nAny comment that is not made up is okay."
  },
  {
    "objectID": "DANL310_midterm-spring-2023-a.html#q2a",
    "href": "DANL310_midterm-spring-2023-a.html#q2a",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam Example Answers",
    "section": "Q2a",
    "text": "Q2a\n\nReplicate the following ggplot.\n\nThe following describes the type values:\n\nn_ot_us: Number of US tweets\nn_ot_wrld: Number of worldwide tweets\nn_rt_lk_us: Number of US retweets & likes\nn_rt_lk_wrld: Number of worldwide retweets & likes\n\nUse the colors, maroon and #428bca properly.\n\n\n\n# The following line filters the rows of the n_tweets_long data frame that have a value of \"n_ot_us\" or \"n_ot_wrld\" in the \"type\" column. \n# It then creates a new column called \"type\" that replaces \"n_ot_us\" with \"US\" and \"n_ot_wrld\" with \"Worldwide\".\nn_tweets_long1 <- n_tweets_long %>% \n  filter(type %in% c(\"n_ot_us\", \"n_ot_wrld\") ) %>% \n  mutate(type = ifelse(type == \"n_ot_us\", \"US\", \"Worldwide\"))\n\n\n# The following line filters the rows of the n_tweets_long data frame that have a value of \"n_rt_lk_us\" or \"n_rt_lk_wrld\" in the \"type\" column. \n# It then creates a new column called \"type\" that replaces \"n_rt_lk_us\" with \"US\" and \"n_rt_lk_wrld\" with \"Worldwide\".\nn_tweets_long2 <- n_tweets_long %>% \n  filter(type %in% c(\"n_rt_lk_us\", \"n_rt_lk_wrld\") ) %>% \n  mutate(type = ifelse(type == \"n_rt_lk_us\", \"US\", \"Worldwide\"))\n  \n\n\np2 <- ggplot(mapping = aes(x = year, y = n)) +  # Create a ggplot object with the mapping of the x-axis to the \"year\" variable and y-axis to the \"n\" variable\n  geom_col(n_tweets_long1,  # Add a column chart layer with the \"n_tweets_long1\" data\n           mapping = aes(fill = type),  # Map the \"type\" variable to the fill aesthetic of the chart\n           position = 'dodge', alpha = .67) +  # Set the position of the columns to \"dodge\" and the transparency to 0.67\n  geom_line(n_tweets_long2,  # Add a line chart layer with the \"n_tweets_long2\" data\n            mapping = aes(color = type),  # Map the \"type\" variable to the color aesthetic of the chart\n            size = 1.5) +  # Set the line size to 1.5\n  geom_point(data = n_tweets_long2,  # Add a point chart layer with the \"n_tweets_long2\" data\n             size = 2,  # Set the point size to 2\n             color = 'black')  +  # Set the point color to black\n  scale_x_continuous(breaks = seq(2012, 2017, 1)) +  # Set the x-axis breaks to the sequence from 2012 to 2017 with an interval of 1\n  scale_y_continuous(label = scales::comma) +  # Format the y-axis labels using the comma function from the scales package\n  scale_color_manual(values = c('maroon', '#428bca')) +  # Manually set the color values for the color aesthetic\n  scale_fill_manual(values = c('maroon', '#428bca')) +   # Manually set the color values for the fill aesthetic\n  guides(fill = guide_legend(reverse = TRUE,  # Customize the fill legend guide by reversing the order of the legend, positioning the labels at the bottom, and setting the number of rows to 2 and the key width to 2\n                             # title.position = \"top\",\n                             label.position = \"bottom\",\n                             keywidth = 2,\n                             nrow = 2,\n                             order = 1),\n         color = guide_legend(reverse = TRUE,  # Customize the color legend guide by reversing the order of the legend, positioning the labels at the bottom, and setting the number of rows to 2 and the key width to 2\n                             # title.position = \"top\",\n                             label.position = \"bottom\",\n                             keywidth = 2,\n                             nrow = 2,\n                             order = 2)) +\n  labs(x = \"Year\",  # Add x-axis label \"Year\"\n       y = \"Number of Tweets, Retweets & Likes\\n (in thousand)\",  # Add y-axis label \"Number of Tweets, Retweets & Likes (in thousand)\"\n       fill = \"Tweets\",  # Add fill legend label \"Tweets\"\n       color = \"Retweets and likes\",  # Add color legend label \"Retweets and likes\"\n       caption = 'Source: Choe, \"Social Media Campaigns, Lobbying, and Climate Change Legislation:\\n Evidence from #climatechange/#globalwarming and Energy Lobbies\" (2023)') +  # Add caption with source information\n  theme_ipsum() +  # Use the 'theme_ipsum' theme from the 'ggthemes' package\n  theme(\n  axis.title.y = element_text(\n    size = rel(1.5),\n    margin = margin(t = 0, r = 20, b = 0, l = 0) # set the margin for the y axis title\n  ),\n  axis.title.x = element_text(\n    size = rel(1.5),\n    margin = margin(t = 10, r = 0, b = 0, l = 0) # set the margin for the x axis title\n  ),\n  axis.text.x = element_text(\n    size = rel(1.25) # set the font size for the x axis tick labels\n  ),\n  axis.text.y = element_text(\n    size = rel(1.25) # set the font size for the y axis tick labels\n  ),\n  legend.position = 'top', # set the position of the legend\n  legend.title = element_text(\n    size = rel(1),\n    face = 'bold',\n    hjust = .5 # set the font size, face and horizontal justification for the legend title\n  ),\n  legend.text = element_text(\n    size = rel(1) # set the font size for the legend text\n  ),\n  legend.spacing.x = unit(1.25, \"cm\"), # set the horizontal spacing between legend items\n  plot.caption = element_text(\n    size = rel(1.25),\n    hjust = .5 # set the font size and horizontal justification for the plot caption\n  )\n)\n\n\np2"
  },
  {
    "objectID": "DANL310_midterm-spring-2023-a.html#q2b.",
    "href": "DANL310_midterm-spring-2023-a.html#q2b.",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam Example Answers",
    "section": "Q2b.",
    "text": "Q2b.\n\nMake a simple comment on the visualization result.\n\nAny comment that is not made up is okay."
  },
  {
    "objectID": "DANL310_midterm-spring-2023-a.html#q3a",
    "href": "DANL310_midterm-spring-2023-a.html#q3a",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam Example Answers",
    "section": "Q3a",
    "text": "Q3a\n\nReplicate the following ggplot.\n\nYou should calculate the proportion of Pit Bull (or Mix) for each zip code.\nYou should join data.frames properly.\nChoose the color palette from the viridis scales https://ggplot2.tidyverse.org/reference/scale_viridis.html.\nUse coord_map(projection = \"albers\", lat0 = 39, lat1 = 45).\n\n\n\n# Joining two data frames using a common variable\nnyc_zips_df <- nyc_zips_df %>% \n  left_join(nyc_zips_coord)\n\n# Creating a data frame of the top 5 dog breeds by count\nnyc_dogs <- nyc_dog_license %>%\n  group_by(breed_rc) %>% \n  summarise(N = n()) %>% \n  filter(dense_rank(-N)<=5)\n\n# Creating a data frame of dog breed frequency and percentage by zip code for the top 5 breeds\nnyc_fb <- nyc_dog_license %>%\n  group_by(zip_code, breed_rc) %>%\n  summarize(n = n()) %>%\n  mutate(freq = n / sum(n),\n         pct = round(freq*100, 2)) %>%\n  filter(breed_rc %in% nyc_dogs$breed_rc )\n\n\n# theme_nymap <- function(base_size=9, base_family=\"\") {\n#   require(grid)\n#   theme_bw(base_size=base_size, base_family=base_family) %+replace%\n#     theme(axis.line=element_blank(),\n#           axis.text=element_blank(),\n#           axis.ticks=element_blank(),\n#           axis.title=element_blank(),\n#           panel.background=element_blank(),\n#           panel.border=element_blank(),\n#           panel.grid=element_blank(),\n#           panel.spacing=unit(0, \"lines\"),\n#           plot.background=element_blank(),\n#     )\n# }\n\n\n# Create a map of New York City zip codes colored by the share of Pit Bull dogs \n# and their mixes out of all licensed dogs, based on licensing data\nfb_map <- nyc_zips_df %>% \n  left_join(nyc_fb)\n\n# Filter for Pit Bull breeds and plot the map\nfilter(fb_map, breed_rc %in% c('Pit Bull (or Mix)')) %>% \n  ggplot(mapping = aes(x = X, y = Y, \n                       fill = pct,\n                       group = zip_code)) +\n  geom_polygon(color = \"gray80\", \n               size = 0.1) +    # draw the zip code polygons\n  scale_fill_viridis_c(option = \"inferno\",\n                       breaks = seq(0,24,2)) +  # set the color scale for Pit Bull share\n  labs(fill = \"Pit Bull's Share of All Licensed Dogs (%)\",\n       title = \"New York City's Pit Bull\",\n       subtitle = \"By Zip Code. Based on Licensing Data\") +  # set the map title and legend title\n  theme_map() +  # set the map theme\n  theme(legend.justification = c(.5,.5),\n        legend.position = 'top',\n        legend.direction = \"horizontal\",\n        legend.text = element_text(size= rel(1.25)),\n        legend.title = element_text(size= rel(1.25),\n                                face = 'bold',\n                                hjust = .5),\n        plot.title = element_text(hjust = .5,\n                                  vjust = .5,\n                                  face = 'bold',\n                                  size = rel(2.25)),\n        plot.subtitle = element_text(hjust = .5,\n                                     vjust = .5,\n                                     size = rel(1.25))) +  # customize the theme of the plot\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +  # set the map projection\n  guides(fill = guide_legend(title.position = \"top\",\n                             label.position = \"bottom\",\n                             keywidth = 1, nrow = 1))  # set the legend position"
  },
  {
    "objectID": "DANL310_midterm-spring-2023-a.html#q3b",
    "href": "DANL310_midterm-spring-2023-a.html#q3b",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam Example Answers",
    "section": "Q3b",
    "text": "Q3b\n\nWhich zip_code does have the highest proportion of Pit Bull (or Mix)?\n\n\nq3b <- fb_map %>% \n  select(zip_code, breed_rc, pct) %>% \n  filter(breed_rc == 'Pit Bull (or Mix)') %>% \n  arrange(-pct) %>% \n  distinct()"
  },
  {
    "objectID": "DANL310_midterm-spring-2023-a.html#q4a",
    "href": "DANL310_midterm-spring-2023-a.html#q4a",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam Example Answers",
    "section": "Q4a",
    "text": "Q4a\n\nReplicate the following ggplot.\n\n\n# Create a new variable year extracted from the Date column\nstock <- stock %>% \n  mutate(year = year(Date))\n\np <- ggplot(data = filter(stock, year >= 2019, year <= 2022 ) , \n            aes(x = log(Volume), y = log(Close), color = company))\n\np + \n  geom_point(alpha = .05) +\n  geom_smooth(method = lm, color = 'black') +\n  facet_grid( company ~ year, scales = 'free' ) +\n  labs(x = 'Volume (in log)',\n       y = 'Close (in log)') +\n  guides(color = \"none\")"
  },
  {
    "objectID": "DANL310_midterm-spring-2023-a.html#q4b",
    "href": "DANL310_midterm-spring-2023-a.html#q4b",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam Example Answers",
    "section": "Q4b",
    "text": "Q4b\n\nIn 2020, which company’s stock trading Volume does seem to be the most insensitive to a change in Close price?\n\n\n# Create a new variable year extracted from the Date column\nstock <- stock %>% \n  mutate(year = year(Date))\n\np <- ggplot(data = filter(stock, year >= 2019, year <= 2022 ) , \n            aes(x = log(Volume), y = log(Close), color = company))\n\np + \n  geom_point(alpha = .05) +\n  geom_smooth(method = lm, color = 'black') +\n  facet_grid( company ~ year ) +\n  labs(x = 'Volume (in log)',\n       y = 'Close (in log)') +\n  guides(color = \"none\")\n\n\n\n\n\nIn 2020, TSLA’s trading Volume seems to be the most insensitive to a change in Close price.\nThe reason is that for a one unit increase in log(Price), the least amount of log(Volume) changes for TSLA.\n\n\nreg <- lm(log(Volume) ~ log(Close) * company,\n          data = filter(stock, year == 2020))\nreg_sum <- tidy(reg, conf.int = T) %>% \n  filter(str_detect(term, \"log\"), term != \"log(Close)\")\n\nggplot(reg_sum,\n       aes(x = estimate, y = fct_reorder(term, estimate),\n           xmin = conf.low, \n           xmax = conf.high)) +\n  geom_point() +\n  geom_pointrange() +\n  geom_vline(xintercept = 0, color = 'red', lty = 2)"
  },
  {
    "objectID": "DANL310_lab1a.html",
    "href": "DANL310_lab1a.html",
    "title": "R Visualization Lab 1 - Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(socviz)"
  },
  {
    "objectID": "DANL310_lab1a.html#variable-description",
    "href": "DANL310_lab1a.html#variable-description",
    "title": "R Visualization Lab 1 - Maps",
    "section": "Variable Description",
    "text": "Variable Description\n\nbelief:\n\nhuman: Estimated percentage who think that global warming is caused mostly by human activities.\nhappening: Estimated percentage who think that global warming is happening.\n\n\n\nclimate_opinion_long <- read_csv(\n  'https://bcdanl.github.io/data/climate_opinion_2021.csv')\n\nRows: 6284 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): GeoName, belief\ndbl (2): id, perc\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nclimate_opinion_long\n\n# A tibble: 6,284 × 4\n      id GeoName                 belief     perc\n   <dbl> <chr>                   <chr>     <dbl>\n 1  1001 Autauga County, Alabama happening  59.2\n 2  1001 Autauga County, Alabama human      45.5\n 3  1003 Baldwin County, Alabama happening  60.5\n 4  1003 Baldwin County, Alabama human      44.7\n 5  1005 Barbour County, Alabama happening  68.1\n 6  1005 Barbour County, Alabama human      50.5\n 7  1007 Bibb County, Alabama    happening  57.6\n 8  1007 Bibb County, Alabama    human      42.6\n 9  1009 Blount County, Alabama  happening  52.5\n10  1009 Blount County, Alabama  human      41.5\n# … with 6,274 more rows"
  },
  {
    "objectID": "DANL310_lab1a.html#q1a.",
    "href": "DANL310_lab1a.html#q1a.",
    "title": "R Visualization Lab 1 - Maps",
    "section": "Q1a.",
    "text": "Q1a.\n\nFilter climate_opinion_long, so that climate_opinion_long has only estimated percentage who think that global warming is caused mostly by human activities.\nThen join the two data.frames socviz::county_map and the resulting data.frame above.\n\n\nclimate_opinion_long <- climate_opinion_long %>% \n  filter(belief == 'human')\n\ncounty_map <- county_map\ncounty_map$id <- as.integer(county_map$id)\ncounty_full <- left_join(county_map, climate_opinion_long)\n\nJoining, by = \"id\""
  },
  {
    "objectID": "DANL310_lab1a.html#q1b.",
    "href": "DANL310_lab1a.html#q1b.",
    "title": "R Visualization Lab 1 - Maps",
    "section": "Q1b.",
    "text": "Q1b.\n\nReplicate the following map.\n\nDo not use coord_map(projection = \"albers\", lat0 = 39, lat1 = 45).\n\n\n\np1 <- ggplot(data = county_full) + \n  geom_polygon(mapping = aes(x = long, y = lat,\n                             group = group, \n                             fill = perc),\n               color = \"grey60\", \n               linewidth = 0.1) \n\n\np2 <- p1 + scale_fill_gradient2( \n  low = '#2E74C0',  \n  high = '#CB454A',  \n  mid = 'white', # transparent white\n  na.value = \"grey80\",\n  midpoint = 50,\n  breaks = c(quantile(climate_opinion_long$perc, 0, na.rm = T),\n             quantile(climate_opinion_long$perc, .25, na.rm = T),\n             quantile(climate_opinion_long$perc, .5, na.rm = T),\n             quantile(climate_opinion_long$perc, .75, na.rm = T),\n             quantile(climate_opinion_long$perc, 1, na.rm = T)),\n  labels = c(paste(round(quantile(climate_opinion_long$perc, 0, na.rm = T), 1),\"\\n(Min)\"),\n             paste(round(quantile(climate_opinion_long$perc, .25, na.rm = T), 1),\"\\n(25th)\"),\n             paste(round(quantile(climate_opinion_long$perc, .5, na.rm = T), 1),\"\\n(50th)\"),\n             paste(round(quantile(climate_opinion_long$perc, .75, na.rm = T), 1),\"\\n(75th)\"),\n             paste(round(quantile(climate_opinion_long$perc, 1, na.rm = T), 1),\"\\n(Max)\")\n  ),\n  guide = guide_colorbar( direction = \"horizontal\",\n                          barwidth = 25,\n                          title.vjust = 1 )\n) \n\np <- p2 + labs(fill = \"Percent\\nBelief\", title = \"U.S. Climate Opinion, 2021\",\n               caption = \"Sources: Yale Program on Climate Change Communication\\n(https://climatecommunication.yale.edu/visualizations-data/ycom-us/)\") +\n  theme_map() +\n  theme(plot.margin = unit( c(1, 1, 3.85, 0.5), \"cm\"),\n        legend.position = c(0.5, -.15),\n        legend.justification = c(.5,.5),\n        strip.background = element_rect( colour = \"black\",\n                                         fill = \"white\",\n                                         color = \"grey80\" )\n  ) +\n  guides(fill = guide_colourbar(direction = \"horizontal\", barwidth = 25,\n                                title.vjust = 1))\np"
  },
  {
    "objectID": "DANL200_lab1a.html",
    "href": "DANL200_lab1a.html",
    "title": "R Lab 1 - ggplot visualization",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n# install.packages(\"hexbin\")   # if you do not have the \"hexbin\" package\nlibrary(hexbin)"
  },
  {
    "objectID": "DANL200_lab1a.html#q1a",
    "href": "DANL200_lab1a.html#q1a",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1a",
    "text": "Q1a\nRead the data file, bikeshare_cleaned.csv, as the data.frame object with the name, bikeshare, using (1) the read_csv() function and (2) its URL, https://bcdanl.github.io/data/bikeshare_cleaned.csv.\n\nurl <- 'https://bcdanl.github.io/data/bikeshare_cleaned.csv'\nbikeshare <- read_csv(url)\n\nRows: 17376 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): month, date, wkday, seasons, weather_cond\ndbl (7): cnt, year, hr, holiday, temp, hum, windspeed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbikeshare\n\n# A tibble: 17,376 × 12\n     cnt  year month date     hr wkday    holiday seasons weather…¹   temp   hum\n   <dbl> <dbl> <chr> <chr> <dbl> <chr>      <dbl> <chr>   <chr>      <dbl> <dbl>\n 1    16  2011 01    01        0 saturday       0 spring  Clear or… -1.33  0.947\n 2    40  2011 01    01        1 saturday       0 spring  Clear or… -1.44  0.896\n 3    32  2011 01    01        2 saturday       0 spring  Clear or… -1.44  0.896\n 4    13  2011 01    01        3 saturday       0 spring  Clear or… -1.33  0.636\n 5     1  2011 01    01        4 saturday       0 spring  Clear or… -1.33  0.636\n 6     1  2011 01    01        5 saturday       0 spring  Mist or … -1.33  0.636\n 7     2  2011 01    01        6 saturday       0 spring  Clear or… -1.44  0.896\n 8     3  2011 01    01        7 saturday       0 spring  Clear or… -1.54  1.21 \n 9     8  2011 01    01        8 saturday       0 spring  Clear or… -1.33  0.636\n10    14  2011 01    01        9 saturday       0 spring  Clear or… -0.919 0.688\n# … with 17,366 more rows, 1 more variable: windspeed <dbl>, and abbreviated\n#   variable name ¹​weather_cond\n\n\n\nskim(bikeshare)\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nbikeshare\n\n\n\n\nNumber of rows\n\n\n17376\n\n\n\n\nNumber of columns\n\n\n12\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\ncharacter\n\n\n5\n\n\n\n\nnumeric\n\n\n7\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nempty\n\n\nn_unique\n\n\nwhitespace\n\n\n\n\n\n\nmonth\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n12\n\n\n0\n\n\n\n\ndate\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n31\n\n\n0\n\n\n\n\nwkday\n\n\n0\n\n\n1\n\n\n6\n\n\n9\n\n\n0\n\n\n7\n\n\n0\n\n\n\n\nseasons\n\n\n0\n\n\n1\n\n\n4\n\n\n6\n\n\n0\n\n\n4\n\n\n0\n\n\n\n\nweather_cond\n\n\n0\n\n\n1\n\n\n14\n\n\n24\n\n\n0\n\n\n3\n\n\n0\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\ncnt\n\n\n0\n\n\n1\n\n\n189.48\n\n\n181.40\n\n\n1.00\n\n\n40.00\n\n\n142.00\n\n\n281.00\n\n\n977.00\n\n\n▇▃▁▁▁\n\n\n\n\nyear\n\n\n0\n\n\n1\n\n\n2011.50\n\n\n0.50\n\n\n2011.00\n\n\n2011.00\n\n\n2012.00\n\n\n2012.00\n\n\n2012.00\n\n\n▇▁▁▁▇\n\n\n\n\nhr\n\n\n0\n\n\n1\n\n\n11.55\n\n\n6.91\n\n\n0.00\n\n\n6.00\n\n\n12.00\n\n\n18.00\n\n\n23.00\n\n\n▇▇▆▇▇\n\n\n\n\nholiday\n\n\n0\n\n\n1\n\n\n0.03\n\n\n0.17\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n▇▁▁▁▁\n\n\n\n\ntemp\n\n\n0\n\n\n1\n\n\n0.00\n\n\n1.00\n\n\n-2.48\n\n\n-0.82\n\n\n0.02\n\n\n0.85\n\n\n2.61\n\n\n▂▇▇▇▁\n\n\n\n\nhum\n\n\n0\n\n\n1\n\n\n0.00\n\n\n1.00\n\n\n-3.25\n\n\n-0.76\n\n\n0.01\n\n\n0.79\n\n\n1.93\n\n\n▁▃▇▇▆\n\n\n\n\nwindspeed\n\n\n0\n\n\n1\n\n\n0.00\n\n\n1.00\n\n\n-1.55\n\n\n-0.70\n\n\n0.03\n\n\n0.52\n\n\n5.40\n\n\n▇▆▂▁▁\n\n\n\n\n\n\ntable(bikeshare$wkday)\n\n\n   friday    monday  saturday    sunday  thursday   tuesday wednesday \n     2487      2478      2511      2502      2471      2453      2474 \n\ntable(bikeshare$month)\n\n\n  01   02   03   04   05   06   07   08   09   10   11   12 \n1426 1341 1473 1437 1488 1440 1488 1475 1437 1451 1437 1483 \n\ntable(bikeshare$seasons)\n\n\n  fall spring summer winter \n  4496   4239   4409   4232 \n\ntable(bikeshare$weather_cond)\n\n\n     Clear or Few Cloudy Light Snow or Light Rain           Mist or Cloudy \n                   11413                     1419                     4544 \n\nprop.table( table( bikeshare$wkday ) )\n\n\n   friday    monday  saturday    sunday  thursday   tuesday wednesday \n0.1431285 0.1426105 0.1445097 0.1439917 0.1422076 0.1411717 0.1423803 \n\nprop.table( table( bikeshare$month ) )\n\n\n        01         02         03         04         05         06         07 \n0.08206722 0.07717541 0.08477210 0.08270028 0.08563536 0.08287293 0.08563536 \n        08         09         10         11         12 \n0.08488720 0.08270028 0.08350599 0.08270028 0.08534761 \n\nprop.table( table( bikeshare$seasons ) )\n\n\n     fall    spring    summer    winter \n0.2587477 0.2439572 0.2537408 0.2435543 \n\nprop.table( table( bikeshare$weather_cond ) )\n\n\n     Clear or Few Cloudy Light Snow or Light Rain           Mist or Cloudy \n              0.65682551               0.08166436               0.26151013 \n\n\nUse the data.frame bikeshare for the rest of questions in Question 1.\n\nDescription of variables in the data file, bikeshare_cleaned.csv\nThe data set, bikeshare_cleaned.csv, includes 17376 observations of hourly counts from 2011 to 2012 for bike rides (rentals) in Washington D.C.\n\ncnt: count of total bikes rented out\nyear: year\nmonth: month\ndate: date\nhr: hours\nwkday: week day\nholiday: holiday if holiday == 1; non-holiday otherwise\nseasons: season\nweather_cond: weather condition\ntemp: temperature, measured in standard deviations from average.\nhum: humidity, measured in standard deviations from average.\nwindspeed: wind speed, measured in standard deviations from average."
  },
  {
    "objectID": "DANL200_lab1a.html#q1b",
    "href": "DANL200_lab1a.html#q1b",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1b",
    "text": "Q1b\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of cnt.\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = cnt),\n                 binwidth = 5) \n\n\n\n\n\n\n\nThe distribution of cnt is right-skewed.\nThe most common values for cnt range from 0 to 50.\nNote: When visualizing a histogram, we should experiment on either bins (the number of bins) or binwidth (the width of a bin)."
  },
  {
    "objectID": "DANL200_lab1a.html#q1c",
    "href": "DANL200_lab1a.html#q1c",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1c",
    "text": "Q1c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of cnt by year and month.\n\n# density plot\nggplot(bikeshare) +\n  geom_density( aes(x = cnt, fill = month),\n                color = NA,\n                show.legend = F) +\n  facet_grid(month~year) \n\n# histogram\nggplot(bikeshare) +\n  geom_histogram( aes(x = cnt, fill = month),\n                  binwidth = 10,\n                  color = NA,\n                  show.legend = F) +\n  facet_grid(month~year) \n\n# boxplot\nggplot(bikeshare) +\n  geom_boxplot( aes(x = cnt, y = month, \n                    fill = month),\n                  show.legend = F) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverall, the demand for bike rentals tends to be higher in 2012 than in 2011.\nOverall, the demand for bike rentals tends to be lower in winter."
  },
  {
    "objectID": "DANL200_lab1a.html#q1d",
    "href": "DANL200_lab1a.html#q1d",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1d",
    "text": "Q1d\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of temp by year and month.\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = temp, fill = month),\n                  binwidth = .1,\n                 show.legend = F) +\n  geom_vline(xintercept = 0, color = 'red') + \n  facet_grid(month ~ year) \n\n\n\n\n\n\n\nWe observe the four seasons when it comes to temperature.\nJanuary tends to be the coldest and July is the hottest.\nThe distribution of temperature across months looks similar across years 2011-2012."
  },
  {
    "objectID": "DANL200_lab1a.html#q1e",
    "href": "DANL200_lab1a.html#q1e",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1e",
    "text": "Q1e\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of hum by year and month.\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = hum, fill = month),\n                binwidth = .1,\n                show.legend = F) +\n  geom_vline(xintercept = 0, color = 'red') + \n  facet_grid(month ~ year) \n\n\n\n\n\n\n\nIn years 2011-2012 in Washington D.C., May, August, and September tend to be more humid than other months.\nIn years 2011-2012 in Washington D.C., January and February tend to be less humid than other months.\nOverall, the distribution of humidity across months looks similar across years 2011-2012 in Washington D.C."
  },
  {
    "objectID": "DANL200_lab1a.html#q1f",
    "href": "DANL200_lab1a.html#q1f",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1f",
    "text": "Q1f\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of windspeed by year and month.\n\nggplot(bikeshare) +\n  geom_density(aes(x = windspeed)) +\n  geom_vline(xintercept = 0, color = 'red') + \n  facet_grid(month ~ year) \n\n\n\n\n\n\n\nOverall, the monthly distribution of wind speed looks similar across years 2011-2012 in Washington D.C.\nIn years 2011-2012 in Washington D.C., noticeably slow wind speed, -1.5, which is a deviation from the standardized mean of wind speed 0, is observed throughout all months."
  },
  {
    "objectID": "DANL200_lab1a.html#q1g",
    "href": "DANL200_lab1a.html#q1g",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1g",
    "text": "Q1g\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between temp and cnt.\n\nggplot(bikeshare,\n       aes(x = temp, y = cnt)) +\n  geom_hex() +\n  geom_smooth(color = 'red') +\n  geom_smooth(method = lm) \n\n\n\n\n\n\n\ntemp and cnt are positively associated with each other.\nToo high temp (above 1.5) may lead to lower cnt."
  },
  {
    "objectID": "DANL200_lab1a.html#q1h",
    "href": "DANL200_lab1a.html#q1h",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1h",
    "text": "Q1h\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between temp and cnt by year and month.\n\nggplot(bikeshare, aes(x = temp, y = cnt )) +\n    geom_point(alpha = .1)  +\n    geom_smooth(color = \"red3\",\n                fill = \"orchid\",\n                method = lm)  +\n    geom_smooth(color = \"royalblue\",\n                fill = \"orchid\")  +\n  facet_grid(month~year)\n\n\n\n\n\n\n\nOverall, temp and cnt are positively associated with each other.\nIn June, July, and August, the association between temp and cnt switches from positive to negative at which temp is around 1.5."
  },
  {
    "objectID": "DANL200_lab1a.html#q1i",
    "href": "DANL200_lab1a.html#q1i",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1i",
    "text": "Q1i\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between weather_cond and cnt.\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = cnt),\n                 binwidth = 5) + \n  facet_grid(. ~ weather_cond) \n\nggplot(bikeshare) +\n  geom_density(aes(x = cnt)) + \n  facet_grid(. ~ weather_cond) \n\n\n\n\n\n\n\n\n\n\nIn 2012-2013 in Washington D.C., people rented out bikes more often when weather_cond is Clear or Few Cloudy.\nThe most common values for cnt are around 50 across all values of weather_cond.\nWhen weather_cond is Light Snow or Light Rain, people are more likely to rent less number of bikes."
  },
  {
    "objectID": "DANL200_lab1a.html#q1j",
    "href": "DANL200_lab1a.html#q1j",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1j",
    "text": "Q1j\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between weather_cond and cnt by hr.\n\nggplot(bikeshare) +\n  geom_density(aes(x = cnt, fill = as.factor(hr)),\n               color = NA,\n               show.legend = F) + \n  facet_grid(hr ~ weather_cond, scale = \"free_y\") \n\n\n\n\n\n\n\nThe values of cnt during commuting hours (hr 7:00 A.M.-8:59 A.M. and 5:00 P.M.-7:59 P.M.) are often larger than other commuting hours.\n\nIt implies that a shortage of rental bikes is more likely to happen during these hours."
  },
  {
    "objectID": "DANL200_lab1a.html#q1k",
    "href": "DANL200_lab1a.html#q1k",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1k",
    "text": "Q1k\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between wkday and cnt.\n\nggplot(bikeshare) +\n  geom_density(aes(x = cnt,\n                   fill = as.factor(wkday)),\n               color = NA,\n               show.legend = F) + \n  facet_grid(. ~ wkday)\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = cnt,\n                   fill = as.factor(wkday)),\n                 binwidth = 10,\n               color = NA,\n               show.legend = F) + \n  facet_grid(. ~ wkday)\n\n\nggplot(bikeshare,\n       aes(x = wkday, y = cnt)) +\n  geom_boxplot( aes(fill = wkday),\n                show.legend = F ) +\n  stat_summary(fun = mean)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of cnt is right-skewed, and looks similar across all values of wkday."
  },
  {
    "objectID": "DANL200_lab1a.html#q1l",
    "href": "DANL200_lab1a.html#q1l",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1l",
    "text": "Q1l\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between wkday and cnt by hr.\n\nggplot(bikeshare) +\n  geom_density(aes(x = cnt,\n                     fill = as.factor(hr)),\n               color = NA, \n               show.legend = F) + \n  facet_grid(hr~wkday, scale = \"free_y\") \n\n\nggplot(bikeshare) +\n  geom_histogram(aes(x = cnt,\n                     fill = as.factor(hr)),\n                 binwidth = 10,\n               color = NA, \n               show.legend = F) + \n  facet_grid(hr~wkday, scale = \"free_y\") \n\n\nggplot(bikeshare) +\n  geom_boxplot(aes(x = cnt,\n                   y = as.factor(hr),\n                   fill = as.factor(hr)),\n               show.legend = F) + \n  facet_grid(.~wkday, scale = \"free_y\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuring hours from 10 to 15, people tend to rent out bikes more on Saturday and Sunday than on other week days.\nDuring the morning commuting hours, people tend to rent out bikes less on Saturday and Sunday than on other week days."
  },
  {
    "objectID": "DANL200_lab1a.html#q2a",
    "href": "DANL200_lab1a.html#q2a",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2a",
    "text": "Q2a\nRead the data file, NY_school_enrollment_socioecon.csv, as the data.frame object with the name, NY_school_enrollment_socioecon, using (1) the read_csv() function and (2) its URL, https://bcdanl.github.io/data/NY_school_enrollment_socioecon.csv.\n\nurl2 <- 'https://bcdanl.github.io/data/NY_school_enrollment_socioecon.csv'\nNY_school_enrollment_socioecon <- read_csv(url2)\nNY_school_enrollment_socioecon\n\n\nWe can view the data.frame NY_school_enrollment_socioecon:\n\n\n\n# A tibble: 372 × 310\n    fips  year county_name pincp pover…¹ pover…² pover…³ pover…⁴ pover…⁵ pover…⁶\n   <dbl> <dbl> <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 36001  2015 Albany      55120    30.9    13.9     8.2     4.6    30.3    13.2\n 2 36001  2016 Albany      55126    28      14.4     8.3     4.1    28.2    13.9\n 3 36001  2017 Albany      58814    28      14       8.1     3.7    26.5    13.7\n 4 36001  2018 Albany      59547    26.4    12.8     8.5     3.8    23.6    12.9\n 5 36001  2019 Albany      61876    27.1    11.9     7.6     4.1    23.6    12  \n 6 36001  2020 Albany      66632    27.4    12.9     8.3     4.1    24.1    11.9\n 7 36003  2015 Allegany    32205    26.5    14.9    10.4     3.9    23      12.9\n 8 36003  2016 Allegany    32417    25.9    14.4    11.2     4      23.2    11.3\n 9 36003  2017 Allegany    34001    25.8    13.9    11.4     4.1    23.9    11.1\n10 36003  2018 Allegany    34553    26.9    14.3    12       4.9    24      11.2\n# … with 362 more rows, 300 more variables: poverty_male_assc <dbl>,\n#   poverty_male_bachelor_or_higher <dbl>, poverty_female_less_than_hs <dbl>,\n#   poverty_female_hs <dbl>, poverty_female_assc <dbl>,\n#   poverty_female_bachelor_or_higher <dbl>, c01_001 <dbl>, c01_002 <dbl>,\n#   c01_003 <dbl>, c01_004 <dbl>, c01_005 <dbl>, c01_006 <dbl>, c01_007 <dbl>,\n#   c01_008 <dbl>, c01_009 <dbl>, c01_010 <dbl>, c01_011 <dbl>, c01_012 <dbl>,\n#   c01_013 <dbl>, c01_014 <dbl>, c01_015 <dbl>, c01_016 <dbl>, …\n\n\nFor description of variables in NY_school_enrollment_socioecon, refer to the file, ny_school_enrollment_socioecon_description.zip, which is in the Files section in our Canvas web-page. (I recommend you to extract the zip file, and then read the file, ny_school_enrollment_socioecon_description.csv, using Excel or Numbers.)\n\nHere are some details about the data.frame, NY_school_enrollment_socioecon:\nThe geographic and time units of observation (row) in the data.frame, NY_school_enrollment_socioecon, are New York county and year.\n\n\n\n\n\n \n  \n    FIPS \n    year \n    county_name \n    pincp \n    c01_001 \n    c02_002 \n  \n \n\n  \n    36001 \n    2015 \n    Albany \n    55793 \n    84463 \n    4.7 \n  \n\n\n\n\n\n\nFor example, the observation above means that in Albany county in year 2015 …\n\nAverage personal income of people is $55,793.\nPopulation 3 years and over enrolled in school is 84,463.\nPercent of population 3 years and over enrolled in nursery school and preschool is 4.7%.\n\nThe following is sample observations from Bronx and Livingston counties:\n\n\n\n\n\n\n\n\n\n\n\nThe following describes the variables:\n\nc01_010: Total!!Population enrolled in college or graduate school\nSo, c01_010 is total population enrolled in college or graduate school;\nc02_010: Percent!!Population enrolled in college or graduate school\nSo, c02_010 is a percent of total population enrolled in college or graduate school;\nIn which county is more likely for a person to be enrolled in a college or graduate school?\n\nA county’s college enrollment level can be represented by an overall tendency of that county’s residents to be enrolled in college (as long as we are interested in analyzing how a human behaves overall!).\nThe size of a county’s population enrolled in college or graduate school (c01_010) may not be appropriate to represent a county’s college enrollment level.\n\nA county’s larger size of population enrolled in college does not necessarily mean people in people in that county are likely to be enrolled in college.\n\nConsider the following example:\n\n\n\n\n\n \n  \n    County \n    Total.Population \n    Bachelor.s.Degree \n    High.School \n    Percent.of.Bachelor.s.Degree \n    Percent.of.High.School \n  \n \n\n  \n    A \n    100,000 \n    1,000 \n    99,000 \n    1.0% \n    99.0% \n  \n  \n    B \n    1,000 \n    999 \n    1 \n    99.9% \n    0.1% \n  \n\n\n\n\n\n\nAlthough County A has the larger number of people that have bachelor’s degrees than County B, it is more appropriate to say that people in County B have a higher college enrollment than people in County A.\nThis is because the overall tendency of County B’s people to attend college is stronger than that of County A’s people.\nSimilarly, to represent a standard of living of people in a country, we do not use a country’s gross domestic product (GDP) but its GDP per capita (GDP per capita is GDP devided by population).\n\nFor example, China records the second largest GDP in the world as of now. However, World Bank still considers China a middle-income country, because of its relatively low level of GDP per capita."
  },
  {
    "objectID": "DANL200_lab1a.html#q2b",
    "href": "DANL200_lab1a.html#q2b",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2b",
    "text": "Q2b\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between college enrollment and educational attainment of population 45 to 64 years, and how such relationship varies by the type (public or private) of colleges.\n\nTo represent a level of educational attainment of population 45 to 64 years in a county, I choose variable, 100 - d01_024, a percent of population 45 to 64 years without bachelor’s degree.\nTo represent a level of college enrollment of population in a county, I choose variable, c02_010, a percent of population enrolled in college or graduate school.\nTo represent a level of public college’s enrollment of population in a county, I choose variable, c04_010, a percent of population enrolled in public college or graduate school.\nTo represent a level of private college’s enrollment of population in a county, I choose variable, c06_010, a percent of population enrolled in private college or graduate school.\nThe following ggplot describes the relationship between college enrollment and educational attainment of population 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon,\n       aes(x = 100 - d01_024, y = c02_010)) +\n  geom_hex()  +\n  geom_smooth(method = lm)  +\n  geom_smooth(color = 'red') +\n  coord_fixed()\n\n\n\n\n\n\n\nThe percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in college or graduate school.\nThe following ggplot describes the relationship between college enrollment in public schools and educational attainment of population 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon,\n       aes(x = 100 - d01_024, y = c04_010)) +\n  geom_hex()  +\n  geom_smooth(method = lm)  +\n  geom_smooth(color = 'red') +\n  coord_fixed()\n\n\n\n\n\n\n\nThe following ggplot describes the relationship between college enrollment in private schools and educational attainment of population 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon,\n       aes(x = 100 - d01_024, y = c06_010)) +\n  geom_hex()  +\n  geom_smooth(method = lm)  +\n  geom_smooth(color = 'red') +\n  coord_fixed()\n\n\n\n\n\n\n\nThe percentage of population 45 to 64 years without Bachelor’s degree is positively associated with the percentage of population enrolled in public college or graduate school.\nThe percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in private college or graduate school."
  },
  {
    "objectID": "DANL200_lab1a.html#q2c",
    "href": "DANL200_lab1a.html#q2c",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2c",
    "text": "Q2c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe how the relationships described in Q3b vary by gender of population 45 to 64 years.\n\nTo represent a level of educational attainment of male population 45 to 64 years in a county, I choose variable, 100 - d03_024, a percent of male population 45 to 64 years without bachelor’s degree.\nTo represent a level of educational attainment of female population 45 to 64 years in a county, I choose variable, 100 - d05_024, a percent of female population 45 to 64 years without bachelor’s degree.\nThe following ggplot describes the relationship between college enrollment and educational attainment of male/female populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100 - d03_024, y = c02_010),\n             color = 'blue')  +\n  geom_point(aes(x = 100 - d05_024, y = c02_010),\n             color = 'red')  +\n  geom_smooth(aes(x = 100 - d03_024, y = c02_010),\n              color = 'blue',  \n              method = lm)  +\n  geom_smooth(aes(x = 100 - d05_024, y = c02_010),\n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of population 45 to 64 years, the percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in college or graduate school.\nGiven the same level of the percentage of population enrolled in college or graduate school, male population 45 to 64 years without bachelor’s degree tends to be higher than female population 45 to 64 years without bachelor’s degree.\nThe following ggplot describes the relationship between public college enrollment and educational attainment of male/female populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100 - d03_024, y = c04_010),\n             color = 'blue')  +\n  geom_point(aes(x = 100 - d05_024, y = c04_010),\n             color = 'red')  +\n  geom_smooth(aes(x = 100 - d03_024, y = c04_010),\n              color = 'blue',  \n              method = lm)  +\n  geom_smooth(aes(x = 100 - d05_024, y = c04_010),\n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of population 45 to 64 years, the percentage of population 45 to 64 years without Bachelor’s degree is positively associated with the percentage of population enrolled in public college or graduate school.\nFor the same level of the percentage of population 45 to 64 years without Bachelor’s degree, a level of educational attainment of female population 45 to 64 years may be associated with a higher level of public college enrollment than that of male population 45 to 64 years.\nGiven the same level of the percentage of population enrolled in public college or graduate school, male population 45 to 64 years without bachelor’s degree tends to be higher than female population 45 to 64 years without bachelor’s degree.\nThe following ggplot describes the relationship between private college enrollment and educational attainment of male/female populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100 - d03_024, y = c06_010),\n             color = 'blue')  +\n  geom_point(aes(x = 100 - d05_024, y = c06_010),\n             color = 'red')  +\n  geom_smooth(aes(x = 100 - d03_024, y = c06_010),\n              color = 'blue',  \n              method = lm)  +\n  geom_smooth(aes(x = 100 - d05_024, y = c06_010),\n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of population 45 to 64 years, the percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in private college or graduate school.\nGiven the same level of the percentage of population enrolled in private college or graduate school, male population 45 to 64 years without bachelor’s degree tends to be higher than female population 45 to 64 years without bachelor’s degree."
  },
  {
    "objectID": "DANL200_lab1a.html#q2d",
    "href": "DANL200_lab1a.html#q2d",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2d",
    "text": "Q2d\nProvide both (1) ggplot codes and (2) a couple of sentences to describe how the relationships described in Q3b vary by gender of college enrollment.\n\nTo represent a level of male college enrollment of population in a county, I choose variable, c02_011, a percent of male population enrolled in college or graduate school.\nTo represent a level of female college enrollment of population in a county, I choose variable, c02_012, a percent of female population enrolled in college or graduate school.\nThe following ggplot describes the relationship between male/female college enrollment and educational attainment of populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100-d01_024, y = c02_011), \n             color = 'blue')  +\n  geom_point(aes(x = 100-d01_024, y = c02_012), \n             color = 'red')  +\n  geom_smooth(aes(x = 100-d01_024, y = c02_011), \n              color = 'blue',\n              method = lm) +\n  geom_smooth(aes(x = 100-d01_024, y = c02_012), \n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of college enrollment, the percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in college or graduate school.\nFor the same level of the percentage of population 45 to 64 years without Bachelor’s degree, a level of educational attainment of population 45 to 64 years may be associated with a higher level of female college enrollment than that of male college enrollment.\nGiven the same level of the percentage of population 45 to 64 years without bachelor’s degree, male population enrolled in college or graduate schools tends to be lower than female population enrolled in college or graduate schools.\nTo represent a level of male population in a county that are enrolled in public colleges, I choose variable, c04_011, a percent of male population enrolled in public college or graduate school.\nTo represent a level of female population in a county that are enrolled in public colleges, I choose variable, c04_012, a percent of female population enrolled in public college or graduate school.\nThe following ggplot describes the relationship between male/female public college enrollment and educational attainment of populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100-d01_024, y = c04_011), \n             color = 'blue')  +\n  geom_point(aes(x = 100-d01_024, y = c04_012), \n             color = 'red')  +\n  geom_smooth(aes(x = 100-d01_024, y = c04_011), \n              color = 'blue',\n              method = lm) +\n  geom_smooth(aes(x = 100-d01_024, y = c04_012), \n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nTo represent a level of male population in a county that are enrolled in private colleges, I choose variable, c06_011, a percent of male population enrolled in private college or graduate school.\nTo represent a level of male population in a county that are enrolled in private colleges, I choose variable, c06_012, a percent of female population enrolled in private college or graduate school.\nThe following ggplot describes the relationship between male/female public college enrollment and educational attainment of populations 45 to 64 years:\n\n\nggplot(NY_school_enrollment_socioecon) +\n  geom_point(aes(x = 100-d01_024, y = c06_011), \n             color = 'blue')  +\n  geom_point(aes(x = 100-d01_024, y = c06_012), \n             color = 'red')  +\n  geom_smooth(aes(x = 100-d01_024, y = c06_011), \n              color = 'blue',\n              method = lm) +\n  geom_smooth(aes(x = 100-d01_024, y = c06_012), \n              color = 'red',\n              method = lm) \n\n\n\n\n\n\n\nRegardless of a gender of college enrollment, the percentage of population 45 to 64 years without Bachelor’s degree is positively associated with the percentage of population enrolled in public college or graduate school.\nRegardless of a gender of college enrollment, the percentage of population 45 to 64 years without Bachelor’s degree is negatively associated with the percentage of population enrolled in private college or graduate school."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html",
    "href": "DANL200_midterm-fall-2022-a.html",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q1a.",
    "href": "DANL200_midterm-fall-2022-a.html#q1a.",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1a.",
    "text": "Q1a.\nMove the column fips to the first and remove column id.\n\nq1a <- county_data %>% \n  relocate(fips) %>% \n  select(-id)"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q1b",
    "href": "DANL200_midterm-fall-2022-a.html#q1b",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1b",
    "text": "Q1b\nProvide both (1) ggplot codes and (2) a simple comment to describe the probability distribution of african_american.\n\nggplot(county_data) + \n  geom_density(aes(x = african_american))\n\n\n\n\n\n\nThe distribution of the variable african_american is right-skewed.\nIt ranges from 0.00% to 85.00%.\nThe values around 2-3% are most likely for the value for african_american in US counties."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q1c",
    "href": "DANL200_midterm-fall-2022-a.html#q1c",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1c",
    "text": "Q1c\nProvide both (1) ggplot codes and (2) a simple comment to describe how the probability distribution of african_american varies by census_region.\n\nggplot(county_data) + \n  geom_density(aes(x = african_american)) +\n  facet_grid( . ~ census_region)\n\n\n\n\n\nThe variable african_american are likely to be higher in South and Northeast of census_region than Midwest and West of census_region."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q1d",
    "href": "DANL200_midterm-fall-2022-a.html#q1d",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1d",
    "text": "Q1d\nProvide both (1) ggplot codes and (2) a simple comment to describe the relationship between travel_time and hh_income.\n\nggplot(county_data,\n       aes(x = travel_time, y = hh_income)) +\n  geom_hex() + \n  geom_smooth() + geom_smooth(method = lm, color = 'red') \n\n\n\n\n\ntravel_time and hh_income are negatively associated overall.\n\nThey are negatively associated initially, and the relationship switches to be negative around the 22 minute of travel_time."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q1e",
    "href": "DANL200_midterm-fall-2022-a.html#q1e",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1e",
    "text": "Q1e\nProvide both (1) ggplot codes and (2) a simple comment to describe how the relationship between travel_time and hh_income varies by pop_dens.\n\nggplot(county_data,\n       aes(x = travel_time, y = hh_income)) +\n  geom_point(alpha = .1) + \n  geom_smooth() + geom_smooth(method = lm, color = 'red')  +\n  facet_grid(.~pop_dens)\n\n\n\n\n\nFor low level of pop_dens, travel_time and hh_income are negatively associated.\nFor high level of pop_dens, travel_time and hh_income are positively associated.\nThe relationship between travel_time and hh_income seems to become more positive as pop_dens increases."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q1f",
    "href": "DANL200_midterm-fall-2022-a.html#q1f",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1f",
    "text": "Q1f\nProvide both (1) ggplot codes and (2) a simple comment to describe how the relationship between travel_time and hh_income varies by pop_dens and census_region.\n\nggplot(county_data,\n       aes(x = travel_time, y = hh_income)) +\n  geom_point(alpha = .25) + \n  geom_smooth() + geom_smooth(method = lm, color = 'red')  +\n  facet_grid(census_region~pop_dens)\n\n\n\n\n\nOverall, the relationship described in Q1e holds across census_region."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#variable-description-1",
    "href": "DANL200_midterm-fall-2022-a.html#variable-description-1",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Variable Description",
    "text": "Variable Description\n\npid: playlist id\nplaylist_name: the name of the playlist\npos: the position of the track in the playlist (zero-based)\nartist_name: the name of the track’s primary artist\ntrack_name: the name of the track\nduration_ms: the duration of the track in milliseconds\nalbum_name: the name of the track’s album"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q2a",
    "href": "DANL200_midterm-fall-2022-a.html#q2a",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2a",
    "text": "Q2a\nFind the ten most popular song. Who are artists for those ten most popular song?\n\nq2a <- spotify_all %>% \n  count(artist_name, track_name) %>% \n  arrange(-n) %>% \n  head(10)\n\nq2a\n\n# A tibble: 10 × 3\n   artist_name      track_name                              n\n   <chr>            <chr>                               <int>\n 1 Drake            One Dance                             143\n 2 Kendrick Lamar   HUMBLE.                               142\n 3 The Chainsmokers Closer                                129\n 4 DRAM             Broccoli (feat. Lil Yachty)           127\n 5 Post Malone      Congratulations                       119\n 6 Migos            Bad and Boujee (feat. Lil Uzi Vert)   117\n 7 KYLE             iSpy (feat. Lil Yachty)               115\n 8 Lil Uzi Vert     XO TOUR Llif3                         113\n 9 Aminé            Caroline                              107\n10 Khalid           Location                              102\n\n\n\nThis example assumes that the most popular song is the song—a combination of artist_name and track_name—that most frequently appears in the data.frame spotify_all."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q2b",
    "href": "DANL200_midterm-fall-2022-a.html#q2b",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2b",
    "text": "Q2b\n\nFind the five most popular artist.\nWhat is the most popular song for each of the five most popular artist?\n\n\nq2b <- spotify_all %>% \n  group_by(artist_name) %>% \n  mutate(n_popular_artist = n()) %>% \n  ungroup() %>% \n  mutate( artist_ranking = dense_rank( desc(n_popular_artist) ) ) %>% \n  filter( artist_ranking <= 5) %>% \n  group_by(artist_name, track_name) %>% \n  mutate(n_popular_track = n()) %>% \n  group_by(artist_name) %>% \n  mutate(track_ranking = dense_rank( desc(n_popular_track) ) ) %>% \n  filter( track_ranking <= 2) %>%   # I just wanted to see the top two tracks for each artist\n  select(artist_name, artist_ranking, n_popular_artist, track_name, track_ranking, n_popular_track) %>% \n  distinct() %>% \n  arrange(artist_ranking, track_ranking)\n\nq2b\n\n# A tibble: 10 × 6\n# Groups:   artist_name [5]\n   artist_name    artist_ranking n_popular_artist track_name  track_ra…¹ n_pop…²\n   <chr>                   <int>            <int> <chr>            <int>   <int>\n 1 Drake                       1             2715 One Dance            1     143\n 2 Drake                       1             2715 Jumpman              2      95\n 3 Kanye West                  2             1065 Gold Digger          1      83\n 4 Kanye West                  2             1065 Stronger             2      62\n 5 Kendrick Lamar              3             1035 HUMBLE.              1     142\n 6 Kendrick Lamar              3             1035 DNA.                 2      76\n 7 Rihanna                     4              915 Needed Me            1      79\n 8 Rihanna                     4              915 Work                 2      65\n 9 The Weeknd                  5              913 Starboy              1     100\n10 The Weeknd                  5              913 The Hills            2      78\n# … with abbreviated variable names ¹​track_ranking, ²​n_popular_track\n\n\n\nThis example assumes that the most popular artist is the artist—the value of artist_name—that most frequently appears in the data.frame spotify_all.\nThis example assumes that the most popular song is the song—a combination of artist_name and track_name—that most frequently appears in the data.frame spotify_all."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q2c",
    "href": "DANL200_midterm-fall-2022-a.html#q2c",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2c",
    "text": "Q2c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between pos and the ten most popular artists.\n\nq2c <- spotify_all %>% \n  group_by(artist_name) %>% \n  mutate(n_popular_artist = n()) %>% \n  ungroup() %>% \n  mutate( artist_ranking = dense_rank( desc(n_popular_artist) ) ) %>% \n  filter( artist_ranking <= 10) \n  \n# boxplot\nggplot(q2c,\n       aes(x = pos, y = fct_reorder(artist_name, -artist_ranking)) ) +\n  geom_boxplot() +\n  stat_summary(\n    fun = mean,\n    color = 'red'\n  )\n\n\n\n# density\nggplot(q2c) +\n  geom_density(aes(x = pos)) + \n  facet_grid(fct_reorder(artist_name, artist_ranking) ~ .  , switch = \"y\") +\n  theme(strip.text.y.left = element_text(angle = 0))\n\n\n\n# histogram\nggplot(q2c) +\n  geom_histogram(aes(x = pos), binwidth = 1) + \n  facet_grid(fct_reorder(artist_name, artist_ranking) ~ .  , switch = \"y\") +\n  theme(strip.text.y.left = element_text(angle = 0))\n\n\n\n\n\nThe relationship between pos and the ten most popular artists can be described by how the distribution of pos varies across the ten most popular artists.\n\nThe distribution of pos does not seem to vary a lot across the ten most popular artists.\nAnything noticeable can be mentioned."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q2d",
    "href": "DANL200_midterm-fall-2022-a.html#q2d",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2d",
    "text": "Q2d\nCreate the data frame with pid-artist level of observations with the following four variables:\n\npid: playlist id\nplaylist_name: the name of the playlist\nartist: the name of the track’s primary artist, which appears only once within a playlist\nn_artist: the number of occurrences of artist within a playlist\n\n\nq2d <- spotify_all %>% \n  count(pid, playlist_name, artist_name) %>% \n  rename(n_artist = n) %>% \n  arrange(pid, -n_artist, artist_name)\n\nq2d\n\n# A tibble: 113,794 × 4\n     pid playlist_name artist_name        n_artist\n   <dbl> <chr>         <chr>                 <int>\n 1     0 Throwbacks    Jesse McCartney           4\n 2     0 Throwbacks    Chris Brown               3\n 3     0 Throwbacks    Justin Bieber             3\n 4     0 Throwbacks    Avril Lavigne             2\n 5     0 Throwbacks    Beyoncé                   2\n 6     0 Throwbacks    Boys Like Girls           2\n 7     0 Throwbacks    Destiny's Child           2\n 8     0 Throwbacks    Miley Cyrus               2\n 9     0 Throwbacks    Ne-Yo                     2\n10     0 Throwbacks    The Pussycat Dolls        2\n# … with 113,784 more rows"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q3a",
    "href": "DANL200_midterm-fall-2022-a.html#q3a",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3a",
    "text": "Q3a\n\nDownload the compressed file, ca_housing.zip, from the Files section in our Canvas web-site.\nExtract the file, ca_housing.zip, so that you can use the file, california_housing.csv.\nRead the data file, california_housing.csv, as the data.frame object with the name, ca_housing, using (1) the read_csv() function and (2) the absolute path name of the file, california_housing.csv, from your local hard disk drive in your laptop.\n\n\nca_housing <- read_csv(\n  '/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/california_housing.csv'\n)"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q3b.",
    "href": "DANL200_midterm-fall-2022-a.html#q3b.",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3b.",
    "text": "Q3b.\nReport the mean, median, minimum, maximum, and standard deviation for the variable, medianHouseValue, in the data.frame, ca_housing.\n\nskim(ca_housing$medianHouseValue)\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nca_housing$medianHouseVal…\n\n\n\n\nNumber of rows\n\n\n20640\n\n\n\n\nNumber of columns\n\n\n1\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n1\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\ndata\n\n\n0\n\n\n1\n\n\n206855.8\n\n\n115395.6\n\n\n14999\n\n\n119600\n\n\n179700\n\n\n264725\n\n\n500001\n\n\n▅▇▅▂▂"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-a.html#q3c.",
    "href": "DANL200_midterm-fall-2022-a.html#q3c.",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3c.",
    "text": "Q3c.\nCalculate the correlation between housingMedianAge and medianHouseValue.\n\ncor(ca_housing$housingMedianAge, ca_housing$medianHouseValue)\n\n[1] 0.1056234"
  },
  {
    "objectID": "DANL210_lab4a.html",
    "href": "DANL210_lab4a.html",
    "title": "Python Lab 4 - Data Concatenates and Merges Example Answers",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab4a.html#q1a",
    "href": "DANL210_lab4a.html#q1a",
    "title": "Python Lab 4 - Data Concatenates and Merges Example Answers",
    "section": "Q1a",
    "text": "Q1a\nWrite a Pandas code to join the two given DataFrames along rows and assign all data.\n\nq1a = pd.concat([student_data1, student_data2])\n\nq1a\n\n  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n0         S4  Scarlette Fisher    201\n1         S5  Carla Williamson    200\n2         S6       Dante Morse    198\n3         S7    Kaiser William    219\n4         S8   Madeeha Preston    201"
  },
  {
    "objectID": "DANL210_lab4a.html#q1b",
    "href": "DANL210_lab4a.html#q1b",
    "title": "Python Lab 4 - Data Concatenates and Merges Example Answers",
    "section": "Q1b",
    "text": "Q1b\nWrite a Pandas code to join the two given DataFrames along columns and assign all data.\n\nq1b = pd.concat([student_data1, student_data2], axis = 1)\n\nq1b\n\n  student_id              name  marks student_id              name  marks\n0         S1  Danniella Fenton    200         S4  Scarlette Fisher    201\n1         S2      Ryder Storey    210         S5  Carla Williamson    200\n2         S3      Bryce Jensen    190         S6       Dante Morse    198\n3         S4         Ed Bernal    222         S7    Kaiser William    219\n4         S5       Kwame Morin    199         S8   Madeeha Preston    201"
  },
  {
    "objectID": "DANL210_lab4a.html#q1c",
    "href": "DANL210_lab4a.html#q1c",
    "title": "Python Lab 4 - Data Concatenates and Merges Example Answers",
    "section": "Q1c",
    "text": "Q1c\nConsider the following Pandas Series\n\ns6 = pd.Series(['S6', 'Scarlette Fisher', 205], index=['student_id', 'name', 'marks'])\n\nWrite a Pandas code to append rows to DataFrame student_data1 and display the combined data using DATAFRAME.append(SERIES, ignore_index = True)\n\nq1c = student_data1.append(s6, ignore_index = True)\n\n<string>:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\nq1c\n\n  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n5         S6  Scarlette Fisher    205"
  },
  {
    "objectID": "DANL210_lab4a.html#q2a",
    "href": "DANL210_lab4a.html#q2a",
    "title": "Python Lab 4 - Data Concatenates and Merges Example Answers",
    "section": "Q2a",
    "text": "Q2a\nMerge flights with weather.\n\npath = '/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/flights.csv'\nflights = pd.read_csv(path)\n\npath = '/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/weather.csv'\nweather = pd.read_csv(path)\n\nflights.columns\n\nIndex(['year', 'month', 'day', 'dep_time', 'sched_dep_time', 'dep_delay',\n       'arr_time', 'sched_arr_time', 'arr_delay', 'carrier', 'flight',\n       'tailnum', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute',\n       'time_hour'],\n      dtype='object')\n\nweather.columns\n\nIndex(['origin', 'year', 'month', 'day', 'hour', 'temp', 'dewp', 'humid',\n       'wind_dir', 'wind_speed', 'wind_gust', 'precip', 'pressure', 'visib',\n       'time_hour'],\n      dtype='object')\n\nflights = flights.merge(weather,\n                        on = ['year', 'month', 'day', \n                              'hour', 'origin'],\n                        how = 'left')\nflights.columns\n\nIndex(['year', 'month', 'day', 'dep_time', 'sched_dep_time', 'dep_delay',\n       'arr_time', 'sched_arr_time', 'arr_delay', 'carrier', 'flight',\n       'tailnum', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute',\n       'time_hour_x', 'temp', 'dewp', 'humid', 'wind_dir', 'wind_speed',\n       'wind_gust', 'precip', 'pressure', 'visib', 'time_hour_y'],\n      dtype='object')"
  },
  {
    "objectID": "DANL210_lab4a.html#q2b",
    "href": "DANL210_lab4a.html#q2b",
    "title": "Python Lab 4 - Data Concatenates and Merges Example Answers",
    "section": "Q2b",
    "text": "Q2b\nFind the airline that has the longest positive dep_delay on average.\n\npath = '/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/airlines.csv'\nairlines = pd.read_csv(path)\n\n\nflights = flights.merge(airlines,\n                        on = ['carrier'],\n                        how = 'left')\n\nq2b = (\n       flights\n       .query('dep_delay >0')\n       .groupby(['carrier', 'name'])['dep_delay']\n       .mean()\n       .sort_values(ascending = False)\n       )\n       \nq2b\n\ncarrier  name                       \nOO       SkyWest Airlines Inc.          58.000000\nYV       Mesa Airlines Inc.             52.952790\nEV       ExpressJet Airlines Inc.       50.329790\n9E       Endeavor Air Inc.              48.920006\nF9       Frontier Airlines Inc.         45.137830\nMQ       Envoy Air                      44.915328\nHA       Hawaiian Airlines Inc.         44.840580\nFL       AirTran Airways Corporation    40.825877\nB6       JetBlue Airways                39.794218\nDL       Delta Air Lines Inc.           37.400236\nAA       American Airlines Inc.         37.169258\nWN       Southwest Airlines Co.         34.857426\nVX       Virgin America                 34.454831\nUS       US Airways Inc.                33.050681\nAS       Alaska Airlines Inc.           31.340708\nUA       United Air Lines Inc.          29.926195\nName: dep_delay, dtype: float64"
  },
  {
    "objectID": "DANL210_lab4a.html#q2c",
    "href": "DANL210_lab4a.html#q2c",
    "title": "Python Lab 4 - Data Concatenates and Merges Example Answers",
    "section": "Q2c",
    "text": "Q2c\nFind the airline that has the largest proportion of flights with longer than 30-minute dep_delay.\n\n\nq2c = (\n       flights\n       .assign(delay = lambda x: np.where(x.dep_delay > 30, 1, 0))\n       .groupby(['carrier', 'name'])\n       .agg(prop = ('delay', lambda x: x.mean()))\n       .sort_values(by='prop', ascending=False)\n       )\n\nq2c\n\n                                         prop\ncarrier name                                 \nEV      ExpressJet Airlines Inc.     0.215144\nYV      Mesa Airlines Inc.           0.191348\nF9      Frontier Airlines Inc.       0.189781\n9E      Endeavor Air Inc.            0.176436\nFL      AirTran Airways Corporation  0.167791\nWN      Southwest Airlines Co.       0.165377\nOO      SkyWest Airlines Inc.        0.156250\nB6      JetBlue Airways              0.153949\nMQ      Envoy Air                    0.141304\nUA      United Air Lines Inc.        0.129839\nVX      Virgin America               0.112941\nAA      American Airlines Inc.       0.108069\nDL      Delta Air Lines Inc.         0.103554\nAS      Alaska Airlines Inc.         0.088235\nUS      US Airways Inc.              0.078545\nHA      Hawaiian Airlines Inc.       0.046784"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html",
    "href": "DANL200_midterm-spring-2023-q.html",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#variable-description",
    "href": "DANL200_midterm-spring-2023-q.html#variable-description",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Variable Description",
    "text": "Variable Description\n\nArea:\n\nCountry\n\nYear:\n\nYear\n\nSSA:\n\nTRUE if Area belongs to Sub-Saharan Africa;\nFALSE otherwise\n\ngdp_per_capita:\n\nGross domestic product per capita, PPP, (constant 2017 international $)\n\ndrinking_water:\n\nPercentage of population using at least basic drinking water services (percent)\n\nsanitation_service:\n\nPercentage of population using at least basic sanitation services (percent)\n\nchildren_stunted:\n\nPercentage of children under 5 years of age who are stunted (percent)\nWhat does it mean stunted?\n\nStunting is the impaired growth and development that children experience from poor nutrition, repeated infection, and inadequate psychosocial stimulation.\n\n\nchildren_overweight:\n\nPercentage of children under 5 years of age who are overweight (percent)\n\ninvestment_pct:\n\nInvestment share of gross domestic product (GDP), (percent)\ninvestment_pct = 100 * I/GDP, where GDP = (Consumption) + (Investment) + (Government Expenditure) + (Net Export)\nIt measures how much of the new value added in the economy is invested."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q1a.",
    "href": "DANL200_midterm-spring-2023-q.html#q1a.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q1a.",
    "text": "Q1a.\n\nFor each SSA value, calculate the mean value of each numeric variable.\nFor each numeric variable, how much is the difference between Sub-Saharan Africa and non-Sub-Saharan Africa in their mean values?"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q1b",
    "href": "DANL200_midterm-spring-2023-q.html#q1b",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q1b",
    "text": "Q1b\nProvide both (1) ggplot code and (2) a simple comment to describe how the relationship between investment_pct and children_stunted varies by SSA."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q1c",
    "href": "DANL200_midterm-spring-2023-q.html#q1c",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q1c",
    "text": "Q1c\nProvide both (1) ggplot code and (2) a simple comment to describe how the relationship between investment_pct and children_stunted varies by SSA and Year."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q1d.",
    "href": "DANL200_midterm-spring-2023-q.html#q1d.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q1d.",
    "text": "Q1d.\nProvide both (1) ggplot code and (2) a simple comment to describe how overall the yearly trend of each Area’s children_stunted varies by SSA."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#variable-description-1",
    "href": "DANL200_midterm-spring-2023-q.html#variable-description-1",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Variable Description",
    "text": "Variable Description\n\nCAMIS:\n\nThis is an unique identifier for the entity (restaurant);\n10-digit integer\n\nDBA:\n\nThis field represents the name (doing business as) of the entity (restaurant);\nPublic business name, may change at discretion of restaurant owner\n\nBORO:\n\nBorough in which the entity (restaurant) is located.;\n• 1 = MANHATTAN\n• 2 = BRONX\n• 3 = BROOKLYN\n• 4 = QUEENS\n• 5 = STATEN ISLAND\n• 0 = Missing;\n\nCUISINE DESCRIPTION:\n\nThis field describes the entity (restaurant) cuisine.\n\nACTION:\n\nThis field represents the actions that is associated with each restaurant inspection. ;\n• Violations were cited in the following area(s).\n• No violations were recorded at the time of this inspection.\n• Establishment re-opened by DOHMH\n• Establishment re-closed by DOHMH\n• Establishment Closed by DOHMH.\n• Violations were cited in the following area(s) and those requiring immediate action were addressed.\n\nVIOLATION CODE:\n\nViolation code associated with an establishment (restaurant) inspection\n\nVIOLATION DESCRIPTION:\n\nViolation description associated with an establishment (restaurant) inspection\n\nCRITICAL FLAG:\n\nIndicator of critical violation;\n• Critical\n• Not Critical\n• Not Applicable;\nCritical violations are those most likely to contribute to food-borne illness\n\nSCORE:\n\nTotal score for a particular inspection;\n\nGRADE:\n\nGrade associated with the inspection;\n• N = Not Yet Graded\n• A = Grade A\n• B = Grade B\n• C = Grade C\n• Z = Grade Pending\n• P = Grade Pending issued on re-opening following an initial inspection that resulted in a closure"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q2a.",
    "href": "DANL200_midterm-spring-2023-q.html#q2a.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2a.",
    "text": "Q2a.\nWhat are the mean, standard deviation, first quartile, median, third quartile, and maximum of SCORE for each GRADE of restaurants?"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q2b.",
    "href": "DANL200_midterm-spring-2023-q.html#q2b.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2b.",
    "text": "Q2b.\n\nHow many restaurants with a GRADE of A are there in NYC?\nHow much percentage of restaurants in NYC are a GRADE of C?"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q2c.",
    "href": "DANL200_midterm-spring-2023-q.html#q2c.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2c.",
    "text": "Q2c.\nProvide both (1) ggplot code and (2) a simple comment to describe how the distribution of SCORE varies by GRADE and CRITICAL FLAG."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q2d.",
    "href": "DANL200_midterm-spring-2023-q.html#q2d.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2d.",
    "text": "Q2d.\nProvide both (1) ggplot code and (2) a simple comment to describe how the probability distribution of CRITICAL FLAG varies by GRADE and BORO."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q2e.",
    "href": "DANL200_midterm-spring-2023-q.html#q2e.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2e.",
    "text": "Q2e.\nFor the 10 most common CUISINE DESCRIPTION values, find the CUISINE DESCRIPTION value that has the highest proportion of GRADE A."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q2f.",
    "href": "DANL200_midterm-spring-2023-q.html#q2f.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2f.",
    "text": "Q2f.\n\nFind the 3 most common names of restaurants (DBA) in each BORO.\n\nIf the third most common DBA values are multiple, please include all the DBA values.\n\nOverall, which DBA value is most common in NYC?"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-q.html#q2g.",
    "href": "DANL200_midterm-spring-2023-q.html#q2g.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2g.",
    "text": "Q2g.\nFor all the DBA values that appear in the result of Q2f, find the DBA value that is most likely to commit critical violation."
  },
  {
    "objectID": "DANL200_hw1a.html",
    "href": "DANL200_hw1a.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "",
    "text": "Step 1. Download the compressed ZIP file, ny_colleges.zip, from the Homework Assignment 1 in our Canvas.\nStep 2. Extract the file, ny_colleges.zip, so that you can access the file, NY_colleges.csv, for Homework Assignment 1.\n\n\n\n\n\n\n\n(1) median debt of college students,\n\n\nnet price of college\n\n\nnumber of college students,\n\n\ncollege majors\n\n\n\nwith a variety of segmentation.\n\nA description of each variable in NY_colleges.csv is provided in ny_colleges.yaml, which we can open in RStudio or other text editors.\nA description of values of each variable in NY_colleges.csv is provided in columns, VARIABLE_NAME, VALUE, and LABEL, in ny_colleges_vars.xlsx, which we can open in Microsoft Excel.\nUsing Ctrl + F (cmd + F for mac users) would be useful to find the variable description.\nI recommend you to copy and paste the description of the variable you use in your R code to your R script with a comment (# …).\nFrom the file, ny_colleges_vars.xlsx, we can find that\n\nCIP**BACHL, whose value is either 0, 1, or 2, indicates whether a university offers a bachelor’s degree in SOME MAJOR.\n\n0: Program not offered\n1: Program offered\n2: Program offered through an exclusively distance-education program"
  },
  {
    "objectID": "DANL200_hw1a.html#q1a",
    "href": "DANL200_hw1a.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1a",
    "text": "Q1a\nRead the data file, NY_colleges.csv, as the data.frame object with the name, NY_colleges, using (1) the read_csv() function and (2) the absolute path name of the file NY_colleges.csv from your local hard disk drive in your laptop.\n\n# The following path is from Byeong-Hak's local hard disk drive.\npath <- \"/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/ny_colleges/NY_colleges.csv\"  \n\nNY_colleges <- read_csv(path)"
  },
  {
    "objectID": "DANL200_hw1a.html#q1b",
    "href": "DANL200_hw1a.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1b",
    "text": "Q1b\nWhat are the mean, median1, minimum, maximum, and standard deviation for each of the following variables?\n\n\naverage net price of public institution;\n\n\naverage net price of private institution;\n\n\nmedian debt for students who have completed;\n\n\nmedian debt for students who have not completed.\n\n\n\nlibrary(skimr)\nskim(NY_colleges$NPT4_PUB) # it does not give us a meaningful summary.\n\n\nData summary\n\n\nName\nNY_colleges$NPT4_PUB\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndata\n0\n1\n4\n5\n0\n422\n0\n\n\n\n\n                           # this is because NPT4_PUB is character type.\n\n# as.numeric() can convert any variable type into a numeric type!\nNY_colleges$NPT4_PUB <- as.numeric( NY_colleges$NPT4_PUB ) \nNY_colleges$NPT4_PRIV <- as.numeric( NY_colleges$NPT4_PRIV )\n\n\nAfter converting the variables into numeric types, we can get the summary statistics for NPT4_PUB and NPT4_PRIV.\n\n\nskim(NY_colleges$NPT4_PUB)\n\n\nData summary\n\n\nName\nNY_colleges$NPT4_PUB\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n4347\n0.09\n11809.64\n5126.4\n1814\n6476\n13412\n15926\n20616\n▅▃▃▇▃\n\n\n\n\nskim(NY_colleges$NPT4_PRIV)\n\n\nData summary\n\n\nName\nNY_colleges$NPT4_PRIV\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n3209\n0.33\n19859\n9714.22\n1053\n12001\n20085\n26628.5\n54902\n▆▇▇▂▁\n\n\n\n\n\n\nThe following is the summary statistics for GRAD_DEBT_MDN and WDRAW_DEBT_MDN.\n\n\nskim(NY_colleges$GRAD_DEBT_MDN)\n\n\nData summary\n\n\nName\nNY_colleges$GRAD_DEBT_MDN\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n1545\n0.68\n17174.19\n6073.82\n2500\n13625\n17125\n21600\n39467\n▂▇▅▂▁\n\n\n\n\nskim(NY_colleges$WDRAW_DEBT_MDN)\n\n\nData summary\n\n\nName\nNY_colleges$WDRAW_DEBT_MD…\n\n\nNumber of rows\n4776\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n1555\n0.67\n6929.58\n2872.37\n1050\n5216\n6500\n8750\n25979\n▇▇▁▁▁"
  },
  {
    "objectID": "DANL200_hw1a.html#instruction-for-q1c-q1i",
    "href": "DANL200_hw1a.html#instruction-for-q1c-q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Instruction for Q1c-Q1i",
    "text": "Instruction for Q1c-Q1i\nFrom Q1c to Q1g, provide both (1) ggplot codes and (2) a couple of sentences to answer the questions."
  },
  {
    "objectID": "DANL200_hw1a.html#q1c",
    "href": "DANL200_hw1a.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1c",
    "text": "Q1c\nCompare between public institutions and private institutions in terms of the distribution of average net price.\n\nggplot( data = NY_colleges ) + \n  geom_freqpoly( mapping = aes( x = NPT4_PUB),\n                 color = \"blue\",\n                 bins = 50 ) +   # blue for public type\n  geom_freqpoly( mapping = aes( x = NPT4_PRIV),\n                 color = \"red\",\n                 bins = 50  ) +  # red for private type\n\n\n\n\n\n\n\nPrivate institutions tend to have a higher average net price than public ones."
  },
  {
    "objectID": "DANL200_hw1a.html#q1d",
    "href": "DANL200_hw1a.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1d",
    "text": "Q1d\nCompare between public institutions and private institutions in terms of the relationship between (1) average net price and (2) median debt for students who have completed.\n\nggplot( data = NY_colleges,\n        mapping = aes( y = GRAD_DEBT_MDN ) ) + \n  geom_point( mapping = aes( x = NPT4_PUB ),\n              color = \"blue\",\n              alpha = .2 ) +   # blue for public type+\n  geom_smooth( mapping = aes( x = NPT4_PUB ),\n               color = \"blue\", \n               method = lm ) +   # blue for public type\n  geom_point( mapping = aes( x = NPT4_PRIV),\n              color = \"red\",\n              alpha = .2  ) +  # red for private type\n  geom_smooth( mapping = aes( x = NPT4_PRIV),\n               color = \"red\", \n               method = lm ) \n\n\n\n\n\n\n\nThe association between average net price and median debt is stronger in public than in private.\nThe levels of average net price and median debt tend to be lower in public than in private."
  },
  {
    "objectID": "DANL200_hw1a.html#q1e",
    "href": "DANL200_hw1a.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1e",
    "text": "Q1e\nHow does the relationship between (1) average net price and (2) median debt for students who have completed vary by levels of family income?\n\nggplot( data = NY_colleges,\n        mapping = aes( y = LO_INC_DEBT_MDN ) ) + \n  geom_point( mapping = aes( x = NPT4_PUB ),\n              color = \"blue\",\n              alpha = .2 ) +   # blue for public type+\n  geom_smooth( mapping = aes( x = NPT4_PUB ),\n               color = \"blue\", \n               method = lm ) +   # blue for public type\n  geom_point( mapping = aes( x = NPT4_PRIV),\n              color = \"red\",\n              alpha = .2  ) +  # red for private type\n  geom_smooth( mapping = aes( x = NPT4_PRIV),\n               color = \"red\", \n               method = lm )\n\n\n\n\n\n\n\nggplot( data = NY_colleges,\n        mapping = aes( y = MD_INC_DEBT_MDN ) ) + \n  geom_point( mapping = aes( x = NPT4_PUB ),\n              color = \"blue\",\n              alpha = .2 ) +   # blue for public type+\n  geom_smooth( mapping = aes( x = NPT4_PUB ),\n               color = \"blue\", \n               method = lm ) +   # blue for public type\n  geom_point( mapping = aes( x = NPT4_PRIV),\n              color = \"red\",\n              alpha = .2  ) +  # red for private type\n  geom_smooth( mapping = aes( x = NPT4_PRIV),\n               color = \"red\", \n               method = lm )\n\n\n\n\n\n\n\nggplot( data = NY_colleges,\n        mapping = aes( y = HI_INC_DEBT_MDN ) ) + \n  geom_point( mapping = aes( x = NPT4_PUB ),\n              color = \"blue\",\n              alpha = .2 ) +   # blue for public type+\n  geom_smooth( mapping = aes( x = NPT4_PUB ),\n               color = \"blue\", \n               method = lm ) +   # blue for public type\n  geom_point( mapping = aes( x = NPT4_PRIV),\n              color = \"red\",\n              alpha = .2  ) +  # red for private type\n  geom_smooth( mapping = aes( x = NPT4_PRIV),\n               color = \"red\", \n               method = lm )\n\n\n\n\n\n\n\nRegardless of the levels of family income, the association between average net price and median debt is stronger in public than in private.\nRegardless of the levels of family income, the levels of average net price and median debt tend to be lower in public than in private."
  },
  {
    "objectID": "DANL200_hw1a.html#q1f",
    "href": "DANL200_hw1a.html#q1f",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1f",
    "text": "Q1f\nCompare between public institutions and private institutions in terms of the proportion of high-income students.\n\nggplot( data = NY_colleges ) + \n  geom_histogram( mapping = aes( x = HI_INC_YR4_N / OVERALL_YR4_N ),\n                  binwidth = .015 ) +\n  facet_wrap( .~ as.factor(CONTROL) )\n\n\n\n\n\n\n\nggplot( data = NY_colleges ) + \n  geom_density( mapping = aes( x = HI_INC_YR4_N / OVERALL_YR4_N )\n                   ) +\n  facet_wrap( .~ as.factor(CONTROL) )\n\n\n\n\n\n\n\nPrivate nonprofit tends to have the larger proportion of high-income students than public and private for-profit."
  },
  {
    "objectID": "DANL200_hw1a.html#q1g",
    "href": "DANL200_hw1a.html#q1g",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1g",
    "text": "Q1g\nDescribe how the number of students in a college varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services.\n\nggplot( data = NY_colleges) +\n  geom_density( mapping = aes(x = OVERALL_YR4_N ) ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\nggplot( data = NY_colleges) +\n  geom_histogram( mapping = aes(x = OVERALL_YR4_N ),\n                  binwidth = 200 ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\n\nggplot( data = NY_colleges ) +\n  geom_boxplot(mapping = aes(y = factor(CIP52BACHL),\n                             x = OVERALL_YR4_N,\n                             fill = factor(CIP52BACHL) ),\n                ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nLog transformation can be useful when a distribution of a variable of interest is highly skewed.\n\nIn such case, log transformation make the distribution more normal-ish.\n\n\n\nggplot( data = NY_colleges) +\n  geom_density( mapping = aes(x = log(OVERALL_YR4_N) ) ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\nggplot( data = NY_colleges) +\n  geom_histogram( mapping = aes(x = log(OVERALL_YR4_N) ) ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\n\nggplot( data = NY_colleges ) +\n  geom_boxplot(mapping = aes(y = factor(CIP52BACHL),\n                             x = log(OVERALL_YR4_N),\n                             fill = factor(CIP52BACHL) ),\n                ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nSchools that offer a business major tend to have more students than schools that do not offer one."
  },
  {
    "objectID": "DANL200_hw1a.html#q1h",
    "href": "DANL200_hw1a.html#q1h",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1h",
    "text": "Q1h\nDescribe how the median debt for students who have completed varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services.\n\nggplot( data = NY_colleges ) +\n  geom_density( mapping = aes(x = GRAD_DEBT_MDN ) ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\nggplot( data = NY_colleges) +\n  geom_histogram( mapping = aes(x = GRAD_DEBT_MDN ),\n                  binwidth = 200 ) +\n  facet_grid( factor(CIP52BACHL) ~ .)\n\n\nggplot( data = NY_colleges ) +\n  geom_boxplot(mapping = aes(y = factor(CIP52BACHL),\n                             x = GRAD_DEBT_MDN,\n                             fill = factor(CIP52BACHL) ),\n               ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nThe levels of median debt tends to be larger for schools that offers a business major online exclusively."
  },
  {
    "objectID": "DANL200_hw1a.html#q1i",
    "href": "DANL200_hw1a.html#q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1i",
    "text": "Q1i\nDescribe how the yearly trend of the median debt for students varies by levels of family income.\n\nggplot( NY_colleges,\n              aes(x = as.factor(YEAR) ) ) +\n  geom_boxplot(fill = 'red',\n               aes( y = LO_INC_DEBT_MDN) ) +\n  ylim(c(0,30000))\n\n\n\n\n\n\n\nggplot( NY_colleges,\n              aes(x = as.factor(YEAR) ) ) +\n  geom_boxplot(fill = 'green',\n               aes( y = MD_INC_DEBT_MDN) ) +\n  ylim(c(0,30000)) \n\n\n\n\n\n\n\nggplot( NY_colleges,\n              aes(x = as.factor(YEAR) ) ) +\n  geom_boxplot(fill = 'blue',\n               aes( y = HI_INC_DEBT_MDN) ) +\n  ylim(c(0,30000)) \n\n\n\n\n\n\n\nThe median debt from low-income students tends to increase less than that from median-income and high-income students."
  },
  {
    "objectID": "DANL310_midterm-spring-2023-q.html",
    "href": "DANL310_midterm-spring-2023-q.html",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam",
    "section": "",
    "text": "library(knitr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(socviz)\nlibrary(ggthemes)\nlibrary(hrbrthemes)\nlibrary(gapminder)"
  },
  {
    "objectID": "DANL310_lab1q.html",
    "href": "DANL310_lab1q.html",
    "title": "R Visualization Lab 1 - Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(socviz)"
  },
  {
    "objectID": "DANL310_lab1q.html#variable-description",
    "href": "DANL310_lab1q.html#variable-description",
    "title": "R Visualization Lab 1 - Maps",
    "section": "Variable Description",
    "text": "Variable Description\n\nbelief:\n\nhuman: Estimated percentage who think that global warming is caused mostly by human activities.\nhappening: Estimated percentage who think that global warming is happening.\n\n\n\nclimate_opinion_long <- read_csv(\n  'https://bcdanl.github.io/data/climate_opinion_2021.csv')\n\nRows: 6284 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): GeoName, belief\ndbl (2): id, perc\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nclimate_opinion_long\n\n# A tibble: 6,284 × 4\n      id GeoName                 belief     perc\n   <dbl> <chr>                   <chr>     <dbl>\n 1  1001 Autauga County, Alabama happening  59.2\n 2  1001 Autauga County, Alabama human      45.5\n 3  1003 Baldwin County, Alabama happening  60.5\n 4  1003 Baldwin County, Alabama human      44.7\n 5  1005 Barbour County, Alabama happening  68.1\n 6  1005 Barbour County, Alabama human      50.5\n 7  1007 Bibb County, Alabama    happening  57.6\n 8  1007 Bibb County, Alabama    human      42.6\n 9  1009 Blount County, Alabama  happening  52.5\n10  1009 Blount County, Alabama  human      41.5\n# … with 6,274 more rows"
  },
  {
    "objectID": "DANL310_lab1q.html#q1a.",
    "href": "DANL310_lab1q.html#q1a.",
    "title": "R Visualization Lab 1 - Maps",
    "section": "Q1a.",
    "text": "Q1a.\n\nFilter climate_opinion_long, so that climate_opinion_long has only estimated percentage who think that global warming is caused mostly by human activities.\nThen join the two data.frames socviz::county_map and the resulting data.frame above.\n\n\n\nJoining, by = \"id\""
  },
  {
    "objectID": "DANL310_lab1q.html#q1b.",
    "href": "DANL310_lab1q.html#q1b.",
    "title": "R Visualization Lab 1 - Maps",
    "section": "Q1b.",
    "text": "Q1b.\n\nReplicate the following map.\n\nDo not use coord_map(projection = \"albers\", lat0 = 39, lat1 = 45)."
  },
  {
    "objectID": "DANL310_hw2q.html",
    "href": "DANL310_hw2q.html",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2",
    "section": "",
    "text": "Add a web-page of the team project proposal to your website. The tab menu to link the web-page must be provided."
  },
  {
    "objectID": "DANL310_hw2q.html#q2a.",
    "href": "DANL310_hw2q.html#q2a.",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2",
    "section": "Q2a.",
    "text": "Q2a.\nUse the following data.frame for Q2a, Q2b, and Q2c.\n\nhdi_corruption <- read_csv(\n  'https://bcdanl.github.io/data/hdi_corruption.csv')"
  },
  {
    "objectID": "DANL310_hw2q.html#q2b",
    "href": "DANL310_hw2q.html#q2b",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\n\nDownload the file labor_supply.zip from the Data folder in the Files section in Canvas. Then, extract labor_supply.zip, so that you can access the labor_supply.csv file.\nVariable description in labor_supply.csv\n\nSEX: 1 if Male; 2 if Female; 9 if NIU (Not in universe)\nNCHLT5: Number of own children under age 5 in a household; 9 if 9+\nLABFORCE: 0 if NIU or members of the armed forces; 1 if not in the labor force; 2 if in the labor force.\nASECWT: sample weight\n\nA sample weight of each observation means how much population each observation represents.\n\nIf you sum ASECWT for each year, you get the size of yearly population in the US.\n\nHouseholds with LABFORCE == 0 is not in labor force.\nLabor force participation rate can be calculated by:\n\n\\[\n(\\text{Labor Force Participation Rate}) \\, = \\, \\frac{(\\text{Size of population in labor force})}{(\\text{Size of civilian population that are not members of the armed force})}\n\\]"
  },
  {
    "objectID": "DANL310_hw2q.html#q2c",
    "href": "DANL310_hw2q.html#q2c",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 2",
    "section": "Q2c",
    "text": "Q2c\n\nlibrary(ggcorrplot) # to create correlation heatmaps using ggcorrplot()\n\nbeer_mkt <- read_csv('https://bcdanl.github.io/data/beer_markets.csv')\n\n\nMake a correlation heat-map with variables that are either strongly correlated or promo-related.\nThe variables are selected by how high the mean value of the absolute value of correlations with the variable is (top 13-15 variables).\nYou can start with the following data.frame:\n\n\nbeer_dummies <- beer_mkt %>% select(-hh, -market) \nreg <- lm(data = beer_dummies,\n          beer_floz ~ .)\nbeer_dummies <-  as.data.frame(model.matrix(reg))[, -1]\nbeer_dummies <- cbind(beer_mkt$beer_floz ,beer_dummies)\nbeer_dummies <- beer_dummies %>% \n  rename(beer_floz = `beer_mkt$beer_floz`)\n\n\nTo calculate a correlation between numeric variables in data.frame, use cor(data.frame)\n\n\n\n\n\n\n\n\n\n\n\nThen, make a correlation heat-map for NY markets with the same selection of variables.\nNY markets are such that whose market value is either ALBANY, BUFFALO-ROCHESTER, URBAN NY, SUBURBAN NY, EXURBAN NY, RURAL NEW YORK, or SYRACUSE.\n\n\n\n\n\n\n\n\n\n\nReferences:\n\nFundamentals of Data Visualization by Claus O. Wilke\nData Visualization: A practical introduction by Kieran Healy"
  },
  {
    "objectID": "DANL210_lab8q.html",
    "href": "DANL210_lab8q.html",
    "title": "Python Lab 8 - Web-scrapping 3",
    "section": "",
    "text": "Go to the following website for web scrapping\n\nhttp://quotes.toscrape.com\n\n\n\n\n\n\nProvide your Python Selenium code to scrape all the quotes.\n\nYou should create the two DataFrames with the following variables:\n\n\nDataFrame about each quote\n\n\nquote\nauthor\ntags\nabout, URL for description about each author.\n\n\nDataFrame about each author\n\n\nabout, URL for description about each author.\nborn_date\nborn_location\nauthor_description\nSave the two DataFrames in the CSV files."
  },
  {
    "objectID": "DANL210_lab5a.html",
    "href": "DANL210_lab5a.html",
    "title": "Python Lab 5 - Dates and Times",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab5a.html#q1a",
    "href": "DANL210_lab5a.html#q1a",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q1a",
    "text": "Q1a\nAdd the new variables, closing_quarter and closing_year, to the DataFrame banks. - closing_quarter: the quarter in which the bank closed (1, 2, 3, or 4) - closing_year: the year in which the bank closed\n\n\nbanks = banks.assign(\n  closing_quarter = banks['Closing Date'].dt.quarter,\n  closing_year = banks['Closing Date'].dt.year\n)\n\nbanks\n\n\n\n\n\n  \n    \n      \n      Bank Name\n      City\n      State\n      Cert\n      Acquiring Institution\n      Closing Date\n      Fund\n      closing_quarter\n      closing_year\n    \n  \n  \n    \n      0\n      Signature Bank\n      New York\n      NY\n      57053\n      Flagstar Bank, N.A.\n      2023-03-12\n      10540\n      1\n      2023\n    \n    \n      1\n      Silicon Valley Bank\n      Santa Clara\n      CA\n      24735\n      First–Citizens Bank & Trust Company\n      2023-03-10\n      10539\n      1\n      2023\n    \n    \n      2\n      Almena State Bank\n      Almena\n      KS\n      15426\n      Equity Bank\n      2020-10-23\n      10538\n      4\n      2020\n    \n    \n      3\n      First City Bank of Florida\n      Fort Walton Beach\n      FL\n      16748\n      United Fidelity Bank, fsb\n      2020-10-16\n      10537\n      4\n      2020\n    \n    \n      4\n      The First State Bank\n      Barboursville\n      WV\n      14361\n      MVB Bank, Inc.\n      2020-04-03\n      10536\n      2\n      2020\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      560\n      Superior Bank, FSB\n      Hinsdale\n      IL\n      32646\n      Superior Federal, FSB\n      2001-07-27\n      6004\n      3\n      2001\n    \n    \n      561\n      Malta National Bank\n      Malta\n      OH\n      6629\n      North Valley Bank\n      2001-05-03\n      4648\n      2\n      2001\n    \n    \n      562\n      First Alliance Bank & Trust Co.\n      Manchester\n      NH\n      34264\n      Southern New Hampshire Bank & Trust\n      2001-02-02\n      4647\n      1\n      2001\n    \n    \n      563\n      National State Bank of Metropolis\n      Metropolis\n      IL\n      3815\n      Banterra Bank of Marion\n      2000-12-14\n      4646\n      4\n      2000\n    \n    \n      564\n      Bank of Honolulu\n      Honolulu\n      HI\n      21029\n      Bank of the Orient\n      2000-10-13\n      4645\n      4\n      2000\n    \n  \n\n565 rows × 9 columns"
  },
  {
    "objectID": "DANL210_lab5a.html#q1b",
    "href": "DANL210_lab5a.html#q1b",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q1b",
    "text": "Q1b\nCount the number of banks that were closed for each pair of year-quarter.\n\nclosing_year_q = (\n  banks\n  .groupby(['closing_year', 'closing_quarter'])['Bank Name']\n  .agg(['size'])\n)\n\nclosing_year_q\n\n\n\n\n\n  \n    \n      \n      \n      size\n    \n    \n      closing_year\n      closing_quarter\n      \n    \n  \n  \n    \n      2000\n      4\n      2\n    \n    \n      2001\n      1\n      1\n    \n    \n      2\n      1\n    \n    \n      3\n      2\n    \n    \n      2002\n      1\n      6\n    \n    \n      2\n      2\n    \n    \n      3\n      1\n    \n    \n      4\n      2\n    \n    \n      2003\n      1\n      1\n    \n    \n      2\n      1\n    \n    \n      4\n      1\n    \n    \n      2004\n      1\n      3\n    \n    \n      2\n      1\n    \n    \n      2007\n      1\n      1\n    \n    \n      3\n      1\n    \n    \n      4\n      1\n    \n    \n      2008\n      1\n      2\n    \n    \n      2\n      2\n    \n    \n      3\n      9\n    \n    \n      4\n      12\n    \n    \n      2009\n      1\n      21\n    \n    \n      2\n      24\n    \n    \n      3\n      50\n    \n    \n      4\n      45\n    \n    \n      2010\n      1\n      41\n    \n    \n      2\n      45\n    \n    \n      3\n      41\n    \n    \n      4\n      30\n    \n    \n      2011\n      1\n      26\n    \n    \n      2\n      22\n    \n    \n      3\n      26\n    \n    \n      4\n      18\n    \n    \n      2012\n      1\n      16\n    \n    \n      2\n      15\n    \n    \n      3\n      12\n    \n    \n      4\n      8\n    \n    \n      2013\n      1\n      4\n    \n    \n      2\n      12\n    \n    \n      3\n      6\n    \n    \n      4\n      2\n    \n    \n      2014\n      1\n      5\n    \n    \n      2\n      7\n    \n    \n      3\n      2\n    \n    \n      4\n      4\n    \n    \n      2015\n      1\n      4\n    \n    \n      2\n      1\n    \n    \n      3\n      1\n    \n    \n      4\n      2\n    \n    \n      2016\n      1\n      1\n    \n    \n      2\n      2\n    \n    \n      3\n      2\n    \n    \n      2017\n      1\n      3\n    \n    \n      2\n      3\n    \n    \n      4\n      2\n    \n    \n      2019\n      2\n      1\n    \n    \n      4\n      3\n    \n    \n      2020\n      1\n      1\n    \n    \n      2\n      1\n    \n    \n      4\n      2\n    \n    \n      2023\n      1\n      2"
  },
  {
    "objectID": "DANL210_lab5a.html#q1c",
    "href": "DANL210_lab5a.html#q1c",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q1c",
    "text": "Q1c\nProvide both seaborn code and a simple comment to describe the quarterly trend of bank failure.\n\nclosing_year_q.plot()\n\n<AxesSubplot:xlabel='closing_year,closing_quarter'>\n\n\n\n\n\n\nclosing_year_q = closing_year_q.reset_index()\n\nclosing_year_q['date_q'] = pd.to_datetime(\n    closing_year_q['closing_year'].astype('str') + '-Q' + closing_year_q['closing_quarter'].astype('str')\n    )\n\nclosing_year_q\n\nsns.lineplot(data = closing_year_q,\n             x = 'date_q',\n             y = 'size')\n\n<AxesSubplot:xlabel='date_q', ylabel='size'>"
  },
  {
    "objectID": "DANL210_lab5a.html#q2a",
    "href": "DANL210_lab5a.html#q2a",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q2a",
    "text": "Q2a\nAdd a variable, date_dt, which is a datetime type of Date variable, to the stock DataFrame.\n\nstock['date_dt'] = pd.to_datetime(stock['Date'])"
  },
  {
    "objectID": "DANL210_lab5a.html#q2b",
    "href": "DANL210_lab5a.html#q2b",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q2b",
    "text": "Q2b\n\nFor each year, find the two dates, for which\n\nTSLA’s Close was the highest of the year.\nTSLA’s Close was the lowest of the year.\n\n\n\nTSLA = stock.query('company == \"TSLA\"')[['date_dt', 'Close']]\n\nTSLA['year'] = TSLA['date_dt'].dt.year\n\nTSLA['ranking'] = TSLA.groupby(['year'])['Close'].rank(method='dense', ascending=False)\n\nTSLA['lowest'] = TSLA.groupby(['year'])['ranking'].transform('max')\n\nq2b_h = TSLA.query('ranking == 1')\nq2b_h\n\nq2b_l = TSLA.query('ranking == lowest')\nq2b_l\n\n\n\n\n\n  \n    \n      \n      date_dt\n      Close\n      year\n      ranking\n      lowest\n    \n  \n  \n    \n      15421\n      2013-01-11\n      2.194000\n      2013\n      249.0\n      249.0\n    \n    \n      15673\n      2014-01-13\n      9.289333\n      2014\n      246.0\n      246.0\n    \n    \n      15976\n      2015-03-27\n      12.333333\n      2015\n      245.0\n      245.0\n    \n    \n      16196\n      2016-02-10\n      9.578000\n      2016\n      247.0\n      247.0\n    \n    \n      16422\n      2017-01-03\n      14.466000\n      2017\n      245.0\n      245.0\n    \n    \n      16866\n      2018-10-08\n      16.704000\n      2018\n      249.0\n      249.0\n    \n    \n      17028\n      2019-06-03\n      11.931333\n      2019\n      249.0\n      249.0\n    \n    \n      17228\n      2020-03-18\n      24.081333\n      2020\n      253.0\n      253.0\n    \n    \n      17472\n      2021-03-08\n      187.666672\n      2021\n      251.0\n      251.0\n    \n    \n      17928\n      2022-12-27\n      109.099998\n      2022\n      250.0\n      250.0\n    \n    \n      17932\n      2023-01-03\n      108.099998\n      2023\n      51.0\n      51.0"
  },
  {
    "objectID": "DANL210_lab5a.html#q2c",
    "href": "DANL210_lab5a.html#q2c",
    "title": "Python Lab 5 - Dates and Times",
    "section": "Q2c",
    "text": "Q2c\n\nCalculate the gap between the two adjacent dates with the highest Close of the year.\nCalculate the gap between the two adjacent dates with the lowest Close of the year.\n\n\nq2b_h['date_lag'] = q2b_h['date_dt'].shift(1)\nq2b_h['diff'] = q2b_h['date_dt'] - q2b_h['date_lag']\nq2b_h\n\n/var/folders/07/nm9t4t294vb5jz6vtqnb6pxm0000gn/T/ipykernel_92603/2872517853.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/07/nm9t4t294vb5jz6vtqnb6pxm0000gn/T/ipykernel_92603/2872517853.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n  \n    \n      \n      date_dt\n      Close\n      year\n      ranking\n      lowest\n      date_lag\n      diff\n    \n  \n  \n    \n      15601\n      2013-09-30\n      12.891333\n      2013\n      1.0\n      249.0\n      NaT\n      NaT\n    \n    \n      15835\n      2014-09-04\n      19.069332\n      2014\n      1.0\n      246.0\n      2013-09-30\n      339 days\n    \n    \n      16054\n      2015-07-20\n      18.817333\n      2015\n      1.0\n      245.0\n      2014-09-04\n      319 days\n    \n    \n      16234\n      2016-04-06\n      17.694668\n      2016\n      1.0\n      247.0\n      2015-07-20\n      261 days\n    \n    \n      16600\n      2017-09-18\n      25.666668\n      2017\n      1.0\n      245.0\n      2016-04-06\n      530 days\n    \n    \n      16823\n      2018-08-07\n      25.304667\n      2018\n      1.0\n      249.0\n      2017-09-18\n      323 days\n    \n    \n      17172\n      2019-12-26\n      28.729334\n      2019\n      1.0\n      249.0\n      2018-08-07\n      506 days\n    \n    \n      17428\n      2020-12-31\n      235.223328\n      2020\n      1.0\n      253.0\n      2019-12-26\n      371 days\n    \n    \n      17641\n      2021-11-04\n      409.970001\n      2021\n      1.0\n      251.0\n      2020-12-31\n      308 days\n    \n    \n      17681\n      2022-01-03\n      399.926666\n      2022\n      1.0\n      250.0\n      2021-11-04\n      60 days\n    \n    \n      17962\n      2023-02-15\n      214.240005\n      2023\n      1.0\n      51.0\n      2022-01-03\n      408 days\n    \n  \n\n\n\n\n\nq2b_l['date_lag'] = q2b_l['date_dt'].shift(1)\nq2b_l['diff'] = q2b_l['date_dt'] - q2b_l['date_lag']\nq2b_l\n\n/var/folders/07/nm9t4t294vb5jz6vtqnb6pxm0000gn/T/ipykernel_92603/3853281973.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/07/nm9t4t294vb5jz6vtqnb6pxm0000gn/T/ipykernel_92603/3853281973.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n  \n    \n      \n      date_dt\n      Close\n      year\n      ranking\n      lowest\n      date_lag\n      diff\n    \n  \n  \n    \n      15421\n      2013-01-11\n      2.194000\n      2013\n      249.0\n      249.0\n      NaT\n      NaT\n    \n    \n      15673\n      2014-01-13\n      9.289333\n      2014\n      246.0\n      246.0\n      2013-01-11\n      367 days\n    \n    \n      15976\n      2015-03-27\n      12.333333\n      2015\n      245.0\n      245.0\n      2014-01-13\n      438 days\n    \n    \n      16196\n      2016-02-10\n      9.578000\n      2016\n      247.0\n      247.0\n      2015-03-27\n      320 days\n    \n    \n      16422\n      2017-01-03\n      14.466000\n      2017\n      245.0\n      245.0\n      2016-02-10\n      328 days\n    \n    \n      16866\n      2018-10-08\n      16.704000\n      2018\n      249.0\n      249.0\n      2017-01-03\n      643 days\n    \n    \n      17028\n      2019-06-03\n      11.931333\n      2019\n      249.0\n      249.0\n      2018-10-08\n      238 days\n    \n    \n      17228\n      2020-03-18\n      24.081333\n      2020\n      253.0\n      253.0\n      2019-06-03\n      289 days\n    \n    \n      17472\n      2021-03-08\n      187.666672\n      2021\n      251.0\n      251.0\n      2020-03-18\n      355 days\n    \n    \n      17928\n      2022-12-27\n      109.099998\n      2022\n      250.0\n      250.0\n      2021-03-08\n      659 days\n    \n    \n      17932\n      2023-01-03\n      108.099998\n      2023\n      51.0\n      51.0\n      2022-12-27\n      7 days"
  },
  {
    "objectID": "DANL200_hw1q.html",
    "href": "DANL200_hw1q.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "",
    "text": "Step 1. Download the compressed ZIP file, ny_colleges.zip, from the Homework Assignment 1 in our Canvas.\nStep 2. Extract the file, ny_colleges.zip, so that you can access the file, NY_colleges.csv, for Homework Assignment 1.\n\n\n\n\n\n\nNY_colleges.csv is university-year level data about\n\n\nmedian debt of college students,\n\n\nnet price of college\n\n\nnumber of college students,\n\n\ncollege majors\n\n\n\nwith a variety of segmentation.\n\nA description of each variable in NY_colleges.csv is provided in ny_colleges.yaml, which we can open in RStudio or other text editors.\nA description of values of each variable in NY_colleges.csv is provided in columns, VARIABLE_NAME, VALUE, and LABEL, in ny_colleges_vars.xlsx, which we can open in Microsoft Excel.\nUsing Ctrl + F (cmd + F for mac users) would be useful to find the variable description.\nI recommend you to copy and paste the description of the variable you use in your R code to your R script with a comment (# …).\nFrom the file, ny_colleges_vars.xlsx, we can find that\n\nCIP**BACHL, whose value is either 0, 1, or 2, indicates whether a university offers a bachelor’s degree in SOME MAJOR.\n\n0: Program not offered\n1: Program offered\n2: Program offered through an exclusively distance-education program"
  },
  {
    "objectID": "DANL200_hw1q.html#q1a",
    "href": "DANL200_hw1q.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1a",
    "text": "Q1a\nRead the data file, NY_colleges.csv, as the data.frame object with the name, NY_colleges, using (1) the read_csv() function and (2) the absolute path name of the file NY_colleges.csv from your local hard disk drive in your laptop."
  },
  {
    "objectID": "DANL200_hw1q.html#q1b",
    "href": "DANL200_hw1q.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1b",
    "text": "Q1b\nWhat are the mean, median1, minimum, maximum, and standard deviation for each of the following variables?\n\n\naverage net price of public institution;\n\n\naverage net price of private institution;\n\n\nmedian debt for students who have completed;\n\n\nmedian debt for students who have not completed."
  },
  {
    "objectID": "DANL200_hw1q.html#instruction-for-q1c-q1i",
    "href": "DANL200_hw1q.html#instruction-for-q1c-q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Instruction for Q1c-Q1i",
    "text": "Instruction for Q1c-Q1i\nFrom Q1c to Q1g, provide both (1) ggplot codes and (2) a couple of sentences to answer the questions."
  },
  {
    "objectID": "DANL200_hw1q.html#q1c",
    "href": "DANL200_hw1q.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1c",
    "text": "Q1c\nCompare between public institutions and private institutions in terms of the distribution of average net price."
  },
  {
    "objectID": "DANL200_hw1q.html#q1d",
    "href": "DANL200_hw1q.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1d",
    "text": "Q1d\nCompare between public institutions and private institutions in terms of the relationship between (1) average net price and (2) median debt for students who have completed."
  },
  {
    "objectID": "DANL200_hw1q.html#q1e",
    "href": "DANL200_hw1q.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1e",
    "text": "Q1e\nHow does the relationship between (1) average net price and (2) median debt for students who have completed vary by levels of family income?"
  },
  {
    "objectID": "DANL200_hw1q.html#q1f",
    "href": "DANL200_hw1q.html#q1f",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1f",
    "text": "Q1f\nCompare between public institutions and private institutions in terms of the proportion of high-income students."
  },
  {
    "objectID": "DANL200_hw1q.html#q1g",
    "href": "DANL200_hw1q.html#q1g",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1g",
    "text": "Q1g\nDescribe how the number of students in a college varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services."
  },
  {
    "objectID": "DANL200_hw1q.html#q1h",
    "href": "DANL200_hw1q.html#q1h",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1h",
    "text": "Q1h\nDescribe how the median debt for students who have completed varies by whether or not a college offers bachelor’s degree in Business, Management, Marketing, and Related Support Services."
  },
  {
    "objectID": "DANL200_hw1q.html#q1i",
    "href": "DANL200_hw1q.html#q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 1",
    "section": "Q1i",
    "text": "Q1i\nDescribe how the yearly trend of the median debt for students varies by levels of family income."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "B.H. CHOE",
    "section": "",
    "text": "Data Analytics at SUNY Geneseo\n\nByeong-Hak Choe\n\n\ndata visualization and machine learning for business and economics with python and r\nweb projects with quarto, r markdown, and python notebook\ndata preparation and management with python, r, sql, and web-api\nweb scrapping and automation with python selenium\nversion control with git and github"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html",
    "href": "DANL200_midterm-spring-2023-a.html",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#variable-description",
    "href": "DANL200_midterm-spring-2023-a.html#variable-description",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Variable Description",
    "text": "Variable Description\n\nArea:\n\nCountry\n\nYear:\n\nYear\n\nSSA:\n\nTRUE if Area belongs to Sub-Saharan Africa;\nFALSE otherwise\n\ngdp_per_capita:\n\nGross domestic product per capita, PPP, (constant 2017 international $)\n\ndrinking_water:\n\nPercentage of population using at least basic drinking water services (percent)\n\nsanitation_service:\n\nPercentage of population using at least basic sanitation services (percent)\n\nchildren_stunted:\n\nPercentage of children under 5 years of age who are stunted (percent)\nWhat does it mean stunted?\n\nStunting is the impaired growth and development that children experience from poor nutrition, repeated infection, and inadequate psychosocial stimulation.\n\n\nchildren_overweight:\n\nPercentage of children under 5 years of age who are overweight (percent)\n\ninvestment_pct:\n\nInvestment share of gross domestic product (GDP), (percent)\ninvestment_pct = 100 * I/GDP, where GDP = (Consumption) + (Investment) + (Government Expenditure) + (Net Export)\nIt measures how much of the new value added in the economy is invested."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q1a.",
    "href": "DANL200_midterm-spring-2023-a.html#q1a.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q1a.",
    "text": "Q1a.\n\nFor each SSA value, calculate the mean value of each numeric variable.\nFor each numeric variable, how much is the difference between Sub-Saharan Africa and non-Sub-Saharan Africa in their mean values?\n\n\nlibrary(skimr)\nq1a <- fao_stat %>% \n  group_by(SSA) %>% \n  skim() %>% \n  filter(skim_type == \"numeric\", skim_variable != 'Year') %>% \n  select(skim_variable, SSA, numeric.mean) %>% \n  group_by(skim_variable) %>% \n  mutate(diff = numeric.mean - lag(numeric.mean)) %>% \n  filter(!is.na(diff)) %>% \n  select(-numeric.mean, -SSA)"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q1b",
    "href": "DANL200_midterm-spring-2023-a.html#q1b",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q1b",
    "text": "Q1b\nProvide both (1) ggplot code and (2) a simple comment to describe how the relationship between investment_pct and children_stunted varies by SSA.\n\nggplot(fao_stat,\n       aes(x = investment_pct , y = children_stunted)) +\n  geom_point(alpha = .25) +\n  geom_smooth() +\n  geom_smooth(method = lm, color = 'red') +\n  facet_wrap(. ~ SSA) +\n  coord_fixed()\n\n\n\n\n\nFor non-Sub-Saharan countries, investment_pct and children_stunted seem to be positively associated.\nFor Sub-Saharan countries, investment_pct and children_stunted seem to be negatively associated."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q1c",
    "href": "DANL200_midterm-spring-2023-a.html#q1c",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q1c",
    "text": "Q1c\nProvide both (1) ggplot code and (2) a simple comment to describe how the relationship between investment_pct and children_stunted varies by SSA and Year.\n\nggplot(fao_stat,\n       aes(x = investment_pct , y = children_stunted)) +\n  geom_point(alpha = .25) +\n  geom_smooth() +\n  geom_smooth(method = lm, color = 'red') +\n  facet_grid(Year ~ SSA) \n\n\n\n\n\nFor non-Sub-Saharan countries, the relationship between investment_pct and children_stunted seems to vary over the years.\nFor Sub-Saharan countries, the relationship between investment_pct and children_stunted seems to be negatively associated over the years."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q1d.",
    "href": "DANL200_midterm-spring-2023-a.html#q1d.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q1d.",
    "text": "Q1d.\nProvide both (1) ggplot code and (2) a simple comment to describe how overall the yearly trend of each Area’s children_stunted varies by SSA.\n\nggplot(fao_stat,\n       aes(x = Year, y = children_stunted)) +\n  geom_line(aes(group = Area), alpha = .2) +\n  geom_smooth(method = lm) +\n  facet_grid(.~SSA)\n\n\n\n\n\nOverall, children_stunted in both non-Sub-Saharan countries and Sub-Saharan countries have decreased over the years.\nOverall, children_stunted in Sub-Saharan countries has always been higher than in non-Sub-Saharan countries."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#variable-description-1",
    "href": "DANL200_midterm-spring-2023-a.html#variable-description-1",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Variable Description",
    "text": "Variable Description\n\nCAMIS:\n\nThis is an unique identifier for the entity (restaurant);\n10-digit integer\n\nDBA:\n\nThis field represents the name (doing business as) of the entity (restaurant);\nPublic business name, may change at discretion of restaurant owner\n\nBORO:\n\nBorough in which the entity (restaurant) is located.;\n• 1 = MANHATTAN\n• 2 = BRONX\n• 3 = BROOKLYN\n• 4 = QUEENS\n• 5 = STATEN ISLAND\n• 0 = Missing;\n\nCUISINE DESCRIPTION:\n\nThis field describes the entity (restaurant) cuisine.\n\nACTION:\n\nThis field represents the actions that is associated with each restaurant inspection. ;\n• Violations were cited in the following area(s).\n• No violations were recorded at the time of this inspection.\n• Establishment re-opened by DOHMH\n• Establishment re-closed by DOHMH\n• Establishment Closed by DOHMH.\n• Violations were cited in the following area(s) and those requiring immediate action were addressed.\n\nVIOLATION CODE:\n\nViolation code associated with an establishment (restaurant) inspection\n\nVIOLATION DESCRIPTION:\n\nViolation description associated with an establishment (restaurant) inspection\n\nCRITICAL FLAG:\n\nIndicator of critical violation;\n• Critical\n• Not Critical\n• Not Applicable;\nCritical violations are those most likely to contribute to food-borne illness\n\nSCORE:\n\nTotal score for a particular inspection;\n\nGRADE:\n\nGrade associated with the inspection;\n• N = Not Yet Graded\n• A = Grade A\n• B = Grade B\n• C = Grade C\n• Z = Grade Pending\n• P = Grade Pending issued on re-opening following an initial inspection that resulted in a closure"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q2a.",
    "href": "DANL200_midterm-spring-2023-a.html#q2a.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2a.",
    "text": "Q2a.\nWhat are the mean, standard deviation, first quartile, median, third quartile, and maximum of SCORE for each GRADE of restaurants?\n\nrestaurant %>% group_by(GRADE) %>% skim(SCORE)\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nPiped data\n\n\n\n\nNumber of rows\n\n\n17633\n\n\n\n\nNumber of columns\n\n\n12\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n1\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nGRADE\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nGRADE\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nSCORE\n\n\nA\n\n\n0\n\n\n1\n\n\n9.26\n\n\n3.42\n\n\n0\n\n\n7\n\n\n10\n\n\n12\n\n\n13\n\n\n▁▂▂▅▇\n\n\n\n\nSCORE\n\n\nB\n\n\n0\n\n\n1\n\n\n21.03\n\n\n4.16\n\n\n0\n\n\n18\n\n\n21\n\n\n24\n\n\n36\n\n\n▁▁▇▇▁\n\n\n\n\nSCORE\n\n\nC\n\n\n0\n\n\n1\n\n\n38.56\n\n\n10.83\n\n\n0\n\n\n31\n\n\n36\n\n\n44\n\n\n86\n\n\n▁▇▇▂▁"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q2b.",
    "href": "DANL200_midterm-spring-2023-a.html#q2b.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2b.",
    "text": "Q2b.\n\nHow many restaurants with a GRADE of A are there in NYC?\nHow much percentage of restaurants in NYC are a GRADE of C?\n\n\nfreq <- as.data.frame( table(restaurant$GRADE) )\nprop <- as.data.frame( 100 * prop.table(table(restaurant$GRADE)) )"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q2c.",
    "href": "DANL200_midterm-spring-2023-a.html#q2c.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2c.",
    "text": "Q2c.\nProvide both (1) ggplot code and (2) a simple comment to describe how the distribution of SCORE varies by GRADE and CRITICAL FLAG.\n\nDensity plots\n\n\nggplot(restaurant) +\n  geom_density(aes(x = SCORE)) +\n  facet_grid( `CRITICAL FLAG` ~ GRADE, scales = 'free_x' )\n\n\n\n\n\nBoxplots\n\n\nggplot(restaurant) +\n  geom_boxplot(aes(x = SCORE, y = GRADE, fill = GRADE) ) +\n  facet_grid( `CRITICAL FLAG` ~ . )\n\n\n\n\n\nHistograms\n\n\nggplot(restaurant) +\n  geom_histogram(aes(x = SCORE), binwidth = 1 ) +\n  facet_grid( `CRITICAL FLAG` ~ GRADE, scales = 'free_x' )\n\n\n\n\n\nMostly,\n\nThe values of SCORE for GRADE A ranges from 0 to 13.\nThe values of SCORE for GRADE B ranges 13 to 27.\nThe values of SCORE for GRADE C ranges 24 to 75.\n\nFor Not Critical type, two SCORE values around 1 and 12 are most common, while 12 is the single most common SCORE value for Critical type."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q2d.",
    "href": "DANL200_midterm-spring-2023-a.html#q2d.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2d.",
    "text": "Q2d.\nProvide both (1) ggplot code and (2) a simple comment to describe how the probability distribution of CRITICAL FLAG varies by GRADE and BORO.\n\nggplot(restaurant) +\n  geom_bar(aes(x = `CRITICAL FLAG`,\n               y = after_stat(prop), group = 1)) +\n  facet_grid( GRADE ~ BORO )\n\n\n\n\n\nFor GRADE A, the probability distribution of CRITICAL FLAG are similar across BOROs.\nFor GRADE B, the restaurants in Staten Island are more likely to be Critical than in other BOROs.\nFor GRADE C, the restaurants in Bronx are more likely to be Critical than in other BOROs."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q2e.",
    "href": "DANL200_midterm-spring-2023-a.html#q2e.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2e.",
    "text": "Q2e.\nFor the 10 most common CUISINE DESCRIPTION values, find the CUISINE DESCRIPTION value that has the highest proportion of GRADE A.\n\nq2e <- restaurant %>% \n  group_by(`CUISINE DESCRIPTION`) %>% \n  mutate(n = n()) %>% \n  ungroup() %>% \n  filter(dense_rank(-n) <= 10) %>% \n  group_by(`CUISINE DESCRIPTION`, GRADE) %>% \n  count() %>% \n  group_by(`CUISINE DESCRIPTION`) %>% \n  mutate(prop_A = n / sum(n)) %>% \n  filter(GRADE == 'A') %>% \n  arrange(-prop_A)"
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q2f.",
    "href": "DANL200_midterm-spring-2023-a.html#q2f.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2f.",
    "text": "Q2f.\n\nFind the 3 most common names of restaurants (DBA) in each BORO.\n\nIf the third most common DBA values are multiple, please include all the DBA values.\n\nOverall, which DBA value is most common in NYC?\n\n\nq2f <- restaurant %>% \n  select(DBA, BORO) %>% \n  group_by(BORO, DBA) %>% \n  summarize(n = n()) %>% \n  mutate(ranking = dense_rank(-n)) %>% \n  filter(ranking <= 3) %>% \n  arrange(BORO, ranking)\n\nq2f_ <- restaurant %>% \n  group_by(DBA) %>% \n  count() %>% \n  arrange(-n)\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\nNote that chipotle mexican grill and subway are both the third most popular franchise/chain in Manhattan.\nOverall, dunkin is the most popular franchise/chain in NYC."
  },
  {
    "objectID": "DANL200_midterm-spring-2023-a.html#q2g.",
    "href": "DANL200_midterm-spring-2023-a.html#q2g.",
    "title": "Spring 2023, DANL 200: Introduction to Data Analytics",
    "section": "Q2g.",
    "text": "Q2g.\nFor all the DBA values that appear in the result of Q2f, find the DBA value that is most likely to commit critical violation.\n\nq2g <- restaurant %>% \n  filter(DBA %in% q2f$DBA) %>% \n  group_by(DBA, `CRITICAL FLAG`) %>% \n  count() %>% \n  group_by(DBA) %>% \n  mutate(lag_n = lag(n),\n         tot = sum(n),\n         prop_crit = lag_n / tot) %>% \n  select(DBA, prop_crit) %>% \n  filter(!is.na(prop_crit)) %>% \n  arrange(-prop_crit)\n\n\n\n\n\n  \n\n\n\n\nAmong popular franchises/chains, subway is most likely to commit Critical violation in NYC."
  },
  {
    "objectID": "DANL210_lab4q.html",
    "href": "DANL210_lab4q.html",
    "title": "Python Lab 4 - Data Concatenates and Merges",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_lab4q.html#q1a",
    "href": "DANL210_lab4q.html#q1a",
    "title": "Python Lab 4 - Data Concatenates and Merges",
    "section": "Q1a",
    "text": "Q1a\nWrite a Pandas code to join the two given DataFrames along rows and assign all data."
  },
  {
    "objectID": "DANL210_lab4q.html#q1b",
    "href": "DANL210_lab4q.html#q1b",
    "title": "Python Lab 4 - Data Concatenates and Merges",
    "section": "Q1b",
    "text": "Q1b\nWrite a Pandas code to join the two given DataFrames along columns and assign all data."
  },
  {
    "objectID": "DANL210_lab4q.html#q1c",
    "href": "DANL210_lab4q.html#q1c",
    "title": "Python Lab 4 - Data Concatenates and Merges",
    "section": "Q1c",
    "text": "Q1c\nConsider the following Pandas Series\n\ns6 = pd.Series(['S6', 'Scarlette Fisher', 205], index=['student_id', 'name', 'marks'])\n\nWrite a Pandas code to append rows to DataFrame student_data1 and display the combined data using DATAFRAME.append(SERIES, ignore_index = True)"
  },
  {
    "objectID": "DANL210_lab4q.html#q2a",
    "href": "DANL210_lab4q.html#q2a",
    "title": "Python Lab 4 - Data Concatenates and Merges",
    "section": "Q2a",
    "text": "Q2a\nMerge flights with weather."
  },
  {
    "objectID": "DANL210_lab4q.html#q2b",
    "href": "DANL210_lab4q.html#q2b",
    "title": "Python Lab 4 - Data Concatenates and Merges",
    "section": "Q2b",
    "text": "Q2b\nFind the airline that has the longest positive dep_delay on average."
  },
  {
    "objectID": "DANL210_lab4q.html#q2c",
    "href": "DANL210_lab4q.html#q2c",
    "title": "Python Lab 4 - Data Concatenates and Merges",
    "section": "Q2c",
    "text": "Q2c\nFind the airline that has the largest proportion of flights with longer than 30-minute dep_delay."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html",
    "href": "DANL200_midterm-fall-2022-q.html",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q1a.",
    "href": "DANL200_midterm-fall-2022-q.html#q1a.",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1a.",
    "text": "Q1a.\nMove the column fips to the first and remove column id."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q1b",
    "href": "DANL200_midterm-fall-2022-q.html#q1b",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1b",
    "text": "Q1b\nProvide both (1) ggplot codes and (2) a simple comment to describe the probability distribution of african_american."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q1c",
    "href": "DANL200_midterm-fall-2022-q.html#q1c",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1c",
    "text": "Q1c\nProvide both (1) ggplot codes and (2) a simple comment to describe how the probability distribution of african_american varies by census_region."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q1d",
    "href": "DANL200_midterm-fall-2022-q.html#q1d",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1d",
    "text": "Q1d\nProvide both (1) ggplot codes and (2) a simple comment to describe the relationship between travel_time and hh_income."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q1e",
    "href": "DANL200_midterm-fall-2022-q.html#q1e",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1e",
    "text": "Q1e\nProvide both (1) ggplot codes and (2) a simple comment to describe how the relationship between travel_time and hh_income varies by pop_dens."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q1f",
    "href": "DANL200_midterm-fall-2022-q.html#q1f",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q1f",
    "text": "Q1f\nProvide both (1) ggplot codes and (2) a simple comment to describe how the relationship between travel_time and hh_income varies by pop_dens and census_region."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#variable-description-1",
    "href": "DANL200_midterm-fall-2022-q.html#variable-description-1",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Variable Description",
    "text": "Variable Description\n\npid: playlist id\nplaylist_name: the name of the playlist\npos: the position of the track in the playlist (zero-based)\nartist_name: the name of the track’s primary artist\ntrack_name: the name of the track\nduration_ms: the duration of the track in milliseconds\nalbum_name: the name of the track’s album"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q2a",
    "href": "DANL200_midterm-fall-2022-q.html#q2a",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2a",
    "text": "Q2a\nFind the ten most popular song. Who are artists for those ten most popular song?"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q2b",
    "href": "DANL200_midterm-fall-2022-q.html#q2b",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2b",
    "text": "Q2b\n\nFind the five most popular artist.\nWhat is the most popular song for each of the five most popular artist?"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q2c",
    "href": "DANL200_midterm-fall-2022-q.html#q2c",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2c",
    "text": "Q2c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between pos and the ten most popular artists."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q2d",
    "href": "DANL200_midterm-fall-2022-q.html#q2d",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q2d",
    "text": "Q2d\nCreate the data frame with pid-artist level of observations with the following four variables:\n\npid: playlist id\nplaylist_name: the name of the playlist\nartist: the name of the track’s primary artist, which appears only once within a playlist\nn_artist: the number of occurrences of artist within a playlist"
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q3a",
    "href": "DANL200_midterm-fall-2022-q.html#q3a",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3a",
    "text": "Q3a\n\nDownload the compressed file, ca_housing.zip, from the Files section in our Canvas web-site.\nExtract the file, ca_housing.zip, so that you can use the file, california_housing.csv.\nRead the data file, california_housing.csv, as the data.frame object with the name, ca_housing, using (1) the read_csv() function and (2) the absolute path name of the file, california_housing.csv, from your local hard disk drive in your laptop."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q3b.",
    "href": "DANL200_midterm-fall-2022-q.html#q3b.",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3b.",
    "text": "Q3b.\nReport the mean, median, minimum, maximum, and standard deviation for the variable, medianHouseValue, in the data.frame, ca_housing."
  },
  {
    "objectID": "DANL200_midterm-fall-2022-q.html#q3c.",
    "href": "DANL200_midterm-fall-2022-q.html#q3c.",
    "title": "Fall 2022, DANL 200: Introduction to Data Analytics",
    "section": "Q3c.",
    "text": "Q3c.\nCalculate the correlation between housingMedianAge and medianHouseValue."
  },
  {
    "objectID": "DANL200_lab1q.html",
    "href": "DANL200_lab1q.html",
    "title": "R Lab 1 - ggplot visualization",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab1q.html#q1a",
    "href": "DANL200_lab1q.html#q1a",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1a",
    "text": "Q1a\nRead the data file, bikeshare_cleaned.csv, as the data.frame object with the name, bikeshare, using (1) the read_csv() function and (2) its URL, https://bcdanl.github.io/data/bikeshare_cleaned.csv.\n\n\n\nUse the data.frame bikeshare for the rest of questions in Question 1.\n\n\nDescription of variables in the data file, bikeshare_cleaned.csv\nThe data set, bikeshare_cleaned.csv, includes 17376 observations of hourly counts from 2011 to 2012 for bike rides (rentals) in Washington D.C.\n\ncnt: count of total bikes rented out\nyear: year\nmonth: month\ndate: date\nhr: hours\nwkday: week day\nholiday: holiday if holiday == 1; non-holiday otherwise\nseasons: season\nweather_cond: weather condition\ntemp: temperature, measured in standard deviations from average.\nhum: humidity, measured in standard deviations from average.\nwindspeed: wind speed, measured in standard deviations from average."
  },
  {
    "objectID": "DANL200_lab1q.html#q1b",
    "href": "DANL200_lab1q.html#q1b",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1b",
    "text": "Q1b\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of cnt."
  },
  {
    "objectID": "DANL200_lab1q.html#q1c",
    "href": "DANL200_lab1q.html#q1c",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1c",
    "text": "Q1c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of cnt by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1d",
    "href": "DANL200_lab1q.html#q1d",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1d",
    "text": "Q1d\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of temp by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1e",
    "href": "DANL200_lab1q.html#q1e",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1e",
    "text": "Q1e\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of hum by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1f",
    "href": "DANL200_lab1q.html#q1f",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1f",
    "text": "Q1f\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the distribution of windspeed by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1g",
    "href": "DANL200_lab1q.html#q1g",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1g",
    "text": "Q1g\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between temp and cnt."
  },
  {
    "objectID": "DANL200_lab1q.html#q1h",
    "href": "DANL200_lab1q.html#q1h",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1h",
    "text": "Q1h\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between temp and cnt by year and month."
  },
  {
    "objectID": "DANL200_lab1q.html#q1i",
    "href": "DANL200_lab1q.html#q1i",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1i",
    "text": "Q1i\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between weather_cond and cnt."
  },
  {
    "objectID": "DANL200_lab1q.html#q1j",
    "href": "DANL200_lab1q.html#q1j",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1j",
    "text": "Q1j\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between weather_cond and cnt by hr."
  },
  {
    "objectID": "DANL200_lab1q.html#q1k",
    "href": "DANL200_lab1q.html#q1k",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1k",
    "text": "Q1k\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between wkday and cnt."
  },
  {
    "objectID": "DANL200_lab1q.html#q1l",
    "href": "DANL200_lab1q.html#q1l",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q1l",
    "text": "Q1l\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between wkday and cnt by hr."
  },
  {
    "objectID": "DANL200_lab1q.html#q2a",
    "href": "DANL200_lab1q.html#q2a",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2a",
    "text": "Q2a\nRead the data file, NY_school_enrollment_socioecon.csv, as the data.frame object with the name, NY_school_enrollment_socioecon, using (1) the read_csv() function and (2) its URL, https://bcdanl.github.io/data/NY_school_enrollment_socioecon.csv.\n\n\n\nFor description of variables in NY_school_enrollment_socioecon, refer to the file, ny_school_enrollment_socioecon_description.zip, which is in the Files section in our Canvas web-page. (I recommend you to extract the zip file, and then read the file, ny_school_enrollment_socioecon_description.csv, using Excel or Numbers.)\n\nHere are some details about the data.frame, NY_school_enrollment_socioecon:\nThe geographic and time units of observation (row) in the data.frame, NY_school_enrollment_socioecon, are New York county and year.\n\n\n\n\n\n \n  \n    FIPS \n    year \n    county_name \n    pincp \n    c01_001 \n    c02_002 \n  \n \n\n  \n    36001 \n    2015 \n    Albany \n    55793 \n    84463 \n    4.7 \n  \n\n\n\n\n\n\nFor example, the observation above means that in Albany county in year 2015 …\n\nAverage personal income of people is $55,793.\nPopulation 3 years and over enrolled in school is 84,463.\nPercent of population 3 years and over enrolled in nursery school and preschool is 4.7%.\n\nThe following is sample observations from Bronx and Livingston counties:\n\n\n\n\n\n\n\n\n\n\n\nThe following describes the variables:\n\nc01_010: Total!!Population enrolled in college or graduate school\nSo, c01_010 is total population enrolled in college or graduate school;\nc02_010: Percent!!Population enrolled in college or graduate school\nSo, c02_010 is a percent of total population enrolled in college or graduate school;\nIn which county is more likely for a person to be enrolled in a college or graduate school?\n\nA county’s college enrollment level can be represented by an overall tendency of that county’s residents to be enrolled in college (as long as we are interested in analyzing how human behaves overall).\nThe size of a county’s population enrolled in college or graduate school (c01_010) may not be appropriate to represent a county’s college enrollment level.\n\nA county’s larger size of population enrolled in college does not necessarily mean people in people in that county are likely to be enrolled in college.\n\nConsider the following example:\n\n\n\n\n\n \n  \n    County \n    Total.Population \n    Bachelor.s.Degree \n    High.School \n    Percent.of.Bachelor.s.Degree \n    Percent.of.High.School \n  \n \n\n  \n    A \n    100,000 \n    1,000 \n    99,000 \n    1.0% \n    99.0% \n  \n  \n    B \n    1,000 \n    999 \n    1 \n    99.9% \n    0.1% \n  \n\n\n\n\n\n\nAlthough County A has the larger number of people that have bachelor’s degrees than County B, it is more appropriate to say that people in County B have a higher college enrollment than people in County A.\nThis is because the overall tendency of County B’s people to attend college is stronger than that of County A’s people.\nSimilarly, to represent a standard of living of people in a country, we do not use a country’s gross domestic product (GDP) but its GDP per capita (GDP per capita is GDP devided by population).\n\nFor example, China records the second largest GDP in the world as of now. However, World Bank still considers China a middle-income country, because of its relatively low level of GDP per capita."
  },
  {
    "objectID": "DANL200_lab1q.html#q2b",
    "href": "DANL200_lab1q.html#q2b",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2b",
    "text": "Q2b\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between college enrollment and educational attainment of population 45 to 64 years, and how such relationship varies by the type (public or private) of colleges."
  },
  {
    "objectID": "DANL200_lab1q.html#q2c",
    "href": "DANL200_lab1q.html#q2c",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2c",
    "text": "Q2c\nProvide both (1) ggplot codes and (2) a couple of sentences to describe how the relationships described in Q3b vary by gender of population 45 to 64 years."
  },
  {
    "objectID": "DANL200_lab1q.html#q2d",
    "href": "DANL200_lab1q.html#q2d",
    "title": "R Lab 1 - ggplot visualization",
    "section": "Q2d",
    "text": "Q2d\nProvide both (1) ggplot codes and (2) a couple of sentences to describe how the relationships described in Q3b vary by gender of college enrollment."
  },
  {
    "objectID": "choe-beer-markets.html",
    "href": "choe-beer-markets.html",
    "title": "Beer Markets",
    "section": "",
    "text": "Loading Data and Packages\n\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(skimr)   # a better summary of data.frame\nlibrary(scales)  # scales for ggplot\nlibrary(ggthemes)  # additional ggplot themes\nlibrary(hrbrthemes) # additional ggplot themes and color pallets\nlibrary(lubridate)\nlibrary(ggridges)\nlibrary(stargazer)\ntheme_set(theme_ipsum())\n\nbeer_mkt <- read.csv('https://bcdanl.github.io/data/beer_markets.csv')"
  },
  {
    "objectID": "DANL310_lab3a.html",
    "href": "DANL310_lab3a.html",
    "title": "R Visualization Lab 3",
    "section": "",
    "text": "Loading R packages\n\nlibrary(tidyverse)\nlibrary(socviz)\nlibrary(lubridate)\nlibrary(geofacet)\n\n\n\nQuestion 1 - Bar Charts\nThe following data is for Question 1:\n\ntitanic <- read_csv(\n  'https://bcdanl.github.io/data/titanic_cleaned.csv')\n\n\n\n\n\n  \n\n\n\n\n\nReplicate the following ggplot.\n\n\ntitanic <- titanic %>% \n  mutate(surv = ifelse(survived == 0, \"Died\", \"Survived\")) \n\nggplot(data = titanic,\n            aes(x = sex, fill = sex)) + \n  geom_bar() +\n  facet_grid(class ~ surv) +\n  scale_x_discrete(name = NULL) + \n  scale_y_continuous(limits = c(0, 195)) +\n  scale_fill_manual(values = c(\"#D55E00D0\", \"#0072B2D0\"), \n                    guide = \"none\") +\n  theme_bw() + \n  theme(axis.text.y = element_text(margin = margin(7, 7, 7, 7))) +\n  labs( y = \"Number of passengers\")\n\n\n\n\n\n\n\nQuestion 2 - Bar Charts 2\nThe following data is for Question 2:\n\nnyc_flights <- read_csv(\n  'https://bcdanl.github.io/data/nyc_flights_grouped.csv')\n\n\n\n\n\n  \n\n\n\n\nReplicate the following ggplot.\n\n\nnyc_flights %>%\n  group_by(carrier_full) %>%\n  tally() %>%\n  mutate(highlight = ifelse(carrier_full %in% c(\"Delta\", \"American\"), \"yes\", \"no\")) %>%\n  ggplot(aes(x=reorder(carrier_full, n),\n             y=n,\n             fill = highlight)) +\n  scale_fill_manual(values = c(\"#B0B0B0D0\", \"#BD3828D0\"),\n                    guide = \"none\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_x_discrete(name = NULL) +\n  geom_col() +\n  geom_text(aes(label = n), hjust = -.1, vjust = -.5,\n            size = 4) +\n  coord_flip(clip = \"off\") +\n  theme_wsj() +\n  theme(\n    axis.line.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  ylim(c(0, 60000)) +\n  labs( caption = \"Sources: U.S. Department of Transportation,\\nBureau of Transportation Statistics\",\n        y = \"Number of flights\",\n        title = \"Number of flights from NYC\",\n        subtitle = \"Year 2013\")\n\n\n\n\n\n\n\nQuestion 3 - Stocks\nThe following data is for Question 3:\n\nstock = read_csv('https://bcdanl.github.io/data/stocks2013_2023.csv')\n\n\n\n\n\n  \n\n\n\n\nReplicate the following ggplot.\n\nFor each company, the normalized closing price on each date is the company’s closing price on each date divided by the company’s closing price on the first date (2013-01-02).\n\n\n\\[\n\\text{(Normalized Closing Price)}_{\\,\\text{company},\\,\\text{date}}\\,=\\,\\frac{\\text{Closing}_{\\,\\text{company},\\,\\text{date} }}{\\text{Closing}_{\\,\\text{company},\\,\\text{2013-01-02} }}\n\\]\n\nThe normalized closing price on the first date (2013-01-02) is 1 for each company.\ny-axis represents the log of the normalized closing price.\n\n\nlibrary(lubridate)\nq3 <- stock %>% \n  group_by(company) %>% \n  mutate(normal_close =  1 * Close / first(Close) )\n\np <- ggplot(q3, aes(x = Date, y = log(normal_close), color = company))\n\np + \n  geom_line() +\n  geom_hline(yintercept = 0, color = 'red', lty = 2) +\n  scale_x_date(breaks = seq(as.Date(min(q3$Date)), as.Date(max(q3$Date)), by = \"year\")\n) +\n  ylim(c(min(log(q3$normal_close)), max(log(q3$normal_close)))) +\n  labs(y = 'Normalized Closing Price (in log)') +\n  theme(axis.text.x = element_text(angle = 45))"
  },
  {
    "objectID": "DANL200_hw2q.html",
    "href": "DANL200_hw2q.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "",
    "text": "Load R packages you need for Homework Assignment 2"
  },
  {
    "objectID": "DANL200_hw2q.html#q1a",
    "href": "DANL200_hw2q.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1a",
    "text": "Q1a\nProvide both ggplot code and a simple comment to describe the yearly trend of GHG emissions for each sector."
  },
  {
    "objectID": "DANL200_hw2q.html#q1b",
    "href": "DANL200_hw2q.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1b",
    "text": "Q1b\nProvide both ggplot code and a simple comment to describe the yearly trend of United States of America’s GHG emissions for each sector."
  },
  {
    "objectID": "DANL200_hw2q.html#q1c",
    "href": "DANL200_hw2q.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1c",
    "text": "Q1c\nFor each party, calculate the yearly percentage change in GHG emissions for each sector."
  },
  {
    "objectID": "DANL200_hw2q.html#q1d",
    "href": "DANL200_hw2q.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1d",
    "text": "Q1d\nWhich party has reduced GHG emissions most from 1990 level to 2017 level in terms of the percentage change in GHG emissions?"
  },
  {
    "objectID": "DANL200_hw2q.html#q1e",
    "href": "DANL200_hw2q.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1e",
    "text": "Q1e\nWhich sector has reduced GHG emissions most from 1990 level to 2017 level in terms of the percentage change in GHG emissions?"
  },
  {
    "objectID": "DANL200_hw2q.html#q2a",
    "href": "DANL200_hw2q.html#q2a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2a",
    "text": "Q2a\nHow many parties have provided or disbursed positive funding contributions to other countries or regions for their adaptation projects for every single year from 2011 to 2018?"
  },
  {
    "objectID": "DANL200_hw2q.html#q2b",
    "href": "DANL200_hw2q.html#q2b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\nFor each party, calculate the total funding contributions that were disbursed or provided for mitigation projects for each year."
  },
  {
    "objectID": "DANL200_hw2q.html#q2c",
    "href": "DANL200_hw2q.html#q2c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2c",
    "text": "Q2c\nFor each party, calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each year."
  },
  {
    "objectID": "DANL200_hw2q.html#q2d",
    "href": "DANL200_hw2q.html#q2d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2d",
    "text": "Q2d\nProvide both ggplot code and a simple comment to describe the distribution of the ratio between adaptation contribution and mitigation contribution, which is calculated in Q2c."
  },
  {
    "objectID": "DANL200_hw2q.html#q2e",
    "href": "DANL200_hw2q.html#q2e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2e",
    "text": "Q2e\nProvide both ggplot code and a simple comment to describe how the distribution of Contribution varies by Type of support and Status."
  },
  {
    "objectID": "DANL210_lab1q_v3.html",
    "href": "DANL210_lab1q_v3.html",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "",
    "text": "import pandas as pd\nfrom skimpy import skim"
  },
  {
    "objectID": "DANL210_lab1q_v3.html#load-dataframe",
    "href": "DANL210_lab1q_v3.html#load-dataframe",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\ndf_ny = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_pop.csv')\ndf_ny.head(10)\n\n\n\n\n\n  \n    \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n  \n  \n    \n      0\n      36001\n      Albany\n      2015\n      55120\n      44478\n      204024\n    \n    \n      1\n      36001\n      Albany\n      2016\n      55126\n      45357\n      204003\n    \n    \n      2\n      36001\n      Albany\n      2017\n      58814\n      45589\n      204833\n    \n    \n      3\n      36001\n      Albany\n      2018\n      59547\n      45521\n      204509\n    \n    \n      4\n      36001\n      Albany\n      2019\n      61876\n      45150\n      204918\n    \n    \n      5\n      36001\n      Albany\n      2020\n      66632\n      44608\n      205082\n    \n    \n      6\n      36003\n      Allegany\n      2015\n      32205\n      7461\n      30568\n    \n    \n      7\n      36003\n      Allegany\n      2016\n      32417\n      7493\n      30449\n    \n    \n      8\n      36003\n      Allegany\n      2017\n      34001\n      7377\n      30331\n    \n    \n      9\n      36003\n      Allegany\n      2018\n      34553\n      7284\n      30155\n    \n  \n\n\n\n\n\nVariable Description\n\nFIPS: ID number for a county\npincp: average personal income in a county X in year Y\npop_18_24: population 18 to 24 years\npop_25_over: population 25 years and over\n\nSummarize DataFrame df_ny.\n\n\nskim(df_ny)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 372    │ │ int64       │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ string      │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━┓  │\n│ ┃ column_name     ┃ NA   ┃ NA %   ┃ mean     ┃ sd       ┃ p0      ┃ p25     ┃ p75      ┃ p100      ┃ hist    ┃  │\n│ ┡━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━┩  │\n│ │ FIPS            │    0 │      0 │    36000 │       36 │   36000 │   36000 │    36000 │     36000 │ █▇▇▇▇█  │  │\n│ │ year            │    0 │      0 │     2000 │      1.7 │    2000 │    2000 │     2000 │      2000 │ ██████  │  │\n│ │ pincp           │    0 │      0 │    50000 │    20000 │   32000 │   41000 │    52000 │    190000 │   █▁    │  │\n│ │ pop_18_24       │    0 │      0 │    31000 │    49000 │     320 │    4700 │    28000 │    250000 │   █▁    │  │\n│ │ pop_25_over     │    0 │      0 │   220000 │   380000 │    3500 │   35000 │   160000 │   1800000 │    █    │  │\n│ └─────────────────┴──────┴────────┴──────────┴──────────┴─────────┴─────────┴──────────┴───────────┴─────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name               ┃ NA      ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ county_name               │       0 │          0 │                            1 │                      380 │  │\n│ └───────────────────────────┴─────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\ndf_ny.describe()\n\n\n\n\n\n  \n    \n      \n      FIPS\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n  \n  \n    \n      count\n      372.000000\n      372.000000\n      372.000000\n      372.000000\n      3.720000e+02\n    \n    \n      mean\n      36062.000000\n      2017.500000\n      50102.801075\n      30601.064516\n      2.190541e+05\n    \n    \n      std\n      35.839264\n      1.710125\n      20291.643067\n      49283.641237\n      3.797498e+05\n    \n    \n      min\n      36001.000000\n      2015.000000\n      31831.000000\n      323.000000\n      3.485000e+03\n    \n    \n      25%\n      36031.000000\n      2016.000000\n      40612.250000\n      4675.750000\n      3.520200e+04\n    \n    \n      50%\n      36062.000000\n      2017.500000\n      45269.000000\n      9668.500000\n      5.985150e+04\n    \n    \n      75%\n      36093.000000\n      2019.000000\n      51998.000000\n      28099.250000\n      1.605870e+05\n    \n    \n      max\n      36123.000000\n      2020.000000\n      191220.000000\n      251964.000000\n      1.789355e+06\n    \n  \n\n\n\n\n\ndf_ny.groupby('year')['pincp'].describe()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      62.0\n      45449.580645\n      17342.754837\n      31831.0\n      37523.75\n      40790.5\n      47230.50\n      152793.0\n    \n    \n      2016\n      62.0\n      46474.064516\n      18544.679598\n      32417.0\n      38140.00\n      41283.0\n      48046.25\n      163112.0\n    \n    \n      2017\n      62.0\n      49078.548387\n      20323.712796\n      34001.0\n      40197.25\n      43617.5\n      50476.00\n      179655.0\n    \n    \n      2018\n      62.0\n      50400.483871\n      21110.934480\n      34553.0\n      40867.25\n      44751.5\n      52062.25\n      184539.0\n    \n    \n      2019\n      62.0\n      52551.370968\n      21381.379235\n      37131.0\n      42687.25\n      46772.0\n      54339.25\n      187213.0\n    \n    \n      2020\n      62.0\n      56662.758065\n      21384.820890\n      40840.0\n      47044.75\n      50577.5\n      58764.00\n      191220.0\n    \n  \n\n\n\n\n\ndf_ny.groupby('year')['pop_18_24'].describe()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      62.0\n      32025.887097\n      52851.482119\n      466.0\n      5039.00\n      9875.5\n      28233.25\n      251964.0\n    \n    \n      2016\n      62.0\n      31725.241935\n      51964.450784\n      382.0\n      5082.75\n      9900.5\n      28467.75\n      244788.0\n    \n    \n      2017\n      62.0\n      31195.403226\n      50824.225228\n      386.0\n      4903.50\n      9762.5\n      28317.00\n      236951.0\n    \n    \n      2018\n      62.0\n      30169.000000\n      48502.347023\n      406.0\n      4740.50\n      9577.5\n      27736.25\n      223707.0\n    \n    \n      2019\n      62.0\n      29546.500000\n      47141.642477\n      323.0\n      4550.25\n      9473.0\n      27574.00\n      215081.0\n    \n    \n      2020\n      62.0\n      28944.354839\n      45967.793981\n      369.0\n      4403.25\n      9394.0\n      27272.00\n      207966.0\n    \n  \n\n\n\n\n\ndf_ny.groupby('year')['pop_25_over'].describe()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      62.0\n      216706.370968\n      376492.870083\n      3546.0\n      35351.25\n      60028.0\n      159284.50\n      1735647.0\n    \n    \n      2016\n      62.0\n      217807.790323\n      379394.903521\n      3575.0\n      35209.50\n      60022.5\n      159512.25\n      1753695.0\n    \n    \n      2017\n      62.0\n      220335.629032\n      386192.777963\n      3604.0\n      35328.75\n      60108.0\n      160193.00\n      1789355.0\n    \n    \n      2018\n      62.0\n      219457.129032\n      383470.172157\n      3536.0\n      35492.25\n      59851.5\n      160939.50\n      1777281.0\n    \n    \n      2019\n      62.0\n      219869.951613\n      384159.431483\n      3567.0\n      35138.50\n      59689.0\n      161993.50\n      1780247.0\n    \n    \n      2020\n      62.0\n      220147.693548\n      384199.007544\n      3485.0\n      35528.00\n      59656.5\n      163294.50\n      1780524.0"
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1a",
    "href": "DANL210_lab1q_v3.html#q1a",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1a",
    "text": "Q1a\n\nUse .sort_values() to find the top 5 rich counties in NY for each year.\n\nDo not use .apply()"
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1b",
    "href": "DANL210_lab1q_v3.html#q1b",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1b",
    "text": "Q1b\n\nUse .rank() to find the top 5 rich counties in NY for each year.\n\nDo not use .apply()"
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1c",
    "href": "DANL210_lab1q_v3.html#q1c",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1c",
    "text": "Q1c\n\nUse apply() with a lambda function and .sort_values() to find the top 5 rich counties in NY for each year."
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1d",
    "href": "DANL210_lab1q_v3.html#q1d",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1d",
    "text": "Q1d\n\nWrite a function with def and .sort_values() that selects the top 5 pincp values.\nThen, use the defined function in apply() to find the top 5 rich counties in NY for each year."
  },
  {
    "objectID": "DANL210_lab1q_v3.html#q1e",
    "href": "DANL210_lab1q_v3.html#q1e",
    "title": "Python Lab 1 - Pandas Group Operations and apply()",
    "section": "Q1e",
    "text": "Q1e\n\nVisualize the yearly trend of the mean level of pincp."
  },
  {
    "objectID": "DANL210_lab7q.html",
    "href": "DANL210_lab7q.html",
    "title": "Python Lab 7 - Web-scrapping 2",
    "section": "",
    "text": "Let’s do web-scrapping!"
  },
  {
    "objectID": "DANL210_lab7q.html#load-libraries",
    "href": "DANL210_lab7q.html#load-libraries",
    "title": "Python Lab 7 - Web-scrapping 2",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nimport pandas as pd\nfrom selenium import webdriver"
  },
  {
    "objectID": "DANL210_lab7q.html#q1a",
    "href": "DANL210_lab7q.html#q1a",
    "title": "Python Lab 7 - Web-scrapping 2",
    "section": "Q1a",
    "text": "Q1a\nUse (1) double for-loop (nested for-loop), (2) pd.DataFrame([]), (3) pd.concat(), (4) DataFrame.append(), and (5) DataFrame.to_csv() to export data in the table as the CSV file."
  },
  {
    "objectID": "DANL210_lab7q.html#q1b",
    "href": "DANL210_lab7q.html#q1b",
    "title": "Python Lab 7 - Web-scrapping 2",
    "section": "Q1b",
    "text": "Q1b\n\nVisualize the monthly trend of retail oil price using seaborn.\n\nWhen using to_datetime(), please consider using format with the following directives:"
  },
  {
    "objectID": "DANL200_lab2q.html",
    "href": "DANL200_lab2q.html",
    "title": "R Lab 2 - Data Transformation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab2q.html#variable-description",
    "href": "DANL200_lab2q.html#variable-description",
    "title": "R Lab 2 - Data Transformation",
    "section": "Variable Description",
    "text": "Variable Description\n\nid_user: a unique identification number for a Twitter user whom retweeted to a tweet with #climatechange or #globalwarming.\nid_city: a unique identification number for a city.\nFIPS: a unique identification number for a county.\n\nEach row represents an observation of a retweet to a tweet with #climatechange or #globalwarming.\nEach row includes a Twitter user’s geographic information at city or county levels (variables FIPS, county, city) as well as information about timing when a Twitter user retweeted (variables year, month, day, hour, minute, second)."
  },
  {
    "objectID": "DANL200_lab2q.html#q1a",
    "href": "DANL200_lab2q.html#q1a",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1a",
    "text": "Q1a\nHow many Twitter users retweeted on the date, January 1, 2017?"
  },
  {
    "objectID": "DANL200_lab2q.html#q1b",
    "href": "DANL200_lab2q.html#q1b",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1b",
    "text": "Q1b\nWhich city is with the third highest number of retweets on the date, December 1, 2017?"
  },
  {
    "objectID": "DANL200_lab2q.html#q1c",
    "href": "DANL200_lab2q.html#q1c",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1c",
    "text": "Q1c\nFor each year, find the top 5 Twitter users in NY state in terms of the number of retweets they made in NY state. In which city and county do these users live in?"
  },
  {
    "objectID": "DANL200_lab2q.html#q1d",
    "href": "DANL200_lab2q.html#q1d",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1d",
    "text": "Q1d\nSummarize the data set into the data frame with county and month levels of retweets with the following variables:\n\nFIPS, county, year, month;\nn_retweets: the number of retweets in year YYYY and month MM from county C.\n\nThe unique() or distinct() functions can be used to keep only unique/distinct rows from a data frame."
  },
  {
    "objectID": "DANL200_lab2q.html#q1e",
    "href": "DANL200_lab2q.html#q1e",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1e",
    "text": "Q1e\nDescribe the relationship between the number of retweets and county using ggplot. Make a simple comment on your plot."
  },
  {
    "objectID": "DANL200_lab2q.html#variable-description-1",
    "href": "DANL200_lab2q.html#variable-description-1",
    "title": "R Lab 2 - Data Transformation",
    "section": "Variable Description",
    "text": "Variable Description\nEach observation in beer_markets.csv is a household-level record for one transaction of beer.\n\n\nhh: an identifier of the household;\nX_purchase_desc: details on the purchased item;\nquantity: the number of items purchased;\nbrand: Bud Light, Busch Light, Coors Light, Miller Lite, or Natural Light;\nspent: total dollar value of purchase;\nbeer_floz: total volume of beer, in fluid ounces;\nprice_per_floz: price per fl.oz. (i.e., beer spent/beer floz);\ncontainer: the type of container;\npromo: Whether the item was promoted (coupon or otherwise);\nmarket: Scan-track market (or state if rural);\ndemographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher."
  },
  {
    "objectID": "DANL200_lab2q.html#q2a",
    "href": "DANL200_lab2q.html#q2a",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2a",
    "text": "Q2a\n\nFind the top 5 markets in terms of the total volume of beer.\nFind the top 5 markets in terms of the total volume of BUD LIGHT.\nFind the top 5 markets in terms of the total volume of BUSCH LIGHT.\nFind the top 5 markets in terms of the total volume of COORS LIGHT.\nFind the top 5 markets in terms of the total volume of MILLER LITE.\nFind the top 5 markets in terms of the total volume of NATURAL LIGHT."
  },
  {
    "objectID": "DANL200_lab2q.html#q2b",
    "href": "DANL200_lab2q.html#q2b",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2b",
    "text": "Q2b\n\nFor households that purchased BUD LIGHT, what fraction of households did purchase only BUD LIGHT?\nFor households that purchased BUSCH LIGHT, what fraction of households did purchase only BUSCH LIGHT?\nFor households that purchased COORS LIGHT, what fraction of households did purchase only COORS LIGHT?\nFor households that purchased MILLER LITE, what fraction of households did purchase only MILLER LITE?\nFor households that purchased NATURAL LIGHT, what fraction of households did purchase only NATURAL LIGHT?\nWhich beer brand does have the largest base of loyal consumers?"
  },
  {
    "objectID": "DANL200_lab2q.html#q2c",
    "href": "DANL200_lab2q.html#q2c",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2c",
    "text": "Q2c\n\nCalculate the number of beer transactions for each household.\nCalculate the fraction of each beer brand for each household."
  },
  {
    "objectID": "DANL210_hw4q.html",
    "href": "DANL210_hw4q.html",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 4",
    "section": "",
    "text": "Go to the following website for webscrapping\n\nhttp://books.toscrape.com\n\n\n\n\n\nProvide your Python Selenium code to create the CSV file of DataFrame with the following variables:\n\nbook category (e.g., “Travel”)\nbook title (e.g., “It’s Only the Himalayas”)\nbook price (e.g., “£45.17”)\n\nYour code should scrap all the books.\n\n\n\n\n\nFind the five most expensive books in each category.\n\n\n\n\nCalculate the mean, median, standard deviation of book price for each book category."
  },
  {
    "objectID": "DANL310_lab2q.html",
    "href": "DANL310_lab2q.html",
    "title": "R Visualization Lab 2 - Maps 2",
    "section": "",
    "text": "Loading R packages\n\nlibrary(tidyverse)\nlibrary(socviz)\nlibrary(lubridate)\n\nLoading required package: timechange\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(geofacet)\n\n\nAttaching package: 'geofacet'\n\n\nThe following object is masked from 'package:socviz':\n\n    election\n\n\n\n\nQuestion 1 - Unemployment Rate Maps with geofacet::facet_geo()\nThe following data is for Question 1:\n\nunemp_house_prices <- read_csv(\n  'https://bcdanl.github.io/data/unemp_house_prices.csv')\n\n\n\n\n\n  \n\n\n\n\n\nUse geom_area(), geom_line(), and facet_geo(~state, labeller = adjust_labels) to replicate the following figure\n\n\nadjust_labels <- as_labeller(\n  function(x) {\n    case_when(\n      x == \"New Hampshire\" ~ \"N. Hampshire\",\n      x == \"District of Columbia\" ~ \"DC\",\n      TRUE ~ x\n    )\n  }\n)\n\n\n\n\n\n\n\n\nQuestion 2 - Election maps and data clearning\nThe following data is for Question 2:\n\nelection_panel <- read_csv(\n  'https://bcdanl.github.io/data/election_panel.csv')\n\n\n\n\n\n  \n\n\n\n\nReplicate the following map.\n\nDo not use coord_map(projection = \"albers\", lat0 = 39, lat1 = 45)."
  },
  {
    "objectID": "DANL310_hw1q.html",
    "href": "DANL310_hw1q.html",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "",
    "text": "Renovate your personal website on GitHub using Quarto.\n\nFAQ about Quarto for R Markdown users are provided below: https://quarto.org/docs/faq/rmarkdown.html\nA guide for creating a Quarto website is provided in the following webpage: https://quarto.org/docs/websites/."
  },
  {
    "objectID": "DANL310_hw1q.html#q2a.",
    "href": "DANL310_hw1q.html#q2a.",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2a.",
    "text": "Q2a.\nUse the following data.frame for Q2a, Q2b, and Q2c.\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')"
  },
  {
    "objectID": "DANL310_hw1q.html#q2b",
    "href": "DANL310_hw1q.html#q2b",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2b",
    "text": "Q2b"
  },
  {
    "objectID": "DANL310_hw1q.html#q2c",
    "href": "DANL310_hw1q.html#q2c",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2c",
    "text": "Q2c\nUse ggridges::geom_density_ridges() for Q2c."
  },
  {
    "objectID": "DANL310_hw1q.html#q2d",
    "href": "DANL310_hw1q.html#q2d",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2d",
    "text": "Q2d\nUse ggplot::mtcars for Q2d."
  },
  {
    "objectID": "DANL310_hw1q.html#q2e",
    "href": "DANL310_hw1q.html#q2e",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2e",
    "text": "Q2e\nUse the following data.frame for Q2e.\n\npopgrowth_df <- read_csv(\n  'https://bcdanl.github.io/data/popgrowth.csv')"
  },
  {
    "objectID": "DANL310_hw1q.html#q2f",
    "href": "DANL310_hw1q.html#q2f",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2f",
    "text": "Q2f\nUse the following data.frame for Q2f.\n\nmale_Aus <- read_csv(\n  'https://bcdanl.github.io/data/aus_athletics_male.csv')"
  },
  {
    "objectID": "DANL310_hw1q.html#q2g",
    "href": "DANL310_hw1q.html#q2g",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2g",
    "text": "Q2g\nUse the following data.frame for Q2g.\n\ntitanic <- read_csv(\n  'https://bcdanl.github.io/data/titanic_cleaned.csv')"
  },
  {
    "objectID": "DANL310_hw1q.html#q2h",
    "href": "DANL310_hw1q.html#q2h",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2h",
    "text": "Q2h\nUse the following data.frame for Q2h.\n\ncows_filtered <- read_csv(\n  'https://bcdanl.github.io/data/cows_filtered.csv')"
  },
  {
    "objectID": "DANL210_lab6a.html",
    "href": "DANL210_lab6a.html",
    "title": "Python Lab 6 - Web-scrapping 1",
    "section": "",
    "text": "import os\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nimport pandas as pd"
  },
  {
    "objectID": "DANL210_lab6a.html#basic-setting-for-selenium-web-scrapping",
    "href": "DANL210_lab6a.html#basic-setting-for-selenium-web-scrapping",
    "title": "Python Lab 6 - Web-scrapping 1",
    "section": "Basic Setting for Selenium Web-scrapping",
    "text": "Basic Setting for Selenium Web-scrapping\n\nwd_path = '/Users/byeong-hakchoe/Google Drive/suny-geneseo/spring2023/lecture_codes/' # Pathname of working directory\nos.chdir(wd_path)  \nos.getcwd() # TO CHECK WORKING DIRECTORY\n\noptions = Options()\noptions.add_argument(\"window-size=700,1200\")\n\ndriver_path = \"/Users/byeong-hakchoe/Downloads/chromedriver_mac64 (1)/chromedriver\" # Pathname of chromedriver\ndriver = webdriver.Chrome(chrome_options = options, \n                          executable_path = driver_path)"
  },
  {
    "objectID": "DANL210_lab6a.html#q1a",
    "href": "DANL210_lab6a.html#q1a",
    "title": "Python Lab 6 - Web-scrapping 1",
    "section": "Q1a",
    "text": "Q1a\n\nGet the number of rows in the second table.\n\n\nnrows = driver.find_elements(By.XPATH, '//*[@id=\"table02\"]/tbody/tr')\nnrows = len(nrows)"
  },
  {
    "objectID": "DANL210_lab6a.html#q1b",
    "href": "DANL210_lab6a.html#q1b",
    "title": "Python Lab 6 - Web-scrapping 1",
    "section": "Q1b",
    "text": "Q1b\n\nFind the XPath for each cell in the first two rows of the second table.\nXPath of “Tiger Nixon” is\n\n//*[@id=\"table02\"]/tbody/tr[1]/td[1]\n\nXPath of “Tiger Nixon” is\n\n//*[@id=\"table02\"]/tbody/tr[1]/td[1]\n\nXPath of “System Architect” is\n\n//*[@id=\"table02\"]/tbody/tr[1]/td[2]\n\nXPath of “Garrett Winters” is\n\n//*[@id=\"table02\"]/tbody/tr[2]/td[1]\n\nXPath of “Accountant” is\n\n//*[@id=\"table02\"]/tbody/tr[2]/td[2]\n\nSo, we can find the pattern from the numbers in the XPaths.\n\nFinding this pattern is essential when writing a loop code."
  },
  {
    "objectID": "DANL210_lab6a.html#q1c",
    "href": "DANL210_lab6a.html#q1c",
    "title": "Python Lab 6 - Web-scrapping 1",
    "section": "Q1c",
    "text": "Q1c\n\nUse (1) for-loop, (2) pd.DataFrame([]), (3) pd.concat(), (4) DataFrame.append(), and (5) DataFrame.to_csv() to export data in the second table as the CSV file.\n\n\ndf = pd.DataFrame()\nfor i in range(1, nrows+1):\n    name = driver.find_element(By.XPATH, '//*[@id=\"table02\"]/tbody/tr['+str(i)+']/td[1]').text\n    name = pd.DataFrame([name])\n    # name.columns = ['name']  \n    position = driver.find_element(By.XPATH, '//*[@id=\"table02\"]/tbody/tr['+str(i)+']/td[2]').text\n    position = pd.DataFrame([position])\n    office = driver.find_element(By.XPATH, '//*[@id=\"table02\"]/tbody/tr['+str(i)+']/td[3]').text\n    office = pd.DataFrame([office])\n    age = driver.find_element(By.XPATH, '//*[@id=\"table02\"]/tbody/tr['+str(i)+']/td[4]').text\n    age = pd.DataFrame([age])\n    start_date = driver.find_element(By.XPATH, '//*[@id=\"table02\"]/tbody/tr['+str(i)+']/td[5]').text\n    start_date = pd.DataFrame([start_date])\n    salary = driver.find_element(By.XPATH, '//*[@id=\"table02\"]/tbody/tr['+str(i)+']/td[6]').text\n    salary = pd.DataFrame([salary])\n    data = pd.concat([name, position, office, age, start_date, salary], axis=1) \n    df = df.append(data)\n\n\n# table header by xpath\nncols = driver.find_elements(By.XPATH, '//*[@id=\"table02\"]/thead/tr/th')\nncols = len(ncols)\n\nheader = []\nfor i in range(1, ncols+1):\n    head = driver.find_element(By.XPATH, '//*[@id=\"table02\"]/thead/tr/th['+str(i)+']').text\n    header.append(head)\n\ndf.columns = header\ndf.to_csv(\"data/table_example_20230420.csv\", index = False)"
  },
  {
    "objectID": "git-large-file-push.html",
    "href": "git-large-file-push.html",
    "title": "GitHub Repository and Large Files",
    "section": "",
    "text": "Individual files in a GitHub repository are strictly limited to a 100 MB maximum size limit.\nIf we push a large file (> 100 MB) to your GitHub repository, we get the following error:\n\nremote: error File: FILE_NAME is ... this exceeds GitHub's file size limit of 100 MB.\n\nEven after we remove the large file from the local repository folder, we still get the same error.\nTo solve the issue, we need to follow the procedure in the following page:\n\nhttps://docs.github.com/en/authentication/keeping-your-account-and-data-secure/removing-sensitive-data-from-a-repository#removing-a-file-that-contained-a-password\nI recommend using the BFG to fix this problem.\n\nUsing the BFG\n\nDownload the bfg-1.14.0.jar file.\n\n\nhttps://rtyley.github.io/bfg-repo-cleaner/\n\n\nMove the file, bfg-1.14.0.jar, to your local GitHub repository folder.\nIn Terminal where the directory is your local repo, run the following command:\n\n\njava -jar bfg-1.14.0.jar --delete-files YOUR-FILE-WITH-LARGE-DATA\nYou may get the error message: 'java' is not recognized as an internal or external command, operable program or batch file.\n\nIf you, a Windows user, encounter the above error message from running the above line , here is what you can do.\n\nSearch for “Environment Variables” in the Windows search bar and click on “Edit the system environment variables”.\nIn the System Properties window that opens, click on the “Environment Variables” button.\nIn the “Environment Variables” window, under “System Variables”, scroll down and find the “Path” variable, then click on the “Edit” button.\nIn the “Edit environment variable” window, click on the “New” button and add the path to the bin directory of your Java installation. For example, if your Java installation is in C:\\Program Files\\Java\\jdk1.8.0_291, then add C:\\Program Files\\Java\\jdk1.8.0_291\\bin to the PATH variable. - Downlaod the jdk installation file from Oracle if it is not installed on a computer.\nClick “OK” in all the windows to close them.\nOpen a new Command Prompt window and try running the BFG Repo Cleaner command again."
  },
  {
    "objectID": "DANL200_lab3a.html",
    "href": "DANL200_lab3a.html",
    "title": "R Lab 3 - Pivoting",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab3a.html#q1a",
    "href": "DANL200_lab3a.html#q1a",
    "title": "R Lab 3 - Pivoting",
    "section": "Q1a",
    "text": "Q1a\n\nDescribe how the distribution of rating varies across week 1, week 2, and week 3 using the faceted histogram.\n\n\n# making billboard longer\nbillboard_long <- billboard %>% \n  pivot_longer(cols = starts_with('wk'),\n               names_to = 'week',\n               values_to = 'rating')\n\n# ggplot\nggplot( data = filter(billboard_long,\n                      week %in% c('wk1','wk2','wk3')),\n        aes(x = rating) ) +\n  geom_histogram(bins = 75) +\n  facet_wrap(week ~., ncol = 1)"
  },
  {
    "objectID": "DANL200_lab3a.html#q1b",
    "href": "DANL200_lab3a.html#q1b",
    "title": "R Lab 3 - Pivoting",
    "section": "Q1b",
    "text": "Q1b\n\nWhich artist(s) have the most number of tracks in billboard data.frame?\nDo not double-count an artist’s tracks if they appear in multiple weeks.\n\n\nq1b <- billboard_long %>% \n  select(artist, track) %>% \n  distinct() %>% \n  group_by(artist) %>% \n  count() %>% \n  arrange(-n)"
  },
  {
    "objectID": "DANL200_lab3a.html#q2a",
    "href": "DANL200_lab3a.html#q2a",
    "title": "R Lab 3 - Pivoting",
    "section": "Q2a",
    "text": "Q2a\nMake ny_pincp longer.\n\nny_pincp_long <- ny_pincp %>% \n  pivot_longer(cols = starts_with('pincp'),\n               names_to = 'year',\n               values_to = 'pincp')"
  },
  {
    "objectID": "DANL200_lab3a.html#q2b",
    "href": "DANL200_lab3a.html#q2b",
    "title": "R Lab 3 - Pivoting",
    "section": "Q2b",
    "text": "Q2b\nProvide both (1) ggplot code and (2) a simple comment to describe how overall the yearly trend of NY counties’ average personal incomes are.\n\nny_pincp_long2 <- ny_pincp_long %>% \n  separate(year, into = c('tmp', 'year'), sep = 'pincp') %>% \n  mutate(year = as.integer(year)) %>% \n  select(-tmp) %>% \n  filter(fips != 36000)  # which is state-level average income\n\n\n\n\n\n  \n\n\n\n\nggplot(ny_pincp_long2,\n       aes(x = year, y = pincp)) +\n  geom_line(aes(group = geoname),\n            alpha = .1 ) +\n  geom_smooth(method = lm) +\n  geom_smooth(color = 'red')"
  },
  {
    "objectID": "DANL200_lab3a.html#q3a",
    "href": "DANL200_lab3a.html#q3a",
    "title": "R Lab 3 - Pivoting",
    "section": "Q3a",
    "text": "Q3a\n\nKeep only the following three variables, date, countriesAndTerritories, and cases.\nThen make a wide-form data.frame of covid whose variable names are from countriesAndTerritories and values are from cases.\nThen drop the variable date.\n\n\ncovid_wide <- covid %>%\n  select(date, countriesAndTerritories, cases) %>%\n  pivot_wider(names_from = countriesAndTerritories,\n              values_from = cases) %>% \n  select(-date)"
  },
  {
    "objectID": "DANL200_lab3a.html#q3b",
    "href": "DANL200_lab3a.html#q3b",
    "title": "R Lab 3 - Pivoting",
    "section": "Q3b",
    "text": "Q3b\n\nUse the wide-form data.frame of covid to find the top 10 countries in terms of the correlation between their cases and the USA case.\n\nUse cor(data.frame), which returns a matrix.\nThen convert it to data.frame using as.data.frame(matrix)\n\n\n\ncovid_cor <- as.data.frame( cor(covid_wide) )  \n\n\n\n\n\n  \n\n\n\n\ncovid_cor_us <- covid_cor %>% \n  select(USA) %>% \n  arrange(-USA)"
  },
  {
    "objectID": "DANL200_lab2a.html",
    "href": "DANL200_lab2a.html",
    "title": "R Lab 2 - Data Transformation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab2a.html#variable-description",
    "href": "DANL200_lab2a.html#variable-description",
    "title": "R Lab 2 - Data Transformation",
    "section": "Variable Description",
    "text": "Variable Description\n\nid_user: a unique identification number for a Twitter user whom retweeted to a tweet with #climatechange or #globalwarming.\nid_city: a unique identification number for a city.\nFIPS: a unique identification number for a county.\n\nEach row represents an observation of a retweet to a tweet with #climatechange or #globalwarming.\nEach row includes a Twitter user’s geographic information at city or county levels (variables FIPS, county, city) as well as information about timing when a Twitter user retweeted (variables year, month, day, hour, minute, second)."
  },
  {
    "objectID": "DANL200_lab2a.html#q1a",
    "href": "DANL200_lab2a.html#q1a",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1a",
    "text": "Q1a\nHow many Twitter users retweeted on the date, January 1, 2017?\n\n# 1. using filter(), select(), and distinct()\n\n# Select tweets from January 1st, 2017 in the NY_CC_tweets dataset\nQ1a <- NY_CC_tweets %>% \n  filter(year == 2017, month == 1, day == 1) %>% \n\n  # Select the user IDs from the filtered tweets\n  select(id_user) %>% \n\n  # Remove duplicate user IDs\n  distinct()  \n \n# 19\n\n\n\n# 2. using filter() and summarise() with n_distinct()\n\n# Select tweets from January 1st, 2017 in the NY_CC_tweets dataset\nQ1a <- NY_CC_tweets %>% \n  filter(year == 2017, month == 1, day == 1) %>% \n\n  # Count the number of distinct user IDs in the filtered tweets\n  summarise(n_users = n_distinct(id_user))\n\n# 19\n\n\n\n# 3. using filter(), select() and mutate() with n() and distinct()\n\n# Select tweets from January 1st, 2017 in the NY_CC_tweets dataset\nQ1a <- NY_CC_tweets %>% \n  filter(year == 2017, month == 1, day == 1) %>% \n\n  # Select the user IDs and date columns from the filtered tweets\n  select(id_user, year, month, day) %>% \n\n  # Remove duplicate rows based on all columns\n  distinct() %>% \n\n  # Add a column with the number of users in each group of distinct date columns\n  mutate( n_users = n() ) %>% \n\n  # Select only the date and user count columns\n  select( year, month, day, n_users ) %>% \n\n  # Remove duplicate rows based on all columns\n  distinct()\n\n# 19"
  },
  {
    "objectID": "DANL200_lab2a.html#q1b",
    "href": "DANL200_lab2a.html#q1b",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1b",
    "text": "Q1b\nWhich city is with the third highest number of retweets on the date, December 1, 2017?\n\n# 1. filter(), group_by(), \n# mutate() with n(), select(), distinct(), and arrange()\n\nQ1b <- NY_CC_tweets %>% \n  filter(year == 2017, month == 12, day == 1) %>% # Filter tweets from December 1st, 2017.\n  group_by(id_city) %>% # Group tweets by id_city.\n  mutate(n_rt = n()) %>% # Count the number of retweets for each tweet and add as a new column \"n_rt\".\n  select(id_city, city, n_rt) %>% # Keep only columns \"id_city\", \"city\", and \"n_rt\".\n  distinct() %>% # Remove duplicate rows (if any).\n  arrange(-n_rt) # Sort by \"n_rt\" in descending order.\n# brooklyn, ny \n\n\n# 2. filter(), group_by(), \n# summarise() with n(), and arrange()\nQ1b <- NY_CC_tweets %>% \n  filter(year == 2017, month == 12, day == 1) %>% # Filter tweets from December 1st, 2017.\n  group_by(id_city, city) %>% # Group tweets by both \"id_city\" and \"city\".\n  summarize(n_rt = n()) %>% # Summarize the number of retweets for each city.\n  arrange(-n_rt) # Sort by \"n_rt\" in descending order.\n# brooklyn, ny \n\n\n# 3. filter(), group_by(), \n# summarise() with n(), ungroup(), and mutate() with dense_rank()\nQ1b <- NY_CC_tweets %>% \n  filter(year == 2017, month == 12, day == 1) %>% # Filter tweets from December 1st, 2017.\n  group_by(id_city, city) %>% # Group tweets by city and id_city.\n  summarize(n_rt = n()) %>% # Summarize the number of retweets for each city.\n  ungroup() %>% # Remove grouping.\n  mutate(ranking = dense_rank(desc(n_rt))) %>% # Calculate the ranking of each city based on retweet count.\n  filter(ranking == 3) # Keep only the city with ranking 3 (i.e., the 3rd most retweeted city).\n# brooklyn, ny"
  },
  {
    "objectID": "DANL200_lab2a.html#q1c",
    "href": "DANL200_lab2a.html#q1c",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1c",
    "text": "Q1c\nFor each year, find the top 5 Twitter users in NY state in terms of the number of retweets they made in NY state. In which city and county do these users live in?\n\n# 1. group_by(), mutate() with n(), \n# group_by(), mutate() with dense_rank(), \n# filter(), select(), distinct(), and arrange()\n\nQ1c <- NY_CC_tweets %>% \n  group_by(id_user, year) %>% # Group tweets by both \"id_user\" and \"year\".\n  mutate(n_rt = n()) %>% # Count the number of retweets for each user for each year.\n  group_by(year) %>% # Group by \"year\".\n  mutate(n_rt_rank = dense_rank(desc(n_rt))) %>% # Calculate the ranking of each user based on the number of retweets in descending order.\n  filter(n_rt_rank <= 5) %>% # Keep only the top 5 users in terms of number of retweets for each year.\n  select(-(month:second)) %>% # Remove columns \"month\" through \"second\" to use distinct().\n  distinct() %>% # Remove duplicate rows (if any).\n  arrange(year, n_rt_rank) # Sort by \"year\" and then by \"n_rt_rank\".\n\n\n\n# 2. group_by(), summarise() with n(), \n# filter(), arrange(), and head()\n\nQ1c_2012 <- NY_CC_tweets %>% \n  group_by(id_user, year, city, county) %>% # Group tweets by \"id_user\", \"year\", \"city\", and \"county\".\n  summarise(n_rt = n()) %>% # Summarize the number of retweets for each user in each city and county.\n  filter(year == 2012) %>% # Keep only tweets from 2012.\n  arrange(-n_rt) %>% # Sort by \"n_rt\" in descending order.\n  head(5) # Keep only the top 5 rows.\n\n\nQ1c_2013 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2013 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2014 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2014 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2015 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2015 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2016 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2016 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2017 <- NY_CC_tweets %>% \n  group_by( id_user, year, city, county ) %>% \n  summarise( n_rt = n() ) %>% \n  filter( year == 2017 ) %>% \n  arrange(-n_rt) %>% \n  head(5)\n\n\n# 4. group_by(), mutate() with n(), \n# filter(), select(), unique(), arrange(), and head()\nQ1c_2012 <- NY_CC_tweets %>% \n  group_by(id_user, year) %>% # Group tweets by \"id_user\" and \"year\".\n  mutate(n_rt = n()) %>% # Add a column \"n_rt\" with the number of retweets for each group.\n  filter(year == 2012) %>% # Keep only tweets from the year 2012.\n  select(year, id_user, n_rt, city, county) %>% # Select columns \"year\", \"id_user\", \"n_rt\", \"city\", and \"county\".\n  unique() %>% # Remove duplicates.\n  arrange(-n_rt) %>% # Sort by \"n_rt\" in descending order.\n  head(5) # Keep only the top 5 rows.\n\n\nQ1c_2013 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2013 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2014 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2014 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2015 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2015 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2016 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2016 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\nQ1c_2017 <- NY_CC_tweets %>% \n  group_by( id_user, year ) %>% \n  mutate( n_rt = n() ) %>% \n  filter( year == 2017 ) %>% \n  select(year, id_user, n_rt, city, county) %>% \n  unique() %>% \n  arrange(-n_rt) %>% \n  head(5)\n\n\n# cf. I do not require using functions, \n# but here I am showing you how functions make code simpler\n\n# Define a function \"top_users_yr\" that takes a year \"yr\" as an input and returns the top users of retweets for that year.\ntop_users_yr <- function(yr){\n  NY_CC_tweets %>% \n    group_by(id_user, year) %>% # Group tweets by \"id_user\" and \"year\".\n    mutate(n_rt = n()) %>% # Add a column \"n_rt\" with the number of retweets for each group.\n    filter(year == yr) %>% # Keep only tweets from the year \"yr\".\n    select(year, id_user, n_rt, city, county) %>% # Select columns \"year\", \"id_user\", \"n_rt\", \"city\", and \"county\".\n    unique() %>% # Remove duplicates.\n    arrange(-n_rt) # Sort by \"n_rt\" in descending order.\n}\n\n# Loop over the years from 2012 to 2017 and print the top 5 users of retweets for each year.\nfor (i in 2012:2017){\n  print(head(top_users_yr(i), n = 5))\n}"
  },
  {
    "objectID": "DANL200_lab2a.html#q1d",
    "href": "DANL200_lab2a.html#q1d",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1d",
    "text": "Q1d\nSummarize the data set into the data frame with county and month levels of retweets with the following variables:\n\nFIPS, county, year, month;\nn_retweets: the number of retweets in year YYYY and month MM from county C.\n\nThe unique() or distinct() functions can be used to keep only unique/distinct rows from a data frame.\n\n# 1. group_by() and summarize() with n()\nQ1d <- NY_CC_tweets %>% \n  group_by(FIPS, county, year, month) %>% \n  summarise(n_retweets = n())\n\n# 2. count() and rename()\nQ1d <- NY_CC_tweets %>% \n  count(FIPS, county, year, month) %>% \n  rename(n_retweets = n)\n\n# 3. group_by(), mutate() with n(), \n# select(), arrange() and distinct()\nQ1d <- NY_CC_tweets %>% \n  group_by(FIPS, county, year, month) %>% \n  mutate(n_retweets = n()) %>% \n  select(FIPS, county, year, month, n_retweets) %>% \n  arrange(FIPS, county, year, month) %>% \n  distinct()\n\nQ1d <- NY_CC_tweets %>% \n  mutate(dum = 1) %>% \n  group_by(FIPS, county, year, month) %>% \n  mutate(n_retweets = sum(dum)) %>% \n  select(FIPS, county, year, month, n_retweets) %>% \n  arrange(FIPS, county, year, month) %>% \n  distinct()"
  },
  {
    "objectID": "DANL200_lab2a.html#q1e",
    "href": "DANL200_lab2a.html#q1e",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q1e",
    "text": "Q1e\nDescribe the relationship between the number of retweets and county using ggplot. Make a simple comment on your plot.\n\nQ1e <- NY_CC_tweets %>% \n  group_by(county) %>% \n  summarize(n_retweets = n())\n\n\n\n\n\n  \n\n\n\n\nggplot(data = Q1e) +\n  geom_col(aes(x = n_retweets, y = county)) \n# Most retweets about #climatechange came from New York county.\n\n\n\n\n\n\n\n\nggplot(data = Q1e) +\n  geom_col(aes(x = n_retweets, \n               y = reorder(county, n_retweets) ))\n# reorder(CATEGORICAL_VAR, CONTINUOUS_VAR) reorders \n# CATEGORICAL_VAR based on a values of CONTINUOUS_VAR"
  },
  {
    "objectID": "DANL200_lab2a.html#variable-description-1",
    "href": "DANL200_lab2a.html#variable-description-1",
    "title": "R Lab 2 - Data Transformation",
    "section": "Variable Description",
    "text": "Variable Description\nEach observation in beer_markets.csv is a household-level record for one transaction of beer.\n\n\nhh: an identifier of the household;\nX_purchase_desc: details on the purchased item;\nquantity: the number of items purchased;\nbrand: Bud Light, Busch Light, Coors Light, Miller Lite, or Natural Light;\nspent: total dollar value of purchase;\nbeer_floz: total volume of beer, in fluid ounces;\nprice_per_floz: price per fl.oz. (i.e., beer spent/beer floz);\ncontainer: the type of container;\npromo: Whether the item was promoted (coupon or otherwise);\nmarket: Scan-track market (or state if rural);\ndemographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher."
  },
  {
    "objectID": "DANL200_lab2a.html#q2a",
    "href": "DANL200_lab2a.html#q2a",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2a",
    "text": "Q2a\n\nFind the top 5 markets in terms of the total volume of beer.\nFind the top 5 markets in terms of the total volume of BUD LIGHT.\nFind the top 5 markets in terms of the total volume of BUSCH LIGHT.\nFind the top 5 markets in terms of the total volume of COORS LIGHT.\nFind the top 5 markets in terms of the total volume of MILLER LITE.\nFind the top 5 markets in terms of the total volume of NATURAL LIGHT.\n\n\n\n\n\nQ2a <- beer_markets %>% \n   group_by(market) %>% \n   summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n   arrange(-beer_floz_tot) %>% \n   slice(1:5)  # returns the first five rows of the resulting data frame\n\nQ2a\n\n# A tibble: 5 × 2\n  market      beer_floz_tot\n  <chr>               <dbl>\n1 TAMPA              890418\n2 PHOENIX            675318\n3 MIAMI              632684\n4 SAN ANTONIO        631852\n5 CHICAGO            558878\n\n\n\nQ2a_bud <- beer_markets %>% \n  filter(brand == \"BUD LIGHT\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)\nQ2a_bud\n\n# A tibble: 5 × 2\n  market    beer_floz_tot\n  <chr>             <dbl>\n1 PHOENIX          271012\n2 TAMPA            171830\n3 MIAMI            152990\n4 ST. LOUIS        140982\n5 CHARLOTTE        136786\n\nQ2a_busch <- beer_markets %>% \n  filter(brand == \"BUSCH LIGHT\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)\nQ2a_busch\n\n# A tibble: 5 × 2\n  market      beer_floz_tot\n  <chr>               <dbl>\n1 RURAL IOWA         184000\n2 HOUSTON            154896\n3 CHICAGO            118916\n4 TAMPA              110680\n5 MINNEAPOLIS         92992\n\nQ2a_coors <- beer_markets %>% \n  filter(brand == \"COORS LIGHT\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)\nQ2a_coors\n\n# A tibble: 5 × 2\n  market       beer_floz_tot\n  <chr>                <dbl>\n1 TAMPA               146260\n2 RURAL TEXAS         139662\n3 SURBURBAN NY        127291\n4 DENVER              101902\n5 LOS ANGELES         101545\n\nQ2a_miller <- beer_markets %>% \n  filter(brand == \"MILLER LITE\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)\nQ2a_miller\n\n# A tibble: 5 × 2\n  market          beer_floz_tot\n  <chr>                   <dbl>\n1 CHICAGO                339984\n2 MIAMI                  261624\n3 SAN ANTONIO            228088\n4 PHOENIX                194424\n5 RURAL WISCONSIN        169368\n\nQ2a_natural <- beer_markets %>% \n  filter(brand == \"NATURAL LIGHT\") %>% \n  group_by(market) %>% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %>% \n  arrange(-beer_floz_tot) %>% \n  slice(1:5)\nQ2a_natural\n\n# A tibble: 5 × 2\n  market      beer_floz_tot\n  <chr>               <dbl>\n1 TAMPA              311872\n2 SAN ANTONIO        159864\n3 DALLAS             151424\n4 DETROIT            129840\n5 LOS ANGELES        123372"
  },
  {
    "objectID": "DANL200_lab2a.html#q2b",
    "href": "DANL200_lab2a.html#q2b",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2b",
    "text": "Q2b\n\nFor households that purchased BUD LIGHT, what fraction of households did purchase only BUD LIGHT?\nFor households that purchased BUSCH LIGHT, what fraction of households did purchase only BUSCH LIGHT?\nFor households that purchased COORS LIGHT, what fraction of households did purchase only COORS LIGHT?\nFor households that purchased MILLER LITE, what fraction of households did purchase only MILLER LITE?\nFor households that purchased NATURAL LIGHT, what fraction of households did purchase only NATURAL LIGHT?\nWhich beer brand does have the largest base of loyal consumers?\n\n\n# Add columns with binary values for specific beer brands\nbeer_markets <- beer_markets %>% \n  mutate(bud = ifelse(brand==\"BUD LIGHT\", 1, 0), # column for Bud Light\n         busch = ifelse(brand==\"BUSCH LIGHT\", 1, 0), # column for Busch Light\n         coors = ifelse(brand==\"COORS LIGHT\", 1, 0), # column for Coors Light\n         miller = ifelse(brand==\"MILLER LITE\", 1, 0), # column for Miller Lite\n         natural = ifelse(brand==\"NATURAL LIGHT\", 1, 0) ) # column for Natural Light\n\n\nQ2b_bud <- beer_markets %>%\n  select(hh, bud) %>% # select columns 'hh' and 'bud'\n  arrange(hh, -bud) %>% # sort by 'hh' in ascending order and 'bud' in descending order\n  group_by(hh) %>% # group by 'hh'\n  filter(sum(bud) > 0) %>% # keep households that bought Bud Light at least once\n  mutate(frac_bud = sum(bud)/n(), # calculate fraction of purchases that are Bud Light for each household\n         loyal_bud = ifelse(frac_bud == 1, 1, 0)) %>% # classify households as loyal if they only bought Bud Light\n  select(hh, frac_bud, loyal_bud) %>% # select relevant columns\n  unique() %>% # remove duplicate rows\n  ungroup() %>% # remove grouping\n  mutate(n_hh_bud = n()) %>% # count the number of households that bought Bud Light\n  group_by(loyal_bud, n_hh_bud) %>% # group by 'loyal_bud' and 'n_hh_bud'\n  summarise(n_obs = n()) %>% # count the number of observations in each group\n  ungroup() %>% # remove grouping\n  mutate(n_frac = n_obs/n_hh_bud ) # calculate the fraction of households in each group\n# 0.6600816\n\n\nQ2b_busch <- beer_markets %>%\n  select(hh, busch) %>%\n  arrange(hh, -busch) %>%\n  group_by(hh) %>%\n  filter(sum(busch) > 0) %>% \n  mutate(frac_busch = sum(busch)/n(),\n         loyal_busch = ifelse(frac_busch == 1, 1, 0)) %>%\n  select(hh, frac_busch, loyal_busch) %>% \n  unique() %>% \n  ungroup() %>% \n  mutate(n_hh_busch = n()) %>%\n  group_by(loyal_busch, n_hh_busch) %>% \n  summarise(n_obs = n()) %>% \n  ungroup() %>% \n  mutate(n_frac = n_obs/n_hh_busch )  # 0.472973\n\n\nQ2b_coors <- beer_markets %>%\n  select(hh, coors) %>%\n  arrange(hh, -coors) %>%\n  group_by(hh) %>%\n  filter(sum(coors) > 0) %>% \n  mutate(frac_coors = sum(coors)/n(),\n         loyal_coors = ifelse(frac_coors == 1, 1, 0)) %>%\n  select(hh, frac_coors, loyal_coors) %>% \n  unique() %>% \n  ungroup() %>% \n  mutate(n_hh_coors = n()) %>%\n  group_by(loyal_coors, n_hh_coors) %>% \n  summarise(n_obs = n()) %>% \n  ungroup() %>% \n  mutate(n_frac = n_obs/n_hh_coors )  # 0.6390805\n\n\nQ2b_miller <- beer_markets %>%\n  select(hh, miller) %>%\n  arrange(hh, -miller) %>%\n  group_by(hh) %>%\n  filter(sum(miller) > 0) %>% \n  mutate(frac_miller = sum(miller)/n(),\n         loyal_miller = ifelse(frac_miller == 1, 1, 0)) %>%\n  select(hh, frac_miller, loyal_miller) %>% \n  unique() %>% \n  ungroup() %>% \n  mutate(n_hh_miller = n()) %>%\n  group_by(loyal_miller, n_hh_miller) %>% \n  summarise(n_obs = n()) %>% \n  ungroup() %>% \n  mutate(n_frac = n_obs/n_hh_miller )  # 0.6312989\n\nQ2b_natural <- beer_markets %>%\n  select(hh, natural) %>%\n  arrange(hh, -natural) %>%\n  group_by(hh) %>%\n  filter(sum(natural) > 0) %>% \n  mutate(frac_natural = sum(natural)/n(),\n         loyal_natural = ifelse(frac_natural == 1, 1, 0)) %>%\n  select(hh, frac_natural, loyal_natural) %>% \n  unique() %>% \n  ungroup() %>% \n  mutate(n_hh_natural = n()) %>%\n  group_by(loyal_natural, n_hh_natural) %>% \n  summarise(n_obs = n()) %>% \n  ungroup() %>% \n  mutate(n_frac = n_obs/n_hh_natural )  # 0.5096234\n\n# Here I do not provide any comments on the result.\n\n\nHere is another example:\n\n\nQ2b <- beer_markets %>% \n  filter( brand == \"BUD LIGHT\") %>% \n  select(hh, brand) %>% \n  distinct() %>% \n  mutate(n_quantity = n()) \n\n# How many household purchased BUD LIGHT?\n# 4657\n\n\n# How many household purchased only BUD LIGHT?\nQ2b <- beer_markets %>% \n  group_by(hh, brand) %>% \n  mutate(n_brand = n()) %>%  # Calculate the number of beer transactions for each household (hh) for each beer brand\n  group_by(hh) %>% \n  mutate(n_transac = n()) %>%   # Calculate the total number of beer transactions for each household\n  arrange(hh, brand) %>% \n  filter((n_brand/n_transac) == 1, brand == \"BUD LIGHT\") %>%   # Filter for households that only bought Bud Light\n  select(hh) %>% \n  distinct()  # Select the distinct household ids\n\n# How many household purchased only BUD LIGHT?\n# 3074\n \n\n# fraction of hh that purchased only BUD LIGHT is 3074/4657= 0.66"
  },
  {
    "objectID": "DANL200_lab2a.html#q2c",
    "href": "DANL200_lab2a.html#q2c",
    "title": "R Lab 2 - Data Transformation",
    "section": "Q2c",
    "text": "Q2c\n\nCalculate the number of beer transactions for each household.\nCalculate the fraction of each beer brand for each household.\n\n\nQ2c <- beer_markets %>% \n  count(hh, brand) %>% \n  group_by(hh) %>% \n  mutate(n_tot = sum(n)) %>%   # calculates the total number of transactions for each household\n  arrange(hh, brand) %>% \n  mutate(prop = n / n_tot)   # calculates the proportion of transactions for each beer brand in each household"
  },
  {
    "objectID": "DANL210_lab7a.html",
    "href": "DANL210_lab7a.html",
    "title": "Python Lab 7 - Web-scrapping 2 Example Answers",
    "section": "",
    "text": "Let’s do web-scrapping!"
  },
  {
    "objectID": "DANL210_lab7a.html#load-libraries",
    "href": "DANL210_lab7a.html#load-libraries",
    "title": "Python Lab 7 - Web-scrapping 2 Example Answers",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nimport pandas as pd\nfrom selenium import webdriver"
  },
  {
    "objectID": "DANL210_lab7a.html#q1a",
    "href": "DANL210_lab7a.html#q1a",
    "title": "Python Lab 7 - Web-scrapping 2 Example Answers",
    "section": "Q1a",
    "text": "Q1a\nUse (1) double for-loop (nested for-loop), (2) pd.DataFrame([]), (3) pd.concat(), (4) DataFrame.append(), and (5) DataFrame.to_csv() to export data in the table as the CSV file.\n\n# %%\nimport os\nwd_path = '/Users/byeong-hakchoe/Google Drive/suny-geneseo/spring2023/lecture_codes/'\nos.chdir(wd_path)  \nos.getcwd()\n\n\n# %%\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\noptions = Options()\noptions.add_argument(\"window-size=1400,1200\")\ndriver_path = \"/Users/byeong-hakchoe/Downloads/chromedriver_mac64 (1)/chromedriver\"\ndriver = webdriver.Chrome(chrome_options = options, \n                          executable_path = driver_path)\n\n\nurl = \"https://www.eia.gov/petroleum/gasdiesel/gaspump_hist.php\"\ndriver.get(url)\ntable = driver.find_element(By.TAG_NAME, \"tbody\")\nrow = table.find_elements(By.TAG_NAME, \"tr\")\nthead = driver.find_element(By.TAG_NAME, \"thead\")\ncol = thead.find_elements(By.TAG_NAME, \"th\")\n\n# %%\nimport pandas as pd\ndf1 = pd.DataFrame()\ndf2 = pd.DataFrame()\ndata = pd.DataFrame()\nfor i in range(1, len(row) + 1 ):\n    for j in range(1, len(col) + 1):\n        data = driver.find_element(By.XPATH, '/html/body/div[1]/div[2]/div/div[4]/div/div[2]/div/table/tbody/tr['+str(i)+']/td['+str(j)+']').text\n        data = pd.DataFrame([data])\n        df1 = pd.concat([df1, data], axis = 1)\n        data = pd.DataFrame()\n    df2 = df2.append(df1)\n    df1 = pd.DataFrame()\n\ncoln = []\nfor j in range(1, len(ncol) + 1):\n    col = driver.find_element(By.XPATH, '/html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/thead/tr/th['+str(j)+']').text\n    coln.append(col)\n    \n\ndf2.columns = coln\ndf2.to_csv(\"data/table_eia_20230425.csv\",\n           index = False)"
  },
  {
    "objectID": "DANL210_lab7a.html#q1b",
    "href": "DANL210_lab7a.html#q1b",
    "title": "Python Lab 7 - Web-scrapping 2 Example Answers",
    "section": "Q1b",
    "text": "Q1b\n\nVisualize the monthly trend of the retail oil price using seaborn.\n\nWhen using to_datetime(), please consider using format with the following directives:\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf2.columns\ndf2.dtypes\n\ndf2['Mon-yr'] = pd.to_datetime(df2['Mon-yr'], format='%b-%y') # error\n\ndf2['Mon-yr'] = df2['Mon-yr'].str.replace('Sept', 'Sep')\ndf2['Mon-yr'] = pd.to_datetime(df2['Mon-yr'], format='%b-%y') # need to know the format \n\ndf2['Retail Price\\n(Dollars per gallon)'] = pd.to_numeric(df2['Retail Price\\n(Dollars per gallon)'])\n\n\n\nimport seaborn as sns\n\nsns.lineplot(data = df2,\n             x = 'Mon-yr',\n             y = 'Retail Price\\n(Dollars per gallon)')"
  },
  {
    "objectID": "DANL200_hw2a.html",
    "href": "DANL200_hw2a.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "",
    "text": "Load R packages you need for Homework Assignment 2"
  },
  {
    "objectID": "DANL200_hw2a.html#q1a",
    "href": "DANL200_hw2a.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1a",
    "text": "Q1a\nProvide both ggplot code and a simple comment to describe the yearly trend of GHG emissions for each sector.\n\nHere we can consider the yearly total GHG emissions for each sector:\n\n\nghg_emissions %>% \n  group_by(Sector, Year) %>% # Group by sector and year\n  summarise(GHG_emissions = sum(GHG_emissions, na.rm = T)) %>% # Sum GHG emissions for each sector and year\n  ggplot(aes(x = Year, y = GHG_emissions)) + # Create ggplot with Year on x-axis and GHG emissions on y-axis\n  geom_line(aes(color = Sector)) + # Add a line for each sector with a different color\n  geom_point(color = 'black', size = .25) + # Add black points for each year\n  facet_wrap(.~ Sector, scales = 'free_y')\n\n\n\n\n\n\n\nscales = 'free_y' allows the scale of y-axis to vary by subplots.\nscales = 'free_x' allows the scale of x-axis to vary by subplots.\nscales = 'free' allows the scale of both x-axis and y-axis to vary by subplots."
  },
  {
    "objectID": "DANL200_hw2a.html#q1b",
    "href": "DANL200_hw2a.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1b",
    "text": "Q1b\nProvide both ggplot code and a simple comment to describe the yearly trend of United States of America’s GHG emissions for each sector.\n\nghg_emissions %>% \n  filter(Party == \"United States of America\") %>% \n  ggplot(aes(x = Year, y = GHG_emissions)) +\n  geom_line(aes(color = Sector)) +\n  geom_point(color = 'black', size = .25) +\n  facet_wrap(.~ Sector, scales = 'free_y')"
  },
  {
    "objectID": "DANL200_hw2a.html#q1c",
    "href": "DANL200_hw2a.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1c",
    "text": "Q1c\nFor each party, calculate the yearly percentage change in GHG emissions for each sector.\n\nq1c <- ghg_emissions %>% \n  group_by(Party, Sector) %>% \n  # Calculate percentage change between consecutive years\n  mutate(pct = (GHG_emissions - lag(GHG_emissions)) / abs(lag(GHG_emissions)),\n         # Round the percentage change to 2 decimal places\n         pct2 = round(100 * pct, 2) )\n\n\nq1c"
  },
  {
    "objectID": "DANL200_hw2a.html#q1d",
    "href": "DANL200_hw2a.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1d",
    "text": "Q1d\nWhich party has reduced GHG emissions most from 1990 level to 2017 level in terms of the percentage change in GHG emissions?\n\nq1d <- ghg_emissions %>% \n  filter(Year %in% c(1990, 2017)) %>% \n  group_by(Party, Year) %>% \n  summarize(GHG_emissions = sum(GHG_emissions, na.rm = T)) %>% \n  group_by(Party) %>%  # this group_by() is not necessary\n  mutate(pct = (GHG_emissions - lag(GHG_emissions)) / abs(lag(GHG_emissions)) ) %>% \n  filter(!is.na(pct)) %>% \n  arrange(pct)\n\n\nq1d"
  },
  {
    "objectID": "DANL200_hw2a.html#q1e",
    "href": "DANL200_hw2a.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q1e",
    "text": "Q1e\nWhich sector has reduced GHG emissions most from 1990 level to 2017 level in terms of the percentage change in GHG emissions?\n\nq1e <- ghg_emissions %>% \n  filter(Year %in% c(1990, 2017)) %>% \n  group_by(Sector, Year) %>% \n  summarize(GHG_emissions = sum(GHG_emissions, na.rm = T)) %>% \n  group_by(Sector) %>%  # this group_by() is not necessary\n  mutate(pct = (GHG_emissions - lag(GHG_emissions)) / abs(lag(GHG_emissions)) ) %>% \n  filter(!is.na(pct)) %>% \n  arrange(pct) \n\n\nq1e"
  },
  {
    "objectID": "DANL200_hw2a.html#q2a",
    "href": "DANL200_hw2a.html#q2a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2a",
    "text": "Q2a\nHow many parties have provided or disbursed positive funding contributions to other countries or regions for their adaptation projects for every single year from 2011 to 2018?\n\nq2a <- climate_finance %>% \n  filter(Status == \"provided\" | Status == \"disbursed\",  # filter by provided or disbursed status\n         `Type of support` == \"adaptation\") %>%       # filter by adaptation type\n  group_by(Party, Year) %>%                          # group by Party and Year\n  summarise(Contribution = sum(Contribution, na.rm = T)) %>%  # calculate total contribution\n  filter(Contribution > 0) %>%                       # keep rows with non-zero contribution\n  group_by(Party) %>%                                # group by Party (unnecessary step)\n  count() %>%                                        # count number of observations\n  filter(n == 8)                                     # keep Parties with 8 observations\n\n\nq2a\n\n\n\n\n\n  \n\n\n\n\nnrow(q2a)\n\n[1] 11"
  },
  {
    "objectID": "DANL200_hw2a.html#q2b",
    "href": "DANL200_hw2a.html#q2b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\nFor each party, calculate the total funding contributions that were disbursed or provided for mitigation projects for each year.\n\nq2b <- climate_finance %>% \n  filter(Status == \"provided\" | Status == \"disbursed\",\n         `Type of support` == \"mitigation\") %>% \n  group_by(Party, Year) %>% \n  summarise(Contribution = sum(Contribution, na.rm = T))\n\n\nq2b"
  },
  {
    "objectID": "DANL200_hw2a.html#q2c",
    "href": "DANL200_hw2a.html#q2c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2c",
    "text": "Q2c\nFor each party, calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each year.\n\nq2c <- climate_finance %>% \n  group_by(Party, Year, Status, `Type of support`) %>% \n  summarise(Contribution = sum(Contribution, na.rm = T)) %>% \n  filter(Contribution != 0) %>% \n  group_by(Party, Year, Status) %>% # this group_by() is not necessary, but it doesn't hurt\n  mutate(lag_Contribution = lag(Contribution), # calculate lagged Contribution\n         am_ratio = lag_Contribution / Contribution ) %>% # calculate adaptation/mitigation ratio\n  filter(!is.na(am_ratio)) %>% # remove rows with missing values in am_ratio\n  rename(mitigation = Contribution, # rename columns for clarity\n         adaptation = lag_Contribution) %>% \n  select(-`Type of support`) # remove Type of support column\n\n\nq2c\n\n\n\n\n\n  \n\n\n\n\nHere is another example:\n\n\nq2c <- climate_finance %>% \n  group_by(Party, Year, Status) %>%  # group by Party, Year and Status\n  summarise(adaptation = sum(Contribution[`Type of support` == 'adaptation'], na.rm = T), # sum the contributions where the Type of support is adaptation\n            mitigation = sum(Contribution[`Type of support` == 'mitigation'], na.rm = T) # sum the contributions where the Type of support is mitigation\n            ) %>% \n  filter(adaptation != 0, mitigation != 0) %>% # filter out rows where either adaptation or mitigation is 0\n  mutate(am_ratio = adaptation / mitigation ) # calculate the ratio of adaptation to mitigation and store it in a new column named am_ratio\n\n\nq2c"
  },
  {
    "objectID": "DANL200_hw2a.html#q2d",
    "href": "DANL200_hw2a.html#q2d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2d",
    "text": "Q2d\nProvide both ggplot code and a simple comment to describe the distribution of the ratio between adaptation contribution and mitigation contribution, which is calculated in Q2c.\n\nggplot(q2c, aes(x = am_ratio)) +\n  geom_histogram(bins = 100) +\n  geom_vline(xintercept = 1, color = 'red', lty = 2)\n\n\n\n\n\nggplot(q2c, aes(x = log(am_ratio))) +\n  geom_histogram(bins = 100) +\n  geom_vline(xintercept = 0, color = 'red', lty = 2)\n\n\n\n\n\nggplot(q2c, aes(x = am_ratio)) +\n  geom_density() +\n  geom_vline(xintercept = 1, color = 'red', lty = 2)\n\n\n\n\n\nggplot(q2c, aes(x = log(am_ratio))) +\n  geom_density() +\n  geom_vline(xintercept = 0, color = 'red', lty = 2)"
  },
  {
    "objectID": "DANL200_hw2a.html#q2e",
    "href": "DANL200_hw2a.html#q2e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 2",
    "section": "Q2e",
    "text": "Q2e\nProvide both ggplot code and a simple comment to describe how the distribution of Contribution varies by Type of support and Status.\n\nggplot(climate_finance,\n       aes(color = `Type of support`, x = log(Contribution))) +\n  geom_density(show.legend = F) +\n  facet_wrap(.~ Status)\n\n\n\n\n\nggplot(climate_finance,\n       aes(color = `Type of support`, x = log(Contribution))) +\n  geom_freqpoly() +\n  facet_wrap(.~ Status)\n\n\nggplot(climate_finance,\n       aes(color = `Type of support`, x = log(Contribution))) +\n  geom_freqpoly() +\n  facet_wrap(.~ Status) +\n  theme(legend.position = 'top')"
  },
  {
    "objectID": "DANL210_hw5q.html",
    "href": "DANL210_hw5q.html",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 5",
    "section": "",
    "text": "Go to the following website about Climate Change Knowledge Portal at the World Bank\n\nhttps://climateknowledgeportal.worldbank.org/download-data.\n\n\n\n\n\n\n\nShow the code\noptions = webdriver.ChromeOptions()\ndownload_folder = \"/Users/byeong-hakchoe/Google Drive/suny-geneseo/spring2023/lecture_codes/data\"\np = {\"download.default_directory\" : download_folder, \"safebrowsing.enabled\" : \"false\"}\noptions.add_experimental_option(\"prefs\", p)\ndriver = webdriver.Chrome(chrome_options = options, \n                          executable_path = \"chromedriver\")\n\n\n\n\n\n\n\n\nShow the code\nimport os\nfile_list = os.listdir(download_folder)\n\n\n\n\n\n\n\nProvide your Python Selenium code to log in with public login.\n\nI suggest you to sign in with your LinkedIn account.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProvide your Python Selenium code to download the CSV file for the COLLECTION CMIP6 (Mean Projections)’s 50th PERCENTILE of Max-Temperature VARIABLE in the Historical Reference Period, 1995-2014 TIME PERIOD for each country in the COUNTRY menu for each of the three SCENARIOs, SSP1-1.9, SSP1-2.6, and SSP2-4.5, from the Multi-Model Ensemble MODEL.\nThe Selenium code should do the followings:\n\nGo to the TIMESERIES tab.\nSelect the proper menus.\nLoop over the COUNTRY menu.\n\n\nIn the loop, use time.sleep(10) for each download.\nConsider using try-except to prevent the Selenium code from getting errors.\n\n\n\n\n\n\n\nProvide your Python Selenium code to download the CSV file for the COLLECTION CMIP6 (Mean Projections)’s 50th PERCENTILE of Max-Temperature VARIABLE in the Projections, 2015-2100 TIME PERIOD for each country in COUNTRY menu for each of the three SCENARIOs, SSP1-1.9, SSP1-2.6, and SSP2-4.5, from the Multi-Model Ensemble.\nThe Selenium code should do the followings:\n\nGo to TIMESERIES tab.\nSelect the proper menus.\nLoop over the COUNTRY menu and the SCENARIO menu.\n\n\nIn the loop, use time.sleep(10) for each download.\nAlways consider using try-except to prevent the Selenium code from getting errors.\n\n\n\n\n\n\n\nFrom the downloaded CSV files, consider only the first two columns for Q1d.\n\nThe first two columns in the downloaded CSV file are year and average temperature in a country.\nThe rest of the columns in the downloaded CSV file is about temperatures in several regions in a country.\n\nCreate the following two DataFrames.\n\n\nThe tidy year-country level DataFrame for Median (50th) level of Max-Temperature in Historical Reference Period, 1995-2014 for all the three SCENARIOs, SSP1-1.9, SSP1-2.6, and SSP2-4.5.\n\n\nThe tidy year-country level DataFrame for Median (50th) level of Max-Temperature in the time period, Projection, 2015-2100 for all the three SCENARIOs, SSP1-1.9, SSP1-2.6, and SSP2-4.5.\n\n\n\n\n\n\n\n\nCombine the two tidy DataFrames.\nMerge the combined tidy DataFrame with the DataFrame for country groups, provided by the following CSV file:\n\nhttps://bcdanl.github.io/data/worldbank_country_group.csv\n\nProvide both (1) seaborn code and (2) a simple comment to describe how the yearly trend of mean temperature varies by region and scenario."
  },
  {
    "objectID": "DANL310_lab3q.html",
    "href": "DANL310_lab3q.html",
    "title": "R Visualization Lab 3",
    "section": "",
    "text": "Loading R packages\n\nlibrary(tidyverse)\nlibrary(socviz)\nlibrary(lubridate)\nlibrary(geofacet)\n\n\n\nQuestion 1 - Bar Charts\nThe following data is for Question 1:\n\ntitanic <- read_csv(\n  'https://bcdanl.github.io/data/titanic_cleaned.csv')\n\n\n\n\n\n  \n\n\n\n\n\nReplicate the following ggplot.\n\n\n\n\n\n\n\n\n\nQuestion 2 - Bar Charts 2\nThe following data is for Question 2:\n\nnyc_flights <- read_csv(\n  'https://bcdanl.github.io/data/nyc_flights_grouped.csv')\n\n\n\n\n\n  \n\n\n\n\nReplicate the following ggplot.\n\n\n\n\n\n\n\n\n\nQuestion 3 - Stocks\nThe following data is for Question 3:\n\nstock = read_csv('https://bcdanl.github.io/data/stocks2013_2023.csv')\n\n\n\n\n\n  \n\n\n\n\nReplicate the following ggplot.\n\nFor each company, the normalized close price on each date is the company’s close price on each date divided by the company’s close price on the first date (2013-01-02).\n\n\n\\[\n\\text{(Normalized Close Price)}_{\\,\\text{company},\\,\\text{date}}\\,=\\,\\frac{\\text{Close}_{\\,\\text{company},\\,\\text{date} }}{\\text{Close}_{\\,\\text{company},\\,\\text{2013-01-02} }}\n\\]\n\nThe normalized closing price on the first date (2013-01-02) is 1 for each company.\ny-axis represents the log of the normalized closing price."
  },
  {
    "objectID": "DANL200_lab3q.html",
    "href": "DANL200_lab3q.html",
    "title": "R Lab 3 - Pivoting",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab3q.html#q1a",
    "href": "DANL200_lab3q.html#q1a",
    "title": "R Lab 3 - Pivoting",
    "section": "Q1a",
    "text": "Q1a\n\nDescribe how the distribution of rating varies across week 1, week 2, and week 3 using the faceted histogram."
  },
  {
    "objectID": "DANL200_lab3q.html#q1b",
    "href": "DANL200_lab3q.html#q1b",
    "title": "R Lab 3 - Pivoting",
    "section": "Q1b",
    "text": "Q1b\n\nWhich artist(s) have the most number of tracks in billboard data.frame?"
  },
  {
    "objectID": "DANL200_lab3q.html#q2a",
    "href": "DANL200_lab3q.html#q2a",
    "title": "R Lab 3 - Pivoting",
    "section": "Q2a",
    "text": "Q2a\nMake ny_pincp longer."
  },
  {
    "objectID": "DANL200_lab3q.html#q2b",
    "href": "DANL200_lab3q.html#q2b",
    "title": "R Lab 3 - Pivoting",
    "section": "Q2b",
    "text": "Q2b\nProvide both (1) ggplot code and (2) a simple comment to describe how overall the yearly trend of NY counties’ average personal incomes are."
  },
  {
    "objectID": "DANL200_lab3q.html#q3a",
    "href": "DANL200_lab3q.html#q3a",
    "title": "R Lab 3 - Pivoting",
    "section": "Q3a",
    "text": "Q3a\n\nKeep only the following three variables, date, countriesAndTerritories, and cases.\nThen make a wide-form data.frame of covid whose variable names are from countriesAndTerritories and values are from cases.\nThen drop the variable date."
  },
  {
    "objectID": "DANL200_lab3q.html#q3b",
    "href": "DANL200_lab3q.html#q3b",
    "title": "R Lab 3 - Pivoting",
    "section": "Q3b",
    "text": "Q3b\n\nUse the wide-form data.frame of covid to find the top 10 countries in terms of the correlation between their cases and the US case.\n\nFirst, keep only\nUse cor()"
  },
  {
    "objectID": "mba-ch3-regularization.html#bias-variance-tradeoff",
    "href": "mba-ch3-regularization.html#bias-variance-tradeoff",
    "title": "Regularized Regression",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nknitr::include_graphics('lec_figs/bias-variance-tradeoff-1.png')\n\n\n\n\n\n\n\n\n\nDecomposition of MSE\n\nThe mean squared error (MSE) of the model with unknown set of parameters \\(\\mathbf{\\theta}\\) can be decomposed into bias and variance.\n\n\\[\n\\begin{align}\nM S E &\\,=\\, \\frac{\\sum_{i = 1}^{ n}\\,(\\, y_{i} - \\hat{y}_{i} \\,)^2}{n}\\\\\n&\\,=\\,E \\,[\\, y_{i} - \\hat{f}(\\mathbf{X}) \\,]^{2}\\\\\n&\\,=\\, [\\text{Bias}(\\, \\hat{f}(\\mathbf{X}) \\,)]^{2} \\,+\\, Var(\\,\\hat{f}(\\mathbf{X})\\,) \\,+\\, \\text{Var}(\\epsilon)\n\\end{align}\n\\]\n\nIn order to minimize the MSE above, we need to select a machine learning methods that simultaneously achieves low variance and low bias.\n\nNote that both squared bias and variance are nonnegative.\nThe MSE can never lie below \\(\\text{Var}(\\epsilon)\\), the irreducible error.\n\n\n\nknitr::include_graphics('lec_figs/bias-variance-tradeoff-2.png')\n\n\n\n\n\n\n\n\n\nThe horizontal dashed line represents \\(\\text{Var}(\\epsilon)\\), the irreducible error.\nThe vertical dotted line indicates the flexibility level corresponding to the smallest test MSE.\n\n\nknitr::include_graphics('lec_figs/bias-variance-tradeoff-3.png')"
  },
  {
    "objectID": "mba-ch3-regularization.html#regularization",
    "href": "mba-ch3-regularization.html#regularization",
    "title": "Regularized Regression",
    "section": "Regularization",
    "text": "Regularization\n\nRegularization helps find some optimal amount of flexibility that minimizes MSE.\nRegularized regression can resolve the following problems\n\nQuasi-separation in logistic regression:\n\nThe input variable yields an almost perfect prediction of the binary response variable.\n\nMulticolinearity in regression:\n\nVariables age and years_of_workforce in linear regression of income are too highly correlated.\n\n\n\n\nRidge Regression\n\nRidge regression introduces a shrinkage penalty \\(\\lambda \\geq 0\\) by minimizing:\n\n\\[\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j x_{ij}\\big)^2 + \\lambda \\sum_j^p \\beta_j^2 = \\text{SSE} + \\lambda \\sum_j^p \\beta_j^2\\] - As \\(\\lambda\\) increases \\(\\Rightarrow\\) flexibility of models decreases\n\nincreases bias, but decreases variance\nFor fixed value of \\(\\lambda\\), ridge regression fits only a single model\n\nneed to use cross-validation to tune \\(\\lambda\\)\n\nFor example: note how the magnitude of the coefficient for Income trends as \\(\\lambda \\rightarrow \\infty\\)\n\n\n\n\n\n\n\n\n\n\n\nThe coefficient shrinks towards zero, but never actually reaches it.\n\nRidge regression tends to average the collinear variables together.\n\nIncome is always a variable in the learned model, regardless of the value of \\(\\lambda\\).\n\n\n\n\nLasso Regression\n\nRidge regression keeps all variables.\nBut Lasso regression gets lid of variables.\n\nLasso enables variable selection with \\(\\lambda\\) by minimizing:\n\n\n\\[\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j X_{ij}\\big)^2 + \\lambda \\sum_j^p\\vert  \\beta_j \\vert = \\text{SSE} + \\lambda \\sum_j^p \\vert \\beta_j \\vert\\]\n\nAs \\(\\lambda\\) increases \\(\\Rightarrow\\) flexibility of models decreases\n\nincreases bias, but decreases variance\n\nLasso can handle the \\(p > n\\) case, i.e. more variables than observations!\nLasso regression performs variable selection yielding sparse models, which only includes a small subset of the available variables that are relevant to predicting the outcome of interest.\n\n\n\n\n\n\n\n\n\n\n\nThe coefficient shrinks towards and eventually equals zero at \\(\\lambda \\approx 1000\\)\nIf the optimum value of \\(\\lambda\\) is larger, then Income would NOT be included in the learned model.\n\n\n\n\nElastic net\n\\[\\sum_{i}^{n}\\left(Y_{i}-\\beta_{0}-\\sum_{j}^{p} \\beta_{j} X_{i j}\\right)^{2}+\\lambda\\left[(1-\\alpha)\\|\\beta\\|_{2}^{2} / 2+\\alpha\\|\\beta\\|_{1} \\right]\\] - \\(\\vert \\vert \\beta \\vert \\vert_1\\) is the \\(\\ell_1\\) norm: \\(\\vert \\vert \\beta \\vert \\vert_1 = \\sum_j^p \\vert \\beta_j \\vert\\)\n\n\\(\\vert \\vert \\beta \\vert \\vert_2\\) is the \\(\\ell_2\\), Euclidean, norm: \\(\\vert \\vert \\beta \\vert \\vert_2 = \\sqrt{\\sum_j^p \\beta_j^2}\\)\nRidge penalty: \\(\\lambda \\cdot (1 - \\alpha) / 2\\)\nLasso penalty: \\(\\lambda \\cdot \\alpha\\)\n\\(\\alpha\\) controls the mixing between the two types, ranges from 0 to 1\n\n\\(\\alpha = 1\\) returns lasso\n\\(\\alpha = 0\\) return ridge\n\n\n\n\n\nGamma Lasso Regression\n\\[\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j X_{ij}\\big)^2 + \\lambda \\sum_j^p\\vert  \\beta_j \\vert = \\text{SSE} + \\lambda \\sum_j^p \\log(\\, 1 + \\vert \\beta_j \\vert\\,)\\]"
  },
  {
    "objectID": "mba-ch3-regularization.html#putting-a-cost-on-complexity",
    "href": "mba-ch3-regularization.html#putting-a-cost-on-complexity",
    "title": "Regularized Regression",
    "section": "Putting a Cost on Complexity",
    "text": "Putting a Cost on Complexity\n\nWith regularization, we put a cost on the magnitude of each \\(\\beta_{p}\\).\n\nThis penalizes complexity, because the \\(\\beta_{p}\\) coefficients are what allow our predicted \\(\\hat{y}\\) values to move around with different input \\(X\\) values.\n\nIf we force all the \\(\\hat{\\beta}_{p}\\) to be close to zero, then our \\(\\hat{y}\\) values will be shrunk toward \\(\\bar{y}\\) and when we jitter the data your predictions will not change as much as they would if we did not include a penalty term during estimation.\n\\(\\lambda\\) is the penalty weight that determines the price of complexity.\n\nIt is a tuning parameter that needs to be selected in some data-dependent manner.\n\n\nknitr::include_graphics('lec_figs/mba-3-4.png')\n\n\n\n\n\n\n\n\nThe ridge penalty (\\(\\beta^{2}\\)) places little penalty on small values of \\(\\beta\\).\nThe Lasso penalty (\\(\\vert \\beta \\vert\\)) places a constant penalty on incremental deviations from zero.\nThe gamma Lasso penalty (\\(\\log(1 + \\vert \\beta \\vert)\\)) places extreme cost on the move from zero to small values of \\(\\beta\\), but for large values the rate of penalty change is small.\n\nThis encourages lots of zeros in our fit while allowing large betas to be estimated without any bias!\n\n\n\n\nAdvantages of the Lasso\n\nThe Lasso gives the least possible amount of bias on large signals while still retaining the stability of a convex penalty like ridge (convex means that the penalty does not flatten out for large values.).\nThe Lasso will yield automatic variable screening—model selection—some of the \\(\\hat{\\beta}_{p}\\) will be zero!\n\n\nknitr::include_graphics('lec_figs/ridge-lasso-animation.gif')\n\n\n\n\n\nSource: Quora\nHere is another illustration of the Lasso and its path in 2D."
  },
  {
    "objectID": "mba-ch3-regularization.html#regularization-algorithms",
    "href": "mba-ch3-regularization.html#regularization-algorithms",
    "title": "Regularized Regression",
    "section": "Regularization Algorithms",
    "text": "Regularization Algorithms\n\nStandardizing Data\n\nFor either ridge, lasso, gamma lasso or elastic net: we should consider standardizing our data\nCommon convention: within each column, compute then subtract off the sample mean, and compute the divide off the sample standard deviation:\n\n\\[\\tilde{x}_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_{x,j}}\\]\n\nglmnet and gamlr package do this by default and reports coefficients on the original scale\n\n\\(\\lambda\\) and \\(\\alpha\\) are tuning parameters\nWhen using glmnet, the cv.glmnet() function will perform the cross-validation for us\n\ngamlr package does implements the gamma lasso algorithm.\n\nWhen using gamlr, the cv.gamlr() function will perform the cross-validation for us.\n\nglmnetUtils enables us to use a data.frame to provide data to the model.\nglmnet and gamlr use a sparse.matrix, instead of a data.frame, to provide data to the model.\nA sparse matrix is a matrix with many zero entries.\n\nA sparse matrix is almost essential in big data analysis because of its lower storage costs and faster computation.\n\n\n\nknitr::include_graphics('lec_figs/sparse-matrix.png')\n\n\n\n\n\n\n\nk-fold Cross-Validation\n\nSplit the data into k random and roughly evenly sized subsets, called folds. Then, for :\n\nStep 1. Use all data except the k-th fold to train a model\nStep 2. Record the error rate (e.g., MSE, \\(R^2\\), AICc) for predictions on the left-out fold based on the fitted model (out-of-sample (OOS) deviance).\n\n\n\nknitr::include_graphics('lec_figs/pds_fig69.png')\n\n\n\n\n\nFor each candidate model, \\(m \\in\\{1, \\cdots, M\\}\\), with \\(\\lambda_{m}\\), we do k-fold cross-validation (CV).\nSo k-fold CV will yield a set of k OOS deviances for each of our candidate models.\n\nSo we select the model with the best OOS performance!\n\nHow should we choose k?\n\nMore is better but only up to a point. \nWe can experiment on the choice of k.\n\n\n\n\n\nSchematic of cv.glmnet() and cv.gamlr()\n\nknitr::include_graphics('lec_figs/pds_fig718.png')\n\n\n\n\n\n\\(\\lambda_{.min}\\): the \\(\\lambda\\) for the model with the minimum cross-validation (CV) error.\n\\(\\lambda_{1se}\\): corresponds to the model with cross-validation error, which is one standard error (se) of CV error above the minimum CV error.\n\n\n\n\nSchematic of cva.glmnet()\n\nknitr::include_graphics('lec_figs/pds_fig721.png')"
  },
  {
    "objectID": "mba-ch3-regularization.html#references",
    "href": "mba-ch3-regularization.html#references",
    "title": "Regularized Regression",
    "section": "References",
    "text": "References\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani and Jerome Friedman.\n\n\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\nPractical Data Science with R by Nina Zumel and John Mount.\nSummer Undergraduate Research Experience (SURE) 2022 in Statistics at Carnegie Mellon University by Ron Yurko."
  },
  {
    "objectID": "DANL210_lab6q.html",
    "href": "DANL210_lab6q.html",
    "title": "Python Lab 6 - Web-scrapping 1",
    "section": "",
    "text": "import pandas as pd\nfrom selenium import webdriver"
  },
  {
    "objectID": "DANL210_lab6q.html#q1a",
    "href": "DANL210_lab6q.html#q1a",
    "title": "Python Lab 6 - Web-scrapping 1",
    "section": "Q1a",
    "text": "Q1a\n\nGet the number of rows in the second table."
  },
  {
    "objectID": "DANL210_lab6q.html#q1b",
    "href": "DANL210_lab6q.html#q1b",
    "title": "Python Lab 6 - Web-scrapping 1",
    "section": "Q1b",
    "text": "Q1b\n\nFind the XPath for each cell in the first two rows of the second table."
  },
  {
    "objectID": "DANL210_lab6q.html#q1c",
    "href": "DANL210_lab6q.html#q1c",
    "title": "Python Lab 6 - Web-scrapping 1",
    "section": "Q1c",
    "text": "Q1c\n\nUse (1) for-loop, (2) pd.DataFrame([]), (3) pd.concat(), (4) DataFrame.append(), and (5) DataFrame.to_csv() to export data in the second table as the CSV file."
  },
  {
    "objectID": "DANL310_hw1a.html",
    "href": "DANL310_hw1a.html",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "",
    "text": "Renovate your personal website on GitHub using Quarto.\n\nFAQ about Quarto for R Markdown users are provided below: https://quarto.org/docs/faq/rmarkdown.html\nA guide for creating a Quarto website is provided in the following webpage: https://quarto.org/docs/websites/.\nLecture 7 provides a guide to creating a Quarto website on GitHub."
  },
  {
    "objectID": "DANL310_hw1a.html#q2a.",
    "href": "DANL310_hw1a.html#q2a.",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2a.",
    "text": "Q2a.\nUse the following data.frame for Q2a, Q2b, and Q2c.\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')\n\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')\n\nggplot(ncdc_temp, aes(x = date, y = temperature, color = location)) +\n\ngeom_line(size = 1) + # Adds a layer to the ggplot object with a line plot of the temperature data, with a size of 1.\n\nscale_x_date(name = \"month\", limits = c(ymd(\"0000-01-01\"), ymd(\"0001-01-04\")), # Adds a scale to the x-axis with the label \"month\" and limits of Jan 1, 0000 to Jan 4, 0001, and breaks at the beginning of each quarter (Jan, Apr, Jul, Oct), with corresponding labels.\nbreaks = c(ymd(\"0000-01-01\"), ymd(\"0000-04-01\"), ymd(\"0000-07-01\"),\nymd(\"0000-10-01\"), ymd(\"0001-01-01\")),\nlabels = c(\"Jan\", \"Apr\", \"Jul\", \"Oct\", \"Jan\"), expand = c(1/366, 0)) +\n\nscale_y_continuous(limits = c(19.9, 107), # Adds a scale to the y-axis with limits of 19.9 to 107, breaks at every 20 units, and label \"temperature (°F)\".\nbreaks = seq(20, 100, by = 20),\nname = \"temperature (°F)\") +\n\ntheme(legend.title.align = 0.5) # Adjusts the alignment of the legend title to be centered."
  },
  {
    "objectID": "DANL310_hw1a.html#q2b",
    "href": "DANL310_hw1a.html#q2b",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2b",
    "text": "Q2b\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')\n\np <- ggplot(ncdc_temp, aes(x = month, y = temperature)) \n\n  # add a box plot with grey fill\np + geom_boxplot(fill = 'grey90') + \n  # add labels for x and y axes\n  labs(x = \"month\",\n       y = \"mean temperature (°F)\") +\n  # apply a custom theme to the plot\n  theme_clean()"
  },
  {
    "objectID": "DANL310_hw1a.html#q2c",
    "href": "DANL310_hw1a.html#q2c",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2c",
    "text": "Q2c\nUse ggridges::geom_density_ridges() for Q2c.\n\nncdc_temp <- read_csv(\n  'https://bcdanl.github.io/data/ncdc_temp_cleaned.csv')\n\np <- ggplot(ncdc_temp, aes(x = temperature, y = month)) \n\np + geom_density_ridges( # Adds a layer to the ggplot object with a smoothed density plot of the temperature data using the 'ridgeline' plot type.\n  scale = 3, rel_min_height = 0.01, # Sets the scaling and minimum relative height for the plot.\n  bandwidth = 3.4, fill = \"#56B4E9\", color = \"white\" # Sets the bandwidth for the plot, as well as the fill and color for the plot elements.\n) +\n\nscale_x_continuous( # Adds a scale to the x-axis for continuous values.\n  name = \"mean temperature (°F)\", # Sets the label for the x-axis.\n  expand = c(0, 0), breaks = c(0, 25, 50, 75) # Sets the expansion and the break points for the x-axis.\n) +\n\nscale_y_discrete(\n  name = \"month\", expand = c(0, .2, 0, 2.6)) + # Adds a scale to the y-axis for discrete (categorical) values, with a label and a custom expansion.\n\ntheme( # Applies a custom theme to the ggplot object.\n  plot.margin = margin(3, 7, 3, 1.5) # Sets the margin of the plot.\n)"
  },
  {
    "objectID": "DANL310_hw1a.html#q2d",
    "href": "DANL310_hw1a.html#q2d",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2d",
    "text": "Q2d\nUse ggplot::mtcars for Q2d.\n\nm <- ggplot(data = mtcars, aes(x = disp, y = mpg, color = hp)) \n\nm + geom_point(aes(color = hp)) + # add scatter plot with color mapped to \"hp\" variable\n  labs(x = \"displacement(cu. in.)\", y = \"fuel efficiency(mpg)\")+ # add labels to x and y axes\n  scale_color_gradient()+ # add color gradient scale legend\n  scale_fill_brewer(palette = \"Emrld\") # add fill color palette with \"Emrld\" scheme to the legend"
  },
  {
    "objectID": "DANL310_hw1a.html#q2e",
    "href": "DANL310_hw1a.html#q2e",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2e",
    "text": "Q2e\nUse the following data.frame for Q2e.\n\npopgrowth_df <- read_csv(\n  'https://bcdanl.github.io/data/popgrowth.csv')\n\n\np <- ggplot(popgrowth_df, \n            aes(x = reorder(state, popgrowth), \n                y = 100*popgrowth, \n                fill = region))\np + geom_col() + # Add the geom for the columns\n  scale_y_continuous(\n    limits = c(-.6, 37.5), expand = c(0, 0), # Set y axis limits and expansion\n    labels = scales::percent_format(accuracy = 1, scale = 1), # Set percent labels for y axis\n    name = \"population growth, 2000 to 2010\" # Set name for y axis\n    ) +\n  coord_flip() + # Flip the x and y axis\n  theme(legend.position = c(.67, .4), # Set legend position\n        axis.text.y = element_text( size = 6, \n                                    margin = margin(t = 0, r = 0, b = 0, l = 0) )) # Adjust the size and margin for y axis text"
  },
  {
    "objectID": "DANL310_hw1a.html#q2f",
    "href": "DANL310_hw1a.html#q2f",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2f",
    "text": "Q2f\nUse the following data.frame for Q2f.\n\nmale_Aus <- read_csv(\n  'https://bcdanl.github.io/data/aus_athletics_male.csv')\n\n\n# Define color and fill vectors for use in plot\ncolors <- c(\"#BD3828\", rep(\"#808080\", 4))\nfills <- c(\"#BD3828D0\", rep(\"#80808080\", 4))\n\np <- ggplot(male_Aus, aes(x=height, y=pcBfat, shape=sport, color = sport, fill = sport))\n\n# Add geom_point layer with custom size\np + geom_point(size = 3) +\n\n# Set shape values for different sports\n  scale_shape_manual(values = 21:25) +\n\n# Set color values for different sports\n  scale_color_manual(values = colors) +\n\n# Set fill values for different sports\n  scale_fill_manual(values = fills) +\n\n# Set x and y axis labels\n  labs(x = \"height (cm)\",\n       y = \"% body fat\" )"
  },
  {
    "objectID": "DANL310_hw1a.html#q2g",
    "href": "DANL310_hw1a.html#q2g",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2g",
    "text": "Q2g\nUse the following data.frame for Q2g.\n\ntitanic <- read_csv(\n  'https://bcdanl.github.io/data/titanic_cleaned.csv')\n\n\np <- ggplot(titanic, aes(x = age, y = after_stat(count) ) ) \n\n# Add a density line plot for all passengers with transparent color, and fill legend with \"all passengers\"\np + geom_density_line(\n    data = select(titanic, -sex), \n    aes(fill = \"all passengers\"),\n    color = \"transparent\"\n  ) + \n  # Add another density line plot for each sex with transparent color, and fill legend with sex\n  geom_density_line(aes(fill = sex), bw = 2, color = \"transparent\") +\n  # Set the x-axis limits, name, and expand arguments\n  scale_x_continuous(limits = c(0, 75), name = \"passenger age (years)\", expand = c(0, 0)) +\n  # Set the y-axis limits, name, and expand arguments\n  scale_y_continuous(limits = c(0, 26), name = \"count\", expand = c(0, 0)) +\n  # Set the manual color and fill values, breaks, and labels for the legend\n  scale_fill_manual(\n    values = c(\"#b3b3b3a0\", \"#0072B2\", \"#D55E00\"), \n    breaks = c(\"all passengers\", \"male\", \"female\"),\n    labels = c(\"all passengers  \", \"males  \", \"females\"),\n    name = NULL,\n    guide = guide_legend(direction = \"horizontal\")\n  ) +\n  # Set the Cartesian coordinate system to allow for data points to fall outside the plot limits\n  coord_cartesian(clip = \"off\") +\n  # Create separate density line plots for male and female passengers\n  facet_wrap(~sex) +\n  # Set the x-axis line to blank, increase the strip text size, and set the legend position and margin\n  theme(\n    axis.line.x = element_blank(),\n    strip.text = element_text(size = 14, margin = margin(0, 0, 0.2, 0, \"cm\")),\n    legend.position = \"bottom\",\n    legend.justification = \"right\",\n    legend.margin = margin(4.5, 0, 1.5, 0, \"pt\"),\n    legend.spacing.x = grid::unit(4.5, \"pt\"),\n    legend.spacing.y = grid::unit(0, \"pt\"),\n    legend.box.spacing = grid::unit(0, \"cm\")\n  )"
  },
  {
    "objectID": "DANL310_hw1a.html#q2h",
    "href": "DANL310_hw1a.html#q2h",
    "title": "DANL 310: Data Visualization and PresentationHomework Assignment 1",
    "section": "Q2h",
    "text": "Q2h\nUse the following data.frame for Q2h.\n\ncows_filtered <- read_csv(\n  'https://bcdanl.github.io/data/cows_filtered.csv')\n\n\np <- ggplot(cows_filtered, aes(x = butterfat, color = breed, fill = breed))\n\n# add a density line for each breed with some transparency\np + geom_density_line(alpha = .2) +\n\n# set x-axis properties\nscale_x_continuous(\n  expand = c(0, 0), # remove padding from axis limits\n  labels = scales::percent_format(accuracy = 1, scale = 1), # format axis labels as percentages with 1 decimal point\n  name = \"butterfat contents\" # set axis label\n) +\n\n# set y-axis properties\nscale_y_continuous(limits = c(0, 1.99), expand = c(0, 0)) +\n\n# set plot area properties\ncoord_cartesian(clip = \"off\") + # allow density lines to extend beyond axis limits\ntheme(axis.line.x = element_blank()) # remove x-axis line\n\n\n\n\n\n\nReference: Fundamentals of Data Visualization by Claus O. Wilke"
  },
  {
    "objectID": "DANL200_hw3q.html",
    "href": "DANL200_hw3q.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "",
    "text": "Load R packages you need for Homework Assignment 3"
  },
  {
    "objectID": "DANL200_hw3q.html#variable-description",
    "href": "DANL200_hw3q.html#variable-description",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Variable Description",
    "text": "Variable Description\n\nhh: an identifier of the purchasing household;\nX_purchase_desc: details on the purchased item;\nquantity: the number of items purchased;\nbrand: Bud Light, Busch Light, Coors Light, Miller Lite, or Natural Light;\nspent: total dollar value of purchase;\nbeer_floz: total volume of beer, in fluid ounces;\nprice_per_floz: price per fl.oz. (i.e., beer spent/beer floz);\ncontainer: the type of container;\npromo: Whether the item was promoted (coupon or otherwise);\nmarket: Scan-track market (or state if rural);\ndemographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher."
  },
  {
    "objectID": "DANL200_hw3q.html#q1a",
    "href": "DANL200_hw3q.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1a",
    "text": "Q1a\nIn the data.frame, beer_mkts, keep the observations with only ‘CAN’ or ‘NON REFILLABLE BOTTLE’ type containers."
  },
  {
    "objectID": "DANL200_hw3q.html#q1b",
    "href": "DANL200_hw3q.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1b",
    "text": "Q1b\nIn the data.frame, beer_mkts, convert the character variable, market, to the factor variable with the first level ‘BUFFALO-ROCHESTER’."
  },
  {
    "objectID": "DANL200_hw3q.html#q1c",
    "href": "DANL200_hw3q.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1c",
    "text": "Q1c\n\nCreate the data.frame, beer_mkts_NY, that include\n\nonly the following markets,\n\n\nBUFFALO-ROCHESTER;\nALBANY;\nSYRACUSE;\nSURBURBAN NY;\nEXURBAN NY;\nURBAN NY;\n\n\nonly the following beer brands\n\n\nBUD LIGHT;\nCOORS LIGHT;\nMILLER LITE.\n\nProvide both (1) ggplot with beer_mkts_NY and (2) a comment to describe how the relationship between log(price_per_floz) and log(beer_floz) varies by market and brand."
  },
  {
    "objectID": "DANL200_hw3q.html#q1d",
    "href": "DANL200_hw3q.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1d",
    "text": "Q1d\n\nRandomly divide the data.frame, beer_mkts, into training and test data.frames, dtrain and dtest, respectively.\nMake sure that you can replicate the randomization.\n\nApproximately 60% of observations in beer_mkts go to dtrain.\nThe rest of observations in beer_mkts go to dtest."
  },
  {
    "objectID": "DANL200_hw3q.html#q1e",
    "href": "DANL200_hw3q.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1e",
    "text": "Q1e\nUse dtrain to train the linear regression model with the following formula.\n\nformula <- \n  log(price_per_floz) ~  \n    log(beer_floz) + market + container + brand\n\n\nProvide the summary of the linear regression."
  },
  {
    "objectID": "DANL200_hw3q.html#q1f",
    "href": "DANL200_hw3q.html#q1f",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1f",
    "text": "Q1f\nFrom the result in Q1e, interpret the beta estimates of the following variables. 1. marketALBANY 2. marketEXURBAN NY 3. marketRURAL NEW YORK 4. marketSURBURBAN NY 5. marketSYRACUSE 6. marketURBAN NY 7. brandBUSCH LIGHT"
  },
  {
    "objectID": "DANL200_hw3q.html#q1g",
    "href": "DANL200_hw3q.html#q1g",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1g",
    "text": "Q1g\nFrom the result in Q1e, interpret the beta estimate for log(beer_floz)."
  },
  {
    "objectID": "DANL200_hw3q.html#q1h",
    "href": "DANL200_hw3q.html#q1h",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1h",
    "text": "Q1h\nMake a prediction on the price of one 12 fl.oz bottle of beer using the dtest data.frame and the regression result in Q1e."
  },
  {
    "objectID": "DANL200_hw3q.html#q1i",
    "href": "DANL200_hw3q.html#q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1i",
    "text": "Q1i\nProvide both (1) ggplot and (2) a simple comment to describe how the distribution of the predicted price of one 12 fl.oz bottle of beer varies by brand."
  },
  {
    "objectID": "DANL200_hw3q.html#q1j",
    "href": "DANL200_hw3q.html#q1j",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1j",
    "text": "Q1j\n\nUse ggplot to draw a residual plot.\nMake a simple comment on the residual plot."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "B.H. CHOE",
    "section": "",
    "text": "I view myself as an applied economist with interest in environmental economics and a particular focus on climate change. Methodologically, I make use of causal inference/econometrics/machine learning methods, and other data science tools to conduct empirical analysis. I also use computational methods to solve integrated assessment models of climate change and theoretical economic models, such as dynamic contracts.\nMy research focuses on how to improve effectiveness of climate policy at both micro and macro levels. In particular, I am interested in building relevant climate-economy models that analyze the interaction between economies and the climate under risks arising from (1) climate change and (2) a transition to carbon-neutral economies.\nAs an economics and data science teacher, my goal is to equip students with the essential tools of machine learning, econometrics, and data science to think critically about business and real-world socioeconomic issues. In this regard, I teach students to use those tools to address important business strategies, socioeconomic issues, and individual decision-making."
  },
  {
    "objectID": "cv.html#working-papers",
    "href": "cv.html#working-papers",
    "title": "B.H. CHOE",
    "section": "Working papers",
    "text": "Working papers\n\n\n\n\n\n  \n     \n    Social Media Campaigns, Lobbying and Legislation: Evidence from #climatechange/#globalwarming and Energy Lobbies   *Abstract*: To what extent do social media campaigns compete with fossil fuel lobbying on climate change legislation? In this article, I estimate the effect of social media campaigns on a congressperson's legislative activities against climate change actions during the U.S. Congresses (January 2013-January 2019). I find that (1) a 1% increase in the per-capita level of activities of climate change campaigns using Twitter decreases Democrats' tendency to support climate-unfriendly legislation by 0.9%, while it increases Republicans' one by 0.2%; and (2) a 1% increase in the fossil fuel industry's lobbying expenditure relative to the rest of industries' lobbying expenditure increases Republicans' tendency to support climate-unfriendly legislation by 1.1%. I also find that negative sentiment in social media campaigns contributes to affecting congresspersons' support for climate-unfriendly legislation. \n  \n  \n     \n    Climate Finance under Conflicts and Renegotiations: A Dynamic Contract Approach  (AEA 2019 poster)    *Abstract*: Considering climate funds (e.g. the Green Climate Fund) as the financial mechanism to provide funding to developing countries, this paper examines a long-term climate funding relationship between two agents---the rich country and the poor country. Conflicts between the rich and poor countries arise when determining 1) the size of climate funding that the rich country contributes to the poor country, and 2) the funding allocation between climate adaptation and mitigation projects in the poor country. In addition, the rich country cannot be forced to commit contractual contributions to the poor country, and the climate funding relationship can be repeatedly renegotiated. This paper derives the following results: (1) climate funds converge to the first-best in the long-run, in terms of the size of climate funding and its balance between adaptation and mitigation projects, if and only if climate damage becomes sufficiently severe. (2) funding allocation between adaptation and mitigation projects becomes more favorable to the poor country if marginal climate costs in the poor country grow faster than in the rich country. (3) fewer conflicts and fewer renegotiations between the rich and poor countries make climate funding contracts more efficient, remedying inequality between the poor and rich countries."
  },
  {
    "objectID": "cv.html#work-in-progress",
    "href": "cv.html#work-in-progress",
    "title": "B.H. CHOE",
    "section": "Work in progress",
    "text": "Work in progress\n\n\n\n\n\n  \n     \n    Hiding Behind a Small Cake' in an Online Dictator Game: The Way You Hide Matters!, (with Tabare Capitan (1st author) , Jason Shogren, and Benjamin White)  *Abstract*: Using an online dictator game in which receivers have incomplete information of the size of the endowment (big or small), the article, ''Hiding behind a small cake' in a newspaper dictator game (Ockenfels and Werner (2012))'' shows that a few givers who received the big endowment use their giving to signal they received the small endowment (i.e., to lie). In other words, even though a giver will never meet the corresponding receiver, he cares enough about how he could be perceived by others to lie (i.e., second-order beliefs enters his utility function). In our experiment we provide givers with the opportunity to lie about the size of the endowment without using their giving. Similar to Ockenfels and Werner (2012) we find that (i) few take the opportunity to lie---but those who do give less when their giving is not constrained by its role as a signal---and (ii) givers are more likely to lie when the lie is private. However, using a second stage in the experimental design, we show that liars are the most responsive group of givers to a simple message stating the expectation of the receiver."
  },
  {
    "objectID": "cv.html#presentations",
    "href": "cv.html#presentations",
    "title": "B.H. CHOE",
    "section": "Presentations",
    "text": "Presentations\n\n\n\n\n\n  \n    2022 \n    Interdisciplinary Data Science Workshop, Brigham Young University, Provo, Utah \n  \n  \n    2021 \n    Online Seminar, Department of Environmental and Business Economics, University of Southern Denmark \n  \n  \n    2019 \n    University of Wyoming & Colorado State University Economics Graduate Student Symposium, Laramie, Wyoming \n  \n  \n    2019 \n    American Economic Association Annual Meeting (poster session), Atlanta, Georgia \n  \n  \n    2018 \n    The 29th International Conference on Game Theory, Stony Brook, New York"
  },
  {
    "objectID": "cv.html#memberships",
    "href": "cv.html#memberships",
    "title": "B.H. CHOE",
    "section": "Memberships",
    "text": "Memberships\n\n\n\n\n\n  \n    2022—present \n    Southern Economic Association \n  \n  \n    2018—present \n    Association of Environmental and Resource Economists \n  \n  \n    2017—present \n    American Economic Association \n  \n  \n    2017—2019 \n    Econometric Society"
  },
  {
    "objectID": "DANL310_lab2a.html",
    "href": "DANL310_lab2a.html",
    "title": "R Visualization Lab 2 - Maps 2",
    "section": "",
    "text": "Loading R packages\n\nlibrary(tidyverse)\nlibrary(socviz)\nlibrary(lubridate)\nlibrary(geofacet)\n\n\n\nQuestion 1 - Unemployment Rate Maps with geofacet::facet_geo()\nThe following data is for Question 1:\n\nunemp_house_prices <- read_csv(\n  'https://bcdanl.github.io/data/unemp_house_prices.csv')\n\n\n\n\n\n  \n\n\n\n\n\nUse geom_area(), geom_line(), and facet_geo(~state, labeller = adjust_labels) to replicate the following figure\n\n\nadjust_labels <- as_labeller(\n  function(x) {\n    case_when(\n      x == \"New Hampshire\" ~ \"N. Hampshire\",\n      x == \"District of Columbia\" ~ \"DC\",\n      TRUE ~ x\n    )\n  }\n)\n\n\nunemp_house_prices %>% \n  filter(\n    date >= ymd(\"2008-01-01\")\n  ) %>%\n  ggplot(aes(date, unemploy_perc)) + \n  geom_area(fill = \"#56B4E9\", alpha = 0.7) +\n  geom_line() + \n  scale_y_continuous(\n    name = \"unemployment rate\",\n    limits = c(0, 16),\n    breaks = c(0, 5, 10, 15),\n    labels = c(\"0%\", \"5%\", \"10%\", \"15%\")\n  ) +\n  scale_x_date(\n    name = NULL,\n    breaks = ymd(c(\"2009-01-01\", \"2011-01-01\", \n                   \"2013-01-01\", \"2015-01-01\", \"2017-01-01\")),\n    labels = c(\"'09\", \"'11\", \"'13\", \"'15\", \"'17\")\n  ) +\n  facet_geo(~state, labeller = adjust_labels) +\n  theme(\n    strip.text = element_text(\n      margin = margin(3, 3, 3, 3)\n    ),\n    axis.line.x = element_blank()\n  )\n\n\n\n\n\n\n\n\nQuestion 2 - Election maps and data clearning\nThe following data is for Question 2:\n\nelection_panel <- read_csv(\n  'https://bcdanl.github.io/data/election_panel.csv')\n\n\n\n\n\n  \n\n\n\n\nReplicate the following map.\n\nDo not use coord_map(projection = \"albers\", lat0 = 39, lat1 = 45).\n\n\n\nelection_panel_AK <- filter(election_panel,\n                            state_po == \"AK\")\n\nclass(county_map$id)\n\n[1] \"character\"\n\ncounty_map <- county_map\ncounty_map$id <- as.integer(county_map$id)\nelection_panel$id <- as.integer(election_panel$id)\ncounty_map_AK <- filter(county_map,\n                            id >=2000, id < 3000)\n  \ncounty_full <- left_join(county_map, election_panel, by = \"id\")\ncounty_full_AK <- filter(county_full,\n                        id >=2000, id < 3000)\ncounty_full <- county_full %>% \n  arrange(year, county_fips, order)\n\n\nna_map <- function(yr){\n  county_full_na <- filter(county_full, is.na(year)) %>% \n    select(-year) %>% \n    mutate( year = yr)\n}\n\ncounty_full_NAmap <- county_full\nfor (val in as.numeric(levels(factor(county_full$year)))){\n  county_full_NAmap <- rbind(county_full_NAmap, na_map(val))\n}\n\np1 <- ggplot() + geom_polygon(data = filter(county_full_NAmap, !is.na(year)), \n                       mapping = aes(x = long, y = lat, group = group, \n                                     fill = pct_DEMOCRAT ),\n                       color = \"grey60\", size = 0.1) \n\np2 <- p1 + scale_fill_gradient( \n  # low = '#CB454A',  # from party_colors for GOP\n  low = '#FFFFFF',  # transparent white\n  high = '#2E74C0',  # from party_colors for DEM\n  na.value = \"grey80\",\n  # midpoint = quantile(county_full$pct_DEMOCRAT, .5, na.rm = T),\n  breaks = c(quantile(county_full$pct_DEMOCRAT, 0, na.rm = T),\n             quantile(county_full$pct_DEMOCRAT, .25, na.rm = T),\n             quantile(county_full$pct_DEMOCRAT, .5, na.rm = T),\n             quantile(county_full$pct_DEMOCRAT, .75, na.rm = T),\n             quantile(county_full$pct_DEMOCRAT, 1, na.rm = T)),\n  labels = c(paste(round(quantile(county_full$pct_DEMOCRAT, 0, na.rm = T), 1),\"\\n(Min)\"),\n             paste(round(quantile(county_full$pct_DEMOCRAT, .25, na.rm = T), 1),\"\\n(25th)\"),\n             paste(round(quantile(county_full$pct_DEMOCRAT, .5, na.rm = T), 1),\"\\n(Median)\"),\n             paste(round(quantile(county_full$pct_DEMOCRAT, .75, na.rm = T), 1),\"\\n(75th)\"),\n             paste(round(quantile(county_full$pct_DEMOCRAT, 1, na.rm = T), 1),\"\\n(Max)\")\n  ),\n  guide = \"colourbar\"\n  ) \n\np2 + labs(fill = \"Percent\\nDemocrat\",\n          title = \"U.S. Presidential Election, 2000-2020\") +\n  theme_map() + \n  facet_wrap(.~ year) +\n  theme(plot.margin = unit( c(1, 1, 4, 0.5), \"cm\"),\n        legend.position = c(0.5, -.15),\n        legend.justification = c(.5,.5),\n        strip.background = element_rect(fill = \"#e1ecf8\", \n                                        colour = \"black\", size = .1)\n        ) +\n  guides(fill = guide_colourbar(direction = \"horizontal\", barwidth = 25,\n                                title.hjust = -1, title.vjust = 1))"
  },
  {
    "objectID": "DANL210_hw2q.html",
    "href": "DANL210_hw2q.html",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here.\n\n\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "DANL210_hw2q.html#q1a",
    "href": "DANL210_hw2q.html#q1a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1a",
    "text": "Q1a\nWhat are the minimum, first quartile, median, thrid quartile, maximum, mean, and standard deviation of Close and Volume for each company?"
  },
  {
    "objectID": "DANL210_hw2q.html#q1b",
    "href": "DANL210_hw2q.html#q1b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1b",
    "text": "Q1b\nFind the 10 largest values for Volume. What are the companies and dates associated with those 10 largest values for Volume?"
  },
  {
    "objectID": "DANL210_hw2q.html#q1c",
    "href": "DANL210_hw2q.html#q1c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1c",
    "text": "Q1c\nCalculate the Z-scores of Open and Close for each company using apply()."
  },
  {
    "objectID": "DANL210_hw2q.html#q1d",
    "href": "DANL210_hw2q.html#q1d",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1d",
    "text": "Q1d\nUse the transform() method on the stock data to represent all the values of Open, High, Low, Close, Adj Close, and Volume in terms of the first date in the data.\nTo do so, divide all values for each company by the values of the first date in the data for that company."
  },
  {
    "objectID": "DANL210_hw2q.html#q1e",
    "href": "DANL210_hw2q.html#q1e",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1e",
    "text": "Q1e\nProvide both seaborn code and a simple comment to describe the daily trend of normalized values of Close for each company in one plot. The normalized values of Close are the one calculated from Q1d."
  },
  {
    "objectID": "DANL210_hw2q.html#q1f",
    "href": "DANL210_hw2q.html#q1f",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1f",
    "text": "Q1f\nCreate a box plot of Close for each company in one plot. Make a simple comment on the plot."
  },
  {
    "objectID": "DANL210_hw2q.html#q2a",
    "href": "DANL210_hw2q.html#q2a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2a",
    "text": "Q2a\nHow many parties have provided or disbursed positive funding contributions to other countries or regions for their adaptation projects for every single year from 2011 to 2018?"
  },
  {
    "objectID": "DANL210_hw2q.html#q2b",
    "href": "DANL210_hw2q.html#q2b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\nFor each party, calculate the total funding contributions that were disbursed or provided for mitigation projects for each year."
  },
  {
    "objectID": "DANL210_hw2q.html#q2c",
    "href": "DANL210_hw2q.html#q2c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2c",
    "text": "Q2c\nFor each party, calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each year."
  },
  {
    "objectID": "DANL210_hw2q.html#q2d",
    "href": "DANL210_hw2q.html#q2d",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2d",
    "text": "Q2d\nProvide both seaborn code and a simple comment to describe the distribution of the ratio between adaptation contribution and mitigation contribution, which is calculated in Q2c."
  },
  {
    "objectID": "DANL210_hw2q.html#q2e",
    "href": "DANL210_hw2q.html#q2e",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2e",
    "text": "Q2e\nProvide both seaborn code and a simple comment to describe how the distribution of Contribution varies by Type of support and Status."
  },
  {
    "objectID": "DANL310_lab4q.html",
    "href": "DANL310_lab4q.html",
    "title": "R Visualization Lab 4",
    "section": "",
    "text": "library(tidyverse)\n\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\nThe data from the survey is in data/brexit.csv.\n\nbrexit <- read_csv(\"data/brexit.csv\")\n\nIn the course video we made the following visualisation.\n\nbrexit <- brexit %>%\n  mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\nIn this application exercise we tell different stories with the same data.\n\nExercise 1 - Free scales\nAdd scales = \"free_x\" as an argument to the facet_wrap() function. How does the visualisation change? How is the story this visualisation telling different than the story the original plot tells?\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\nExercise 2 - Comparing proportions across facets\nFirst, calculate the proportion of wrong, right, and don’t know answers in each category and then plot these proportions (rather than the counts) and then improve axis labeling. How is the story this visualisation telling different than the story the original plot tells? Hint: You’ll need the scales package to improve axis labeling, which means you’ll need to load it on top of the document as well.\n\n# code goes here\n\n\n\nExercise 3 - Comparing proportions across bars\nRecreate the same visualisation from the previous exercise, this time dodging the bars for opinion proportions for each region, rather than faceting by region and then improve the legend. How is the story this visualisation telling different than the story the previous plot tells?\n\n# code goes here\n\n\n\nReferences\n\nhttps://datasciencebox.org"
  },
  {
    "objectID": "DANL200_lab5a.html",
    "href": "DANL200_lab5a.html",
    "title": "R Lab 5 - Linear Regression 1 Example Answers",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\nlibrary(stargazer)\nlibrary(broom)"
  },
  {
    "objectID": "DANL200_lab5a.html#variable-description",
    "href": "DANL200_lab5a.html#variable-description",
    "title": "R Lab 5 - Linear Regression 1 Example Answers",
    "section": "Variable Description",
    "text": "Variable Description\n\ncnt: count of total rental bikes\nyear: year\nmonth: month\ndate: date\nhr: hours\nwkday: week day\nholiday: holiday if holiday == 1; non-holiday otherwise\nseasons: season\nweather_cond: weather condition\ntemp: temperature, measured in standard deviations from average.\nhum: humidity, measured in standard deviations from average.\nwindspeed: wind speed, measured in standard deviations from average."
  },
  {
    "objectID": "DANL200_lab5a.html#q1a",
    "href": "DANL200_lab5a.html#q1a",
    "title": "R Lab 5 - Linear Regression 1 Example Answers",
    "section": "Q1a",
    "text": "Q1a\n\nFrom the bikeshare data.frame, convert variables year, month, wkday, hr, seasons, and weather_cond into factor variables.\n\nSet wkday in order of ‘sunday’, ‘monday’, ‘tuesday’, and so on.\nSet seasons in order of ‘spring’, ‘summer’, ‘fall’, and ‘winter’\n\n\n\nbikeshare <- bikeshare %>% \n  mutate( year = factor(year),\n          month = factor(month),\n          hr = factor(hr),\n          weather_cond = factor(weather_cond), \n          wkday = factor(wkday, \n                         levels = c(\"sunday\", \"monday\", \"tuesday\", \"wednesday\", \n                                    \"thursday\", \"friday\", \"saturday\") ),\n          seasons = factor(seasons, \n                           levels = c(\"spring\", \"summer\", \"fall\", \"winter\") ) \n  )\n\n# to check the levels of the factor variables\nlevels(bikeshare$year)\n\n[1] \"2011\" \"2012\"\n\nlevels(bikeshare$month)\n\n [1] \"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\" \"11\" \"12\"\n\nlevels(bikeshare$hr)\n\n [1] \"0\"  \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\"\n[16] \"15\" \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\"\n\nlevels(bikeshare$weather_cond)\n\n[1] \"Clear or Few Cloudy\"      \"Light Snow or Light Rain\"\n[3] \"Mist or Cloudy\"          \n\nlevels(bikeshare$wkday)\n\n[1] \"sunday\"    \"monday\"    \"tuesday\"   \"wednesday\" \"thursday\"  \"friday\"   \n[7] \"saturday\" \n\nlevels(bikeshare$seasons)\n\n[1] \"spring\" \"summer\" \"fall\"   \"winter\""
  },
  {
    "objectID": "DANL200_lab5a.html#q1b",
    "href": "DANL200_lab5a.html#q1b",
    "title": "R Lab 5 - Linear Regression 1 Example Answers",
    "section": "Q1b",
    "text": "Q1b\n\nRandomly divide the bikeshare data.frame into training and test data.frames, dtrain and dtest, respectively.\n\nMake sure that you can replicate the randomization.\nApproximately 70% of observations in bikeshare goes to dtrain.\nThe rest of observations in bikeshare goes to dtest.\n\n\n\nset.seed(2023)\nrn <- runif(nrow(bikeshare))\ndtrain <- filter(bikeshare, rn >= .3)\ndtest <- filter(bikeshare, rn < .3)"
  },
  {
    "objectID": "DANL200_lab5a.html#q1c",
    "href": "DANL200_lab5a.html#q1c",
    "title": "R Lab 5 - Linear Regression 1 Example Answers",
    "section": "Q1c",
    "text": "Q1c\n\nUse dtrain to train the five different linear regression models with formula_1, formula_2, formula_3, formula_4, and formula_5, respectively.\n\n\nformula_1 <- \n  cnt ~ temp + windspeed + weather_cond + hr + year \n\nformula_2 <- \n  cnt ~ hum + windspeed + weather_cond + hr + year \n\nformula_3 <- \n  cnt ~ temp + hum + windspeed + weather_cond + hr + year \n\nformula_4 <- \n  cnt ~ temp + hum + windspeed + weather_cond + hr + month + year\n\nformula_5 <- \n  log(cnt) ~ temp + hum + windspeed + weather_cond + hr + month + year\n\n\nmodel_1 <- lm(formula_1, dtrain)\nmodel_2 <- lm(formula_2, dtrain)\nmodel_3 <- lm(formula_3, dtrain)\nmodel_4 <- lm(formula_4, dtrain)\nmodel_5 <- lm(formula_5, dtrain)\n\n\nProvide the summary of the regression results.\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ncnt\n\n\nlog(cnt)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n\n\n\n\n\n\ntemp\n\n\n50.894***\n\n\n\n\n51.521***\n\n\n46.292***\n\n\n0.286***\n\n\n\n\n\n\n(0.997)\n\n\n\n\n(0.999)\n\n\n(2.181)\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhum\n\n\n\n\n-3.398**\n\n\n-9.194***\n\n\n-15.035***\n\n\n-0.050***\n\n\n\n\n\n\n\n\n(1.413)\n\n\n(1.283)\n\n\n(1.294)\n\n\n(0.008)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwindspeed\n\n\n-3.307***\n\n\n-8.343***\n\n\n-5.090***\n\n\n-4.914***\n\n\n-0.042***\n\n\n\n\n\n\n(0.997)\n\n\n(1.131)\n\n\n(1.026)\n\n\n(1.010)\n\n\n(0.006)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweather_condLight Snow or Light Rain\n\n\n-81.988***\n\n\n-91.048***\n\n\n-69.219***\n\n\n-67.009***\n\n\n-0.585***\n\n\n\n\n\n\n(3.608)\n\n\n(4.415)\n\n\n(4.018)\n\n\n(3.928)\n\n\n(0.024)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweather_condMist or Cloudy\n\n\n-19.286***\n\n\n-25.528***\n\n\n-13.460***\n\n\n-12.522***\n\n\n-0.054***\n\n\n\n\n\n\n(2.239)\n\n\n(2.615)\n\n\n(2.377)\n\n\n(2.325)\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr1\n\n\n-15.794**\n\n\n-16.575**\n\n\n-15.184**\n\n\n-14.084**\n\n\n-0.575***\n\n\n\n\n\n\n(6.622)\n\n\n(7.304)\n\n\n(6.609)\n\n\n(6.443)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr2\n\n\n-26.058***\n\n\n-28.606***\n\n\n-24.816***\n\n\n-25.051***\n\n\n-1.160***\n\n\n\n\n\n\n(6.677)\n\n\n(7.365)\n\n\n(6.665)\n\n\n(6.499)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr3\n\n\n-41.278***\n\n\n-43.494***\n\n\n-39.639***\n\n\n-41.063***\n\n\n-1.742***\n\n\n\n\n\n\n(6.739)\n\n\n(7.435)\n\n\n(6.729)\n\n\n(6.566)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr4\n\n\n-42.779***\n\n\n-46.609***\n\n\n-40.353***\n\n\n-41.369***\n\n\n-2.035***\n\n\n\n\n\n\n(6.706)\n\n\n(7.403)\n\n\n(6.700)\n\n\n(6.539)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr5\n\n\n-25.727***\n\n\n-34.156***\n\n\n-23.348***\n\n\n-24.387***\n\n\n-0.918***\n\n\n\n\n\n\n(6.659)\n\n\n(7.350)\n\n\n(6.654)\n\n\n(6.495)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr6\n\n\n33.647***\n\n\n26.281***\n\n\n35.794***\n\n\n37.018***\n\n\n0.308***\n\n\n\n\n\n\n(6.707)\n\n\n(7.401)\n\n\n(6.699)\n\n\n(6.544)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr7\n\n\n171.172***\n\n\n166.186***\n\n\n172.601***\n\n\n171.952***\n\n\n1.285***\n\n\n\n\n\n\n(6.618)\n\n\n(7.300)\n\n\n(6.607)\n\n\n(6.449)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr8\n\n\n309.828***\n\n\n308.961***\n\n\n310.126***\n\n\n308.352***\n\n\n1.876***\n\n\n\n\n\n\n(6.685)\n\n\n(7.372)\n\n\n(6.671)\n\n\n(6.505)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr9\n\n\n166.851***\n\n\n170.830***\n\n\n165.527***\n\n\n164.387***\n\n\n1.578***\n\n\n\n\n\n\n(6.599)\n\n\n(7.279)\n\n\n(6.588)\n\n\n(6.424)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr10\n\n\n111.470***\n\n\n120.043***\n\n\n108.073***\n\n\n106.559***\n\n\n1.248***\n\n\n\n\n\n\n(6.621)\n\n\n(7.316)\n\n\n(6.624)\n\n\n(6.464)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr11\n\n\n143.081***\n\n\n157.059***\n\n\n137.446***\n\n\n134.713***\n\n\n1.377***\n\n\n\n\n\n\n(6.637)\n\n\n(7.358)\n\n\n(6.669)\n\n\n(6.521)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr12\n\n\n184.211***\n\n\n205.473***\n\n\n176.994***\n\n\n173.697***\n\n\n1.555***\n\n\n\n\n\n\n(6.596)\n\n\n(7.333)\n\n\n(6.659)\n\n\n(6.535)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr13\n\n\n179.715***\n\n\n199.416***\n\n\n171.068***\n\n\n165.807***\n\n\n1.520***\n\n\n\n\n\n\n(6.672)\n\n\n(7.453)\n\n\n(6.766)\n\n\n(6.661)\n\n\n(0.041)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr14\n\n\n163.107***\n\n\n187.352***\n\n\n154.337***\n\n\n151.413***\n\n\n1.453***\n\n\n\n\n\n\n(6.704)\n\n\n(7.482)\n\n\n(6.801)\n\n\n(6.696)\n\n\n(0.041)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr15\n\n\n173.759***\n\n\n200.395***\n\n\n165.190***\n\n\n161.330***\n\n\n1.491***\n\n\n\n\n\n\n(6.660)\n\n\n(7.425)\n\n\n(6.753)\n\n\n(6.663)\n\n\n(0.041)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr16\n\n\n234.762***\n\n\n258.897***\n\n\n225.989***\n\n\n222.555***\n\n\n1.763***\n\n\n\n\n\n\n(6.618)\n\n\n(7.389)\n\n\n(6.716)\n\n\n(6.624)\n\n\n(0.041)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr17\n\n\n391.136***\n\n\n414.775***\n\n\n383.255***\n\n\n379.826***\n\n\n2.161***\n\n\n\n\n\n\n(6.628)\n\n\n(7.379)\n\n\n(6.705)\n\n\n(6.598)\n\n\n(0.041)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr18\n\n\n356.947***\n\n\n377.556***\n\n\n350.117***\n\n\n347.911***\n\n\n2.077***\n\n\n\n\n\n\n(6.677)\n\n\n(7.415)\n\n\n(6.731)\n\n\n(6.600)\n\n\n(0.041)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr19\n\n\n240.658***\n\n\n255.114***\n\n\n235.268***\n\n\n232.789***\n\n\n1.780***\n\n\n\n\n\n\n(6.669)\n\n\n(7.389)\n\n\n(6.697)\n\n\n(6.555)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr20\n\n\n163.365***\n\n\n173.733***\n\n\n159.209***\n\n\n155.715***\n\n\n1.492***\n\n\n\n\n\n\n(6.631)\n\n\n(7.335)\n\n\n(6.643)\n\n\n(6.492)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr21\n\n\n111.722***\n\n\n119.095***\n\n\n109.306***\n\n\n107.327***\n\n\n1.234***\n\n\n\n\n\n\n(6.584)\n\n\n(7.268)\n\n\n(6.579)\n\n\n(6.422)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr22\n\n\n74.178***\n\n\n80.531***\n\n\n72.920***\n\n\n71.574***\n\n\n0.990***\n\n\n\n\n\n\n(6.675)\n\n\n(7.362)\n\n\n(6.663)\n\n\n(6.499)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhr23\n\n\n32.050***\n\n\n35.025***\n\n\n30.937***\n\n\n29.761***\n\n\n0.581***\n\n\n\n\n\n\n(6.594)\n\n\n(7.274)\n\n\n(6.583)\n\n\n(6.417)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth02\n\n\n\n\n\n\n\n\n7.658\n\n\n0.134***\n\n\n\n\n\n\n\n\n\n\n\n\n(4.755)\n\n\n(0.029)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth03\n\n\n\n\n\n\n\n\n32.583***\n\n\n0.247***\n\n\n\n\n\n\n\n\n\n\n\n\n(4.908)\n\n\n(0.030)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth04\n\n\n\n\n\n\n\n\n48.869***\n\n\n0.429***\n\n\n\n\n\n\n\n\n\n\n\n\n(5.283)\n\n\n(0.033)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth05\n\n\n\n\n\n\n\n\n57.271***\n\n\n0.535***\n\n\n\n\n\n\n\n\n\n\n\n\n(6.154)\n\n\n(0.038)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth06\n\n\n\n\n\n\n\n\n44.472***\n\n\n0.469***\n\n\n\n\n\n\n\n\n\n\n\n\n(6.831)\n\n\n(0.042)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth07\n\n\n\n\n\n\n\n\n12.637*\n\n\n0.315***\n\n\n\n\n\n\n\n\n\n\n\n\n(7.482)\n\n\n(0.046)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth08\n\n\n\n\n\n\n\n\n36.670***\n\n\n0.383***\n\n\n\n\n\n\n\n\n\n\n\n\n(7.061)\n\n\n(0.044)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth09\n\n\n\n\n\n\n\n\n72.963***\n\n\n0.564***\n\n\n\n\n\n\n\n\n\n\n\n\n(6.350)\n\n\n(0.039)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth10\n\n\n\n\n\n\n\n\n85.623***\n\n\n0.643***\n\n\n\n\n\n\n\n\n\n\n\n\n(5.439)\n\n\n(0.034)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth11\n\n\n\n\n\n\n\n\n62.223***\n\n\n0.594***\n\n\n\n\n\n\n\n\n\n\n\n\n(4.865)\n\n\n(0.030)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth12\n\n\n\n\n\n\n\n\n40.529***\n\n\n0.372***\n\n\n\n\n\n\n\n\n\n\n\n\n(4.665)\n\n\n(0.029)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear2012\n\n\n85.568***\n\n\n88.714***\n\n\n83.974***\n\n\n84.168***\n\n\n0.470***\n\n\n\n\n\n\n(1.927)\n\n\n(2.137)\n\n\n(1.936)\n\n\n(1.897)\n\n\n(0.012)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n28.434***\n\n\n20.736***\n\n\n29.575***\n\n\n-11.128*\n\n\n3.153***\n\n\n\n\n\n\n(4.822)\n\n\n(5.317)\n\n\n(4.815)\n\n\n(6.561)\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n12,049\n\n\n12,049\n\n\n12,049\n\n\n12,049\n\n\n12,049\n\n\n\n\nR2\n\n\n0.661\n\n\n0.588\n\n\n0.663\n\n\n0.680\n\n\n0.817\n\n\n\n\nAdjusted R2\n\n\n0.661\n\n\n0.587\n\n\n0.662\n\n\n0.679\n\n\n0.817\n\n\n\n\nResidual Std. Error\n\n\n105.585 (df = 12020)\n\n\n116.440 (df = 12020)\n\n\n105.364 (df = 12019)\n\n\n102.708 (df = 12008)\n\n\n0.633 (df = 12008)\n\n\n\n\nF Statistic\n\n\n838.688*** (df = 28; 12020)\n\n\n613.290*** (df = 28; 12020)\n\n\n814.928*** (df = 29; 12019)\n\n\n637.808*** (df = 40; 12008)\n\n\n1,342.938*** (df = 40; 12008)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nstargazer(model_1, model_2, model_3, model_4, model_5, type = 'text')"
  },
  {
    "objectID": "DANL200_lab5a.html#q1d",
    "href": "DANL200_lab5a.html#q1d",
    "title": "R Lab 5 - Linear Regression 1 Example Answers",
    "section": "Q1d",
    "text": "Q1d\n\nIn each model, which hr is most strongly associated with cnt?\nInterpret the beta estimate of that hr.\n\n\nsum_model_1 <- tidy(model_1) %>% \n  filter(str_detect(term, \"hr\")) %>% \n  arrange(-abs(estimate)) %>% \n  slice(1) %>% \n  mutate(model = 1) # to denote the model\n\nsum_model_2 <- tidy(model_2) %>% \n  filter(str_detect(term, \"hr\")) %>% \n  arrange(-abs(estimate)) %>% \n  slice(1) %>% \n  mutate(model = 2) # to denote the model\n\nsum_model_3 <- tidy(model_3) %>% \n  filter(str_detect(term, \"hr\")) %>% \n  arrange(-abs(estimate)) %>% \n  slice(1) %>% \n  mutate(model = 3) # to denote the model\n\nsum_model_4 <- tidy(model_4) %>% \n  filter(str_detect(term, \"hr\")) %>% \n  arrange(-abs(estimate)) %>% \n  slice(1) %>% \n  mutate(model = 4) # to denote the model\n\nsum_model_5 <- tidy(model_5) %>% \n  filter(str_detect(term, \"hr\")) %>% \n  arrange(-abs(estimate)) %>% \n  slice(1) %>% \n  mutate(model = 5) # to denote the model\n\nsum_model <- rbind(sum_model_1, sum_model_2, sum_model_3, sum_model_4, sum_model_5)\n\n\n\n\n\n  \n\n\n\n\nAcross all the models, hr17 is most strongly associated with cnt.\n\n\nggplot(sum_model) +\n  geom_pointrange( aes(x = model, \n                       y = estimate,\n                       ymin = estimate - 2*std.error,\n                       ymax = estimate + 2*std.error ) ) +\n  coord_flip()\n\n\n\n\n\nggplot(data = filter(sum_model,\n                     model != 5)) +\n  geom_pointrange( aes(x = model, \n                       y = estimate,\n                       ymin = estimate - 2*std.error,\n                       ymax = estimate + 2*std.error ) ) +\n  coord_flip()\n\n\n\n\n\nIn Model 4, all else being equal, hour 17 relative to hour 0 is associated with an increase in cnt by 380.\nIn Model 5, all else being equal, hour 17 relative to hour 0 is associated with an increase in log(cnt) by 2.16."
  },
  {
    "objectID": "DANL200_lab5a.html#q1e",
    "href": "DANL200_lab5a.html#q1e",
    "title": "R Lab 5 - Linear Regression 1 Example Answers",
    "section": "Q1e",
    "text": "Q1e\n\nFor each model, make a prediction on the outcome variable using the dtest.\n\n\ndtest <- dtest %>% \n  mutate(pred_1 = predict(model_1, dtest),\n         pred_2 = predict(model_2, dtest),\n         pred_3 = predict(model_3, dtest),\n         pred_4 = predict(model_4, dtest),\n         pred_5_log = predict(model_5, dtest),\n         pred_5 = exp(predict(model_5, dtest))\n  )"
  },
  {
    "objectID": "DANL200_lab5a.html#q1f",
    "href": "DANL200_lab5a.html#q1f",
    "title": "R Lab 5 - Linear Regression 1 Example Answers",
    "section": "Q1f",
    "text": "Q1f\n\nDraw a residual plot for each model.\nUsing the residual plots to answer the following questions:\n\nOn average, are the predictions correct in the models in Q1c?\nAre there systematic errors?\n\n\n\ndtest_plot <- dtest %>% \n  select(cnt, starts_with('pred')) %>% \n  pivot_longer(cols = starts_with('pred'),\n               names_to = 'model',\n               values_to = 'pred')\n\n# resitual plot\nggplot(data = filter(dtest_plot,\n               model != 'pred_5_log'), \n       aes(x = pred, y = cnt - pred)) +\n  geom_point(alpha = 0.05, color = \"darkgray\") +\n  geom_smooth( color = \"darkblue\" ) +   \n  geom_hline( aes( yintercept = 0 ),  # perfect prediction \n              color = \"red\", linetype = 2) + \n  geom_vline(aes(xintercept = 0), lty = 'dotted') +\n  facet_wrap(. ~ model)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nOn average, the predictions are correct up to the predicted value below 600.\nThere are two systematic errors:\n\nAll the models have a cone-shape residual plot.\nAll the models except for Model 5 produces the predicted cnt below 0."
  },
  {
    "objectID": "DANL200_lab5a.html#q1g",
    "href": "DANL200_lab5a.html#q1g",
    "title": "R Lab 5 - Linear Regression 1 Example Answers",
    "section": "Q1g",
    "text": "Q1g\n\nCompare the prediction accuracy across the models in Q1c.\n\nWhich model do you prefer most and why?\n\nHere I calculate the root MSE, which is the square root of MSE.\n\nRMSE summarizes the overall difference between the predicted and actual outcome values.\n\n\n\nRMSE <- dtest %>% \n  mutate(pred_1 = predict(model_1, dtest),\n         pred_2 = predict(model_2, dtest),\n         pred_3 = predict(model_3, dtest),\n         pred_4 = predict(model_4, dtest),\n         pred_5 = exp(predict(model_5, dtest)),\n         resid_1_sq = (cnt - pred_1)^2,\n         resid_2_sq = (cnt - pred_2)^2,\n         resid_3_sq = (cnt - pred_3)^2,\n         resid_4_sq = (cnt - pred_4)^2,\n         resid_5_sq = (cnt - pred_5)^2\n  ) %>% \n  summarize(rmse_1 = sqrt(mean(resid_1_sq)),\n            rmse_2 = sqrt(mean(resid_2_sq)),\n            rmse_3 = sqrt(mean(resid_3_sq)),\n            rmse_4 = sqrt(mean(resid_4_sq)),\n            rmse_5 = sqrt(mean(resid_5_sq))\n  )\n\n\n\n\n\n  \n\n\n\n\nI prefer Model 5 most.\n\nModel 5 has the lowest MSE/RMSE so."
  },
  {
    "objectID": "DANL200_lab4q.html",
    "href": "DANL200_lab4q.html",
    "title": "R Lab 4 - Joins",
    "section": "",
    "text": "library(tidyverse)\nlibrary(nycflights13)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab4q.html#q1a",
    "href": "DANL200_lab4q.html#q1a",
    "title": "R Lab 4 - Joins",
    "section": "Q1a",
    "text": "Q1a\nWrite a R code to join the two given data.frames along rows and assign all data."
  },
  {
    "objectID": "DANL200_lab4q.html#q1b",
    "href": "DANL200_lab4q.html#q1b",
    "title": "R Lab 4 - Joins",
    "section": "Q1b",
    "text": "Q1b\nWrite a R code to join the two given data.frames along columns and assign all data."
  },
  {
    "objectID": "DANL200_lab4q.html#q2a",
    "href": "DANL200_lab4q.html#q2a",
    "title": "R Lab 4 - Joins",
    "section": "Q2a",
    "text": "Q2a\nJoin flights with airports, so that we can see information about each flight’s destination airport in the flights data.frame."
  },
  {
    "objectID": "DANL200_lab4q.html#q2b",
    "href": "DANL200_lab4q.html#q2b",
    "title": "R Lab 4 - Joins",
    "section": "Q2b",
    "text": "Q2b\nFind the full name of the airline that has the longest positive dep_delay on average."
  },
  {
    "objectID": "DANL200_lab4q.html#q2c",
    "href": "DANL200_lab4q.html#q2c",
    "title": "R Lab 4 - Joins",
    "section": "Q2c",
    "text": "Q2c\nFind the full name of the airline that has the largest proportion of flights with longer than 30-minute dep_delay."
  },
  {
    "objectID": "DANL200_lab5q.html",
    "href": "DANL200_lab5q.html",
    "title": "R Lab 5 - Linear Regression 1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab5q.html#variable-description",
    "href": "DANL200_lab5q.html#variable-description",
    "title": "R Lab 5 - Linear Regression 1",
    "section": "Variable Description",
    "text": "Variable Description\n\ncnt: count of total rental bikes\nyear: year\nmonth: month\ndate: date\nhr: hours\nwkday: week day\nholiday: holiday if holiday == 1; non-holiday otherwise\nseasons: season\nweather_cond: weather condition\ntemp: temperature, measured in standard deviations from average.\nhum: humidity, measured in standard deviations from average.\nwindspeed: wind speed, measured in standard deviations from average."
  },
  {
    "objectID": "DANL200_lab5q.html#q1a",
    "href": "DANL200_lab5q.html#q1a",
    "title": "R Lab 5 - Linear Regression 1",
    "section": "Q1a",
    "text": "Q1a\n\nFrom the bikeshare data.frame, convert variables year, month, wkday, hr, seasons, and weather_cond into factor variables.\n\nSet wkday in order of ‘sunday’, ‘monday’, ‘tuesday’, and so on.\nSet seasons in order of ‘spring’, ‘summer’, ‘fall’, and ‘winter’"
  },
  {
    "objectID": "DANL200_lab5q.html#q1b",
    "href": "DANL200_lab5q.html#q1b",
    "title": "R Lab 5 - Linear Regression 1",
    "section": "Q1b",
    "text": "Q1b\n\nRandomly divide the bikeshare data.frame into training and test data.frames, dtrain and dtest, respectively.\n\nMake sure that you can replicate the randomization.\nApproximately 70% of observations in bikeshare go to dtrain.\nThe rest of observations in bikeshare go to dtest."
  },
  {
    "objectID": "DANL200_lab5q.html#q1c",
    "href": "DANL200_lab5q.html#q1c",
    "title": "R Lab 5 - Linear Regression 1",
    "section": "Q1c",
    "text": "Q1c\n\nUse dtrain to train the five different linear regression models with formula_1, formula_2, formula_3, formula_4, and formula_5, respectively.\n\n\nformula_1 <- \n  cnt ~ temp + windspeed + weather_cond + hr\n\nformula_2 <- \n  cnt ~ hum + windspeed + weather_cond + hr\n\nformula_3 <- \n  cnt ~ temp + hum + windspeed + weather_cond + hr + year \n\nformula_4 <- \n  cnt ~ temp + hum + windspeed + weather_cond + hr + month + year\n\nformula_5 <- \n  log(cnt) ~ temp + hum + windspeed + weather_cond + hr + month + year\n\n\nProvide the summary of the regression results."
  },
  {
    "objectID": "DANL200_lab5q.html#q1d",
    "href": "DANL200_lab5q.html#q1d",
    "title": "R Lab 5 - Linear Regression 1",
    "section": "Q1d",
    "text": "Q1d\n\nIn each model, which hr is most strongly associated with changes in cnt?\nInterpret the beta estimate of that hr."
  },
  {
    "objectID": "DANL200_lab5q.html#q1e",
    "href": "DANL200_lab5q.html#q1e",
    "title": "R Lab 5 - Linear Regression 1",
    "section": "Q1e",
    "text": "Q1e\n\nFor each model, make a prediction on the outcome variable using the dtest."
  },
  {
    "objectID": "DANL200_lab5q.html#q1f",
    "href": "DANL200_lab5q.html#q1f",
    "title": "R Lab 5 - Linear Regression 1",
    "section": "Q1f",
    "text": "Q1f\n\nDraw a residual plot for each model.\nUsing the residual plots to answer the following questions:\n\nOn average, are the predictions correct in the models in Q1c?\nAre there systematic errors?"
  },
  {
    "objectID": "DANL200_lab5q.html#q1g",
    "href": "DANL200_lab5q.html#q1g",
    "title": "R Lab 5 - Linear Regression 1",
    "section": "Q1g",
    "text": "Q1g\n\nCompare the prediction accuracy across the models in Q1c.\n\nWhich model do you prefer most and why?"
  },
  {
    "objectID": "DANL310_lab4a.html",
    "href": "DANL310_lab4a.html",
    "title": "R Visualization Lab 4",
    "section": "",
    "text": "library(tidyverse)\n\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\nThe data from the survey is in data/brexit.csv.\n\nbrexit <- read_csv('https://bcdanl.github.io/data/brexit.csv')\n\nIn the course video we made the following visualisation.\n\nbrexit <- brexit %>%\n  mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\nIn this application exercise we tell different stories with the same data.\n\nExercise 1 - Free scales\nAdd scales = \"free_x\" as an argument to the facet_wrap() function. How does the visualisation change? How is the story this visualisation telling different than the story the original plot tells?\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\nExercise 2 - Comparing proportions across facets\nFirst, calculate the proportion of wrong, right, and don’t know answers in each region and then plot these proportions (rather than the counts) and then improve axis labeling. How is the story this visualisation telling different than the story the original plot tells? Hint: You’ll need the scales package to improve axis labeling, which means you’ll need to load it on top of the document as well.\n\n# code goes here\nq2 <- brexit %>% \n  group_by(region, opinion) %>% \n  summarise(n = n()) %>% \n  mutate(tot = sum(n),\n         prop = n / tot ) \n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\nq2 <- q2 %>% mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nWarning: 5 unknown levels in `f`: london, rest_of_south, midlands_wales, north, and scot\n5 unknown levels in `f`: london, rest_of_south, midlands_wales, north, and scot\n5 unknown levels in `f`: london, rest_of_south, midlands_wales, north, and scot\n5 unknown levels in `f`: london, rest_of_south, midlands_wales, north, and scot\n5 unknown levels in `f`: london, rest_of_south, midlands_wales, north, and scot\n\n\nWarning: Unknown levels in `f`: london, rest_of_south, midlands_wales, north,\nscot\n\nWarning: Unknown levels in `f`: london, rest_of_south, midlands_wales, north,\nscot\n\nWarning: Unknown levels in `f`: london, rest_of_south, midlands_wales, north,\nscot\n\nWarning: Unknown levels in `f`: london, rest_of_south, midlands_wales, north,\nscot\n\nWarning: Unknown levels in `f`: london, rest_of_south, midlands_wales, north,\nscot\n\nggplot(q2, aes(y = opinion, x = prop,\n               fill = opinion)) +\n  geom_col() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = 'Percent', y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\n\nExercise 3 - Comparing proportions across bars\nRecreate the same visualisation from the previous exercise, this time dodging the bars for opinion proportions for each region, rather than faceting by region and then improve the legend. How is the story this visualisation telling different than the story the previous plot tells?\n\n# code goes here\nggplot(q2, aes(y = opinion, x = prop,\n               fill = opinion)) +\n  geom_col() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = 'Percent', y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\n\nReferences\n\nhttps://datasciencebox.org"
  },
  {
    "objectID": "DANL210_hw2a.html",
    "href": "DANL210_hw2a.html",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here.\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "DANL210_hw2a.html#q1a",
    "href": "DANL210_hw2a.html#q1a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1a",
    "text": "Q1a\nWhat are the minimum, first quartile, median, thrid quartile, maximum, mean, and standard deviation of Close and Volume for each company?\n\n# Group stock data by company and get descriptive statistics for the Close and Volume columns\nq1a = stock.groupby('company')['Close', 'Volume'].describe()\n\n# Rename columns to have the format 'column_name'_'statistic'\nq1a.columns = q1a.columns.get_level_values(0) + '_' +\\\n    q1a.columns.get_level_values(1)\n\n/var/folders/07/nm9t4t294vb5jz6vtqnb6pxm0000gn/T/ipykernel_93466/1541856862.py:2: FutureWarning:\n\nIndexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead."
  },
  {
    "objectID": "DANL210_hw2a.html#q1b",
    "href": "DANL210_hw2a.html#q1b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1b",
    "text": "Q1b\nFind the 10 largest values for Volume. What are the companies and dates associated with those 10 largest values for Volume?\n\n# Calculate dense ranking of stock volumes and add it as a new column to a copy of the stock dataframe\nranking = stock['Volume'].rank(method='dense', ascending=False)\nq1b = stock.copy()\nq1b['ranking'] = ranking\n\n# Sort the dataframe by ranking and select the top 10 stocks\nq1b = (\n  q1b\n  .sort_values(by='ranking')\n  .query('ranking <= 10')\n)"
  },
  {
    "objectID": "DANL210_hw2a.html#q1c",
    "href": "DANL210_hw2a.html#q1c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1c",
    "text": "Q1c\nCalculate the Z-scores of Open and Close for each company using apply().\n\nq1c = (\n  stock.set_index(['company', 'Date'])  # set company and Date as the index\n  .groupby('company')                   # group the data by company\n  .apply(lambda x: ( x - x.mean() ) / x.std() )  # standardize the Close and Volume for each company\n  .reset_index()                       # reset the index to default\n)"
  },
  {
    "objectID": "DANL210_hw2a.html#q1d",
    "href": "DANL210_hw2a.html#q1d",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1d",
    "text": "Q1d\nUse the transform() method on the stock data to represent all the values of Open, High, Low, Close, Adj Close, and Volume in terms of the first date in the data.\nTo do so, divide all values for each company by the values of the first date in the data for that company.\n\n# Set multi-index for DataFrame using 'company' and 'Date' columns\nq1d = stock.set_index(['company', 'Date'])\n\n# Divide each element of q1d by the first element of the corresponding 'company' group\nq1d = ( q1d / q1d.groupby('company').transform('first') )"
  },
  {
    "objectID": "DANL210_hw2a.html#q1e",
    "href": "DANL210_hw2a.html#q1e",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1e",
    "text": "Q1e\nProvide both seaborn code and a simple comment to describe the daily trend of normalized values of Close for each company in one plot. The normalized values of Close are the one calculated from Q1d.\n\ng = sns.lineplot(q1d, # create a line plot using Seaborn\n             x = 'Date', y = 'Close', # specify x and y axes\n             hue = 'company') # specify color grouping by company\n\nimport matplotlib.dates as mdates # import Matplotlib date module\ng.xaxis.set_major_locator(mdates.YearLocator()) # set x-axis ticks to show year\nplt.xticks(rotation=45) # rotate x-axis labels for better readability\n\n(array([-365.,    0.,  365.,  730., 1096., 1461., 1826., 2191., 2557.]),\n [Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, '')])"
  },
  {
    "objectID": "DANL210_hw2a.html#q1f",
    "href": "DANL210_hw2a.html#q1f",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q1f",
    "text": "Q1f\nCreate a box plot of Close for each company in one plot. Make a simple comment on the plot.\n\nsns.boxplot(stock, \n             x = 'company', y = 'Close')\n\n<AxesSubplot:xlabel='company', ylabel='Close'>"
  },
  {
    "objectID": "DANL210_hw2a.html#q2a",
    "href": "DANL210_hw2a.html#q2a",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2a",
    "text": "Q2a\nHow many parties have provided or disbursed positive funding contributions to other countries or regions for their adaptation projects for every single year from 2011 to 2018?\n\nq2a = (\n  climate_finance\n       .query('Status == \"provided\" or Status == \"disbursed\"') # Select only the rows where status is \"provided\" or \"disbursed\"\n       .query('`Type of support` == \"adaptation\"') # Select only the rows where the type of support is \"adaptation\"\n       .groupby(['Party', 'Year']) # Group the data by party and year\n       .agg({'Contribution': 'sum'}) # Calculate the sum of contributions for each party and year\n       .reset_index() # Reset the index of the dataframe\n       .query('Contribution > 0') # Select only the rows where the contribution is greater than 0\n       .groupby(['Party']) # Group the data by party\n       .size() # Count the number of rows for each party\n       .reset_index(name='n') # Reset the index of the dataframe and rename the \"size\" column to \"n\"\n       .query('n == 8') # Select only the rows where the value of \"n\" is 8\n       )\n\nq2a.shape[0] # Output the number of rows in the resulting dataframe\n      \nq2a\n\n\n\n\n\n  \n    \n      \n      Party\n      n\n    \n  \n  \n    \n      2\n      Canada\n      8\n    \n    \n      6\n      Finland\n      8\n    \n    \n      10\n      Iceland\n      8\n    \n    \n      11\n      Ireland\n      8\n    \n    \n      12\n      Japan\n      8\n    \n    \n      16\n      Netherlands\n      8\n    \n    \n      17\n      New Zealand\n      8\n    \n    \n      20\n      Portugal\n      8\n    \n    \n      22\n      Slovakia\n      8\n    \n    \n      24\n      Sweden\n      8\n    \n    \n      25\n      Switzerland\n      8"
  },
  {
    "objectID": "DANL210_hw2a.html#q2b",
    "href": "DANL210_hw2a.html#q2b",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\nFor each party, calculate the total funding contributions that were disbursed or provided for mitigation projects for each year.\n\nq2b = (\n  climate_finance\n       .query('Status == \"provided\" or Status == \"disbursed\"')\n       .query('`Type of support` == \"mitigation\"')\n       .groupby(['Party', 'Year'])\n       .agg({'Contribution': 'sum'})\n       .reset_index()\n       )\n\nq2b\n\n\n\n\n\n  \n    \n      \n      Party\n      Year\n      Contribution\n    \n  \n  \n    \n      0\n      Australia\n      2011\n      5.80\n    \n    \n      1\n      Australia\n      2012\n      36.80\n    \n    \n      2\n      Australia\n      2013\n      38.32\n    \n    \n      3\n      Austria\n      2013\n      62227828.00\n    \n    \n      4\n      Austria\n      2014\n      81472734.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      United Kingdom\n      2012\n      260.37\n    \n    \n      175\n      United Kingdom\n      2013\n      249.41\n    \n    \n      176\n      United Kingdom\n      2014\n      143.75\n    \n    \n      177\n      United Kingdom\n      2015\n      110.98\n    \n    \n      178\n      United Kingdom\n      2016\n      321.60\n    \n  \n\n179 rows × 3 columns"
  },
  {
    "objectID": "DANL210_hw2a.html#q2c",
    "href": "DANL210_hw2a.html#q2c",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2c",
    "text": "Q2c\nFor each party, calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each year.\n\n# Groupby Party, Year, Status, and Type of support and sum the contribution\nq2c_tmp = (\n    climate_finance\n    .groupby(['Party', 'Year', 'Status', 'Type of support'])\n    .agg({'Contribution': 'sum'})\n    .reset_index()    \n    ) \n\n# Filter out rows where the Contribution is 0 and keep only rows with a length of 2 groups\nq2c_tmp = q2c_tmp[ q2c_tmp['Contribution'] != 0 ]\nq2c_tmp = (\n  q2c_tmp\n  .groupby(['Party', 'Year', 'Status'])\n  .filter(lambda x: len(x) == 2)\n  )\n\n# Create a separate dataframe for adaptation and mitigation contributions\nq2ca = q2c_tmp[q2c_tmp['Type of support'] == 'adaptation']\nq2cm = q2c_tmp[q2c_tmp['Type of support'] == 'mitigation']\n\n# Drop the Type of support column, rename the Contribution column, and compute the adaptation to mitigation ratio\nq2c = (\n   q2ca\n   .drop('Type of support', axis=1)\n   .rename(columns={'Contribution': 'adaptation'})\n   .assign(mitigation = q2cm['Contribution'].values,\n           am_ratio = lambda x: x['adaptation'] / x['mitigation'])\n       )"
  },
  {
    "objectID": "DANL210_hw2a.html#q2d",
    "href": "DANL210_hw2a.html#q2d",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2d",
    "text": "Q2d\nProvide both seaborn code and a simple comment to describe the distribution of the ratio between adaptation contribution and mitigation contribution, which is calculated in Q2c.\n\n# Graph 1\nsns.histplot(q2c, x='am_ratio', bins=100)\nplt.axvline(x=1, color='red', linestyle='--')\n\n<matplotlib.lines.Line2D at 0x7fecb0059a30>\n\n\n\n\n\n\n# Graph 2\nsns.histplot(q2c, x=np.log(q2c['am_ratio']), bins=100)\nplt.axvline(x=0, color='red', linestyle='--')\n\n/Users/byeong-hakchoe/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:397: RuntimeWarning:\n\ninvalid value encountered in log\n\n\n\n<matplotlib.lines.Line2D at 0x7fecb01fc220>\n\n\n\n\n\n\n# Graph 3\nsns.kdeplot(q2c, x='am_ratio')\nplt.axvline(x=1, color='red', linestyle='--')\n\n<matplotlib.lines.Line2D at 0x7feccf3b7670>\n\n\n\n\n\n\n# Graph 4\nsns.kdeplot(q2c, x=np.log(q2c['am_ratio']))\nplt.axvline(x=0, color='red', linestyle='--')\n\n/Users/byeong-hakchoe/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:397: RuntimeWarning:\n\ninvalid value encountered in log\n\n\n\n<matplotlib.lines.Line2D at 0x7fecb0383d00>"
  },
  {
    "objectID": "DANL210_hw2a.html#q2e",
    "href": "DANL210_hw2a.html#q2e",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 2",
    "section": "Q2e",
    "text": "Q2e\nProvide both seaborn code and a simple comment to describe how the distribution of Contribution varies by Type of support and Status.\n\n# Graph 1\ng = sns.FacetGrid(data=climate_finance, \n                  row='Status', \n                  hue='Type of support')\ng.map(sns.histplot, \n      'Contribution',\n      alpha = .25)\n\n<seaborn.axisgrid.FacetGrid at 0x7fecb00599d0>\n\n\n\n\n\n\n# Log transformation\nclimate_finance['log_contribution'] = np.log(climate_finance['Contribution'])\n\n/Users/byeong-hakchoe/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:397: RuntimeWarning:\n\ndivide by zero encountered in log\n\n/Users/byeong-hakchoe/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:397: RuntimeWarning:\n\ninvalid value encountered in log\n\n\n\n\n# Graph 2\ng = sns.FacetGrid(data=climate_finance, \n                  row='Status', \n                  hue='Type of support')\ng.map(sns.histplot, \n      'log_contribution',\n      alpha = .25)\n\n<seaborn.axisgrid.FacetGrid at 0x7feca6f94ca0>\n\n\n\n\n\n\n# Graph 3\ng = sns.FacetGrid(data=climate_finance, \n                  row='Status', \n                  hue='Type of support')\ng.map(sns.kdeplot, 'Contribution')\n\n<seaborn.axisgrid.FacetGrid at 0x7feca6f94b50>\n\n\n\n\n\n\n# Graph 4\ng = sns.FacetGrid(data=climate_finance, \n                  row='Status', \n                  hue='Type of support')\ng.map(sns.kdeplot, 'log_contribution')\n\n<seaborn.axisgrid.FacetGrid at 0x7feca7616be0>"
  },
  {
    "objectID": "DANL200_lab4a.html",
    "href": "DANL200_lab4a.html",
    "title": "R Lab 4 - Joins Example Answers",
    "section": "",
    "text": "library(tidyverse)\nlibrary(nycflights13)\nlibrary(skimr)"
  },
  {
    "objectID": "DANL200_lab4a.html#q1a",
    "href": "DANL200_lab4a.html#q1a",
    "title": "R Lab 4 - Joins Example Answers",
    "section": "Q1a",
    "text": "Q1a\nWrite a R code to join the two given data.frames along rows and assign all data.\n\ndf_r <- rbind(student_data1, student_data2)"
  },
  {
    "objectID": "DANL200_lab4a.html#q1b",
    "href": "DANL200_lab4a.html#q1b",
    "title": "R Lab 4 - Joins Example Answers",
    "section": "Q1b",
    "text": "Q1b\nWrite a R code to join the two given data.frames along columns and assign all data.\n\ndf_c <- cbind(student_data1, student_data2)"
  },
  {
    "objectID": "DANL200_lab4a.html#q2a",
    "href": "DANL200_lab4a.html#q2a",
    "title": "R Lab 4 - Joins Example Answers",
    "section": "Q2a",
    "text": "Q2a\nJoin flights with airports, so that we can see information about each flight’s destination airport in the flights data.frame.\n\nq2a <- flights %>% \n  left_join(airports, by = c('dest' = 'faa'))"
  },
  {
    "objectID": "DANL200_lab4a.html#q2b",
    "href": "DANL200_lab4a.html#q2b",
    "title": "R Lab 4 - Joins Example Answers",
    "section": "Q2b",
    "text": "Q2b\nFind the airline that has the longest positive dep_delay on average.\n\nflights <- flights %>% \n  left_join(airlines)\n\nq2b <- flights %>% \n  group_by(carrier, name) %>% \n  summarize(delay_mean = mean(dep_delay[dep_delay> 0],\n                              na.rm = T)\n            ) %>% \n  arrange(-delay_mean)\n\nq2b\n\n# A tibble: 16 × 3\n# Groups:   carrier [16]\n   carrier name                        delay_mean\n   <chr>   <chr>                            <dbl>\n 1 OO      SkyWest Airlines Inc.             58  \n 2 YV      Mesa Airlines Inc.                53.0\n 3 EV      ExpressJet Airlines Inc.          50.3\n 4 9E      Endeavor Air Inc.                 48.9\n 5 F9      Frontier Airlines Inc.            45.1\n 6 MQ      Envoy Air                         44.9\n 7 HA      Hawaiian Airlines Inc.            44.8\n 8 FL      AirTran Airways Corporation       40.8\n 9 B6      JetBlue Airways                   39.8\n10 DL      Delta Air Lines Inc.              37.4\n11 AA      American Airlines Inc.            37.2\n12 WN      Southwest Airlines Co.            34.9\n13 VX      Virgin America                    34.5\n14 US      US Airways Inc.                   33.1\n15 AS      Alaska Airlines Inc.              31.3\n16 UA      United Air Lines Inc.             29.9"
  },
  {
    "objectID": "DANL200_lab4a.html#q2c",
    "href": "DANL200_lab4a.html#q2c",
    "title": "R Lab 4 - Joins Example Answers",
    "section": "Q2c",
    "text": "Q2c\nFind the airline that has the largest proportion of flights with longer than 30-minute dep_delay.\n\nq2c <- flights %>% \n  mutate(delay_over_30 = ifelse(dep_delay > 30, \n                                1, 0) ) %>% \n  group_by(carrier, name) %>% \n  summarize(pct = 100 * mean(delay_over_30,\n                              na.rm = T)) %>% \n  arrange(-pct)\n\nq2c\n\n# A tibble: 16 × 3\n# Groups:   carrier [16]\n   carrier name                          pct\n   <chr>   <chr>                       <dbl>\n 1 EV      ExpressJet Airlines Inc.    22.7 \n 2 YV      Mesa Airlines Inc.          21.1 \n 3 F9      Frontier Airlines Inc.      19.1 \n 4 9E      Endeavor Air Inc.           18.7 \n 5 OO      SkyWest Airlines Inc.       17.2 \n 6 FL      AirTran Airways Corporation 17.2 \n 7 WN      Southwest Airlines Co.      16.8 \n 8 B6      JetBlue Airways             15.5 \n 9 MQ      Envoy Air                   14.8 \n10 UA      United Air Lines Inc.       13.1 \n11 VX      Virgin America              11.4 \n12 AA      American Airlines Inc.      11.0 \n13 DL      Delta Air Lines Inc.        10.4 \n14 AS      Alaska Airlines Inc.         8.85\n15 US      US Airways Inc.              8.12\n16 HA      Hawaiian Airlines Inc.       4.68"
  },
  {
    "objectID": "DANL210_lab1a.html",
    "href": "DANL210_lab1a.html",
    "title": "Python Lab 1 - Pandas Group Operations",
    "section": "",
    "text": "Load Data\n\nimport pandas as pd\ndf_ny = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_pop.csv')\ndf_ny.head(10)\n\n\n\n\n\n  \n    \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n  \n  \n    \n      0\n      36001\n      Albany\n      2015\n      55120\n      44478\n      204024\n    \n    \n      1\n      36001\n      Albany\n      2016\n      55126\n      45357\n      204003\n    \n    \n      2\n      36001\n      Albany\n      2017\n      58814\n      45589\n      204833\n    \n    \n      3\n      36001\n      Albany\n      2018\n      59547\n      45521\n      204509\n    \n    \n      4\n      36001\n      Albany\n      2019\n      61876\n      45150\n      204918\n    \n    \n      5\n      36001\n      Albany\n      2020\n      66632\n      44608\n      205082\n    \n    \n      6\n      36003\n      Allegany\n      2015\n      32205\n      7461\n      30568\n    \n    \n      7\n      36003\n      Allegany\n      2016\n      32417\n      7493\n      30449\n    \n    \n      8\n      36003\n      Allegany\n      2017\n      34001\n      7377\n      30331\n    \n    \n      9\n      36003\n      Allegany\n      2018\n      34553\n      7284\n      30155\n    \n  \n\n\n\n\n\nVariable Description\n\nFIPS: ID number for a county\npincp: average personal income in a county X in year Y\npop_18_24: population 18 to 24 years\npop_25_over: population 25 years and over\n\n\n\n\nQ1a\n\nUse .sort_values() to find the top 5 rich counties in NY for each year.\n\nDo not use .apply().\n\n\n\n# Sorts the DataFrame 'df_ny' by the column 'pincp' in descending order \n# and takes the top 5 records for each 'year' group.\n# Then, sorts the resulting DataFrame by 'year' in ascending order and 'pincp' in descending order.\nq1a = (\n      df_ny\n      .sort_values(by='pincp', ascending=False)\n      .groupby('year').head(5)\n      .sort_values(by=['year', 'pincp'], ascending=[True, False])\n    )\n\nq1a\n\n\n\n\n\n  \n    \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n  \n  \n    \n      180\n      36061\n      New York\n      2015\n      152793\n      161844\n      1229036\n    \n    \n      354\n      36119\n      Westchester\n      2015\n      93495\n      83942\n      659258\n    \n    \n      174\n      36059\n      Nassau\n      2015\n      79301\n      120229\n      931785\n    \n    \n      270\n      36091\n      Saratoga\n      2015\n      61407\n      18696\n      156862\n    \n    \n      306\n      36103\n      Suffolk\n      2015\n      61203\n      138167\n      1022970\n    \n    \n      181\n      36061\n      New York\n      2016\n      163112\n      158011\n      1237623\n    \n    \n      355\n      36119\n      Westchester\n      2016\n      96251\n      85574\n      661283\n    \n    \n      175\n      36059\n      Nassau\n      2016\n      81500\n      121621\n      934765\n    \n    \n      307\n      36103\n      Suffolk\n      2016\n      63757\n      138819\n      1024860\n    \n    \n      271\n      36091\n      Saratoga\n      2016\n      63065\n      18928\n      158211\n    \n    \n      182\n      36061\n      New York\n      2017\n      179655\n      155089\n      1259137\n    \n    \n      356\n      36119\n      Westchester\n      2017\n      102861\n      86345\n      668102\n    \n    \n      176\n      36059\n      Nassau\n      2017\n      85859\n      122061\n      942504\n    \n    \n      308\n      36103\n      Suffolk\n      2017\n      66429\n      138040\n      1030141\n    \n    \n      272\n      36091\n      Saratoga\n      2017\n      65490\n      18899\n      160285\n    \n    \n      183\n      36061\n      New York\n      2018\n      184539\n      149638\n      1247071\n    \n    \n      357\n      36119\n      Westchester\n      2018\n      107252\n      85757\n      665958\n    \n    \n      177\n      36059\n      Nassau\n      2018\n      89242\n      120849\n      940610\n    \n    \n      273\n      36091\n      Saratoga\n      2018\n      70010\n      18915\n      161493\n    \n    \n      309\n      36103\n      Suffolk\n      2018\n      69209\n      136173\n      1028820\n    \n    \n      184\n      36061\n      New York\n      2019\n      187213\n      147692\n      1249365\n    \n    \n      358\n      36119\n      Westchester\n      2019\n      112037\n      85499\n      668290\n    \n    \n      178\n      36059\n      Nassau\n      2019\n      92159\n      119754\n      943013\n    \n    \n      274\n      36091\n      Saratoga\n      2019\n      72219\n      19003\n      163010\n    \n    \n      310\n      36103\n      Suffolk\n      2019\n      72180\n      134777\n      1030842\n    \n    \n      185\n      36061\n      New York\n      2020\n      191220\n      145611\n      1250303\n    \n    \n      359\n      36119\n      Westchester\n      2020\n      115386\n      85113\n      670717\n    \n    \n      179\n      36059\n      Nassau\n      2020\n      96253\n      118047\n      945100\n    \n    \n      275\n      36091\n      Saratoga\n      2020\n      77398\n      18725\n      164817\n    \n    \n      311\n      36103\n      Suffolk\n      2020\n      76713\n      133732\n      1033886\n    \n  \n\n\n\n\n\n\nQ1b\n\nUse .rank() to find the top 5 rich counties in NY for each year.\n\nDo not use apply().\n\n\n\n# Creates a new column 'ranking' in DataFrame 'df_ny' using the 'pincp' column to calculate rankings within each 'year' group\nq1b = (\n   df_ny.assign(\n          ranking = df_ny.groupby('year')['pincp']\n                         .rank(method = 'dense', ascending = False)\n                         )\n)\n\n# Filters the records in DataFrame 'q1b' where the 'ranking' column is less than or equal to 5.\n# Then, sorts the resulting DataFrame by 'year' and 'ranking'.\nq1b = (\n  q1b.query('ranking <= 5')\n  .sort_values(by=['year', 'ranking'])\n)\n\nq1b\n\n\n\n\n\n  \n    \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n      ranking\n    \n  \n  \n    \n      180\n      36061\n      New York\n      2015\n      152793\n      161844\n      1229036\n      1.0\n    \n    \n      354\n      36119\n      Westchester\n      2015\n      93495\n      83942\n      659258\n      2.0\n    \n    \n      174\n      36059\n      Nassau\n      2015\n      79301\n      120229\n      931785\n      3.0\n    \n    \n      270\n      36091\n      Saratoga\n      2015\n      61407\n      18696\n      156862\n      4.0\n    \n    \n      306\n      36103\n      Suffolk\n      2015\n      61203\n      138167\n      1022970\n      5.0\n    \n    \n      181\n      36061\n      New York\n      2016\n      163112\n      158011\n      1237623\n      1.0\n    \n    \n      355\n      36119\n      Westchester\n      2016\n      96251\n      85574\n      661283\n      2.0\n    \n    \n      175\n      36059\n      Nassau\n      2016\n      81500\n      121621\n      934765\n      3.0\n    \n    \n      307\n      36103\n      Suffolk\n      2016\n      63757\n      138819\n      1024860\n      4.0\n    \n    \n      271\n      36091\n      Saratoga\n      2016\n      63065\n      18928\n      158211\n      5.0\n    \n    \n      182\n      36061\n      New York\n      2017\n      179655\n      155089\n      1259137\n      1.0\n    \n    \n      356\n      36119\n      Westchester\n      2017\n      102861\n      86345\n      668102\n      2.0\n    \n    \n      176\n      36059\n      Nassau\n      2017\n      85859\n      122061\n      942504\n      3.0\n    \n    \n      308\n      36103\n      Suffolk\n      2017\n      66429\n      138040\n      1030141\n      4.0\n    \n    \n      272\n      36091\n      Saratoga\n      2017\n      65490\n      18899\n      160285\n      5.0\n    \n    \n      183\n      36061\n      New York\n      2018\n      184539\n      149638\n      1247071\n      1.0\n    \n    \n      357\n      36119\n      Westchester\n      2018\n      107252\n      85757\n      665958\n      2.0\n    \n    \n      177\n      36059\n      Nassau\n      2018\n      89242\n      120849\n      940610\n      3.0\n    \n    \n      273\n      36091\n      Saratoga\n      2018\n      70010\n      18915\n      161493\n      4.0\n    \n    \n      309\n      36103\n      Suffolk\n      2018\n      69209\n      136173\n      1028820\n      5.0\n    \n    \n      184\n      36061\n      New York\n      2019\n      187213\n      147692\n      1249365\n      1.0\n    \n    \n      358\n      36119\n      Westchester\n      2019\n      112037\n      85499\n      668290\n      2.0\n    \n    \n      178\n      36059\n      Nassau\n      2019\n      92159\n      119754\n      943013\n      3.0\n    \n    \n      274\n      36091\n      Saratoga\n      2019\n      72219\n      19003\n      163010\n      4.0\n    \n    \n      310\n      36103\n      Suffolk\n      2019\n      72180\n      134777\n      1030842\n      5.0\n    \n    \n      185\n      36061\n      New York\n      2020\n      191220\n      145611\n      1250303\n      1.0\n    \n    \n      359\n      36119\n      Westchester\n      2020\n      115386\n      85113\n      670717\n      2.0\n    \n    \n      179\n      36059\n      Nassau\n      2020\n      96253\n      118047\n      945100\n      3.0\n    \n    \n      275\n      36091\n      Saratoga\n      2020\n      77398\n      18725\n      164817\n      4.0\n    \n    \n      311\n      36103\n      Suffolk\n      2020\n      76713\n      133732\n      1033886\n      5.0\n    \n  \n\n\n\n\n\n\nQ1c\n\nUse apply() with a lambda function and .sort_values() to find the top 5 rich counties in NY for each year.\n\n\n# Groups the DataFrame 'df_ny' by 'year' and applies a lambda function on each group.\n# The lambda function sorts each group by 'pincp' in descending order and selects the top 5 records.\n# The resulting DataFrame is the concatenation of these groups.\nq1c = (\n      df_ny\n      .groupby('year')\n      .apply(lambda x: x.sort_values(['pincp'], ascending=False).head())\n    )\n\nq1c\n\n\n\n\n\n  \n    \n      \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      180\n      36061\n      New York\n      2015\n      152793\n      161844\n      1229036\n    \n    \n      354\n      36119\n      Westchester\n      2015\n      93495\n      83942\n      659258\n    \n    \n      174\n      36059\n      Nassau\n      2015\n      79301\n      120229\n      931785\n    \n    \n      270\n      36091\n      Saratoga\n      2015\n      61407\n      18696\n      156862\n    \n    \n      306\n      36103\n      Suffolk\n      2015\n      61203\n      138167\n      1022970\n    \n    \n      2016\n      181\n      36061\n      New York\n      2016\n      163112\n      158011\n      1237623\n    \n    \n      355\n      36119\n      Westchester\n      2016\n      96251\n      85574\n      661283\n    \n    \n      175\n      36059\n      Nassau\n      2016\n      81500\n      121621\n      934765\n    \n    \n      307\n      36103\n      Suffolk\n      2016\n      63757\n      138819\n      1024860\n    \n    \n      271\n      36091\n      Saratoga\n      2016\n      63065\n      18928\n      158211\n    \n    \n      2017\n      182\n      36061\n      New York\n      2017\n      179655\n      155089\n      1259137\n    \n    \n      356\n      36119\n      Westchester\n      2017\n      102861\n      86345\n      668102\n    \n    \n      176\n      36059\n      Nassau\n      2017\n      85859\n      122061\n      942504\n    \n    \n      308\n      36103\n      Suffolk\n      2017\n      66429\n      138040\n      1030141\n    \n    \n      272\n      36091\n      Saratoga\n      2017\n      65490\n      18899\n      160285\n    \n    \n      2018\n      183\n      36061\n      New York\n      2018\n      184539\n      149638\n      1247071\n    \n    \n      357\n      36119\n      Westchester\n      2018\n      107252\n      85757\n      665958\n    \n    \n      177\n      36059\n      Nassau\n      2018\n      89242\n      120849\n      940610\n    \n    \n      273\n      36091\n      Saratoga\n      2018\n      70010\n      18915\n      161493\n    \n    \n      309\n      36103\n      Suffolk\n      2018\n      69209\n      136173\n      1028820\n    \n    \n      2019\n      184\n      36061\n      New York\n      2019\n      187213\n      147692\n      1249365\n    \n    \n      358\n      36119\n      Westchester\n      2019\n      112037\n      85499\n      668290\n    \n    \n      178\n      36059\n      Nassau\n      2019\n      92159\n      119754\n      943013\n    \n    \n      274\n      36091\n      Saratoga\n      2019\n      72219\n      19003\n      163010\n    \n    \n      310\n      36103\n      Suffolk\n      2019\n      72180\n      134777\n      1030842\n    \n    \n      2020\n      185\n      36061\n      New York\n      2020\n      191220\n      145611\n      1250303\n    \n    \n      359\n      36119\n      Westchester\n      2020\n      115386\n      85113\n      670717\n    \n    \n      179\n      36059\n      Nassau\n      2020\n      96253\n      118047\n      945100\n    \n    \n      275\n      36091\n      Saratoga\n      2020\n      77398\n      18725\n      164817\n    \n    \n      311\n      36103\n      Suffolk\n      2020\n      76713\n      133732\n      1033886\n    \n  \n\n\n\n\n\n\nQ1d\n\nWrite a function with def and .sort_values() that selects the top 5 pincp values.\nThen, use the defined function in apply() to find the top 5 rich counties in NY for each year.\n\n\n# Defines a function 'top' that sorts a DataFrame by the specified column in descending order and \n# selects the top 'n' records. If 'n' is not specified, the function selects the top 5 records.\ndef top(df, n=5, column=\"pincp\"):\n    return df.sort_values(column, ascending=False)[:n]\n  \n# Groups the DataFrame 'df_ny' by 'year' and applies the 'top' function on each group.\n# The resulting DataFrame is the concatenation of these groups.\nq1d = df_ny.groupby('year').apply(top)\n\nq1d\n\n\n\n\n\n  \n    \n      \n      \n      FIPS\n      county_name\n      year\n      pincp\n      pop_18_24\n      pop_25_over\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2015\n      180\n      36061\n      New York\n      2015\n      152793\n      161844\n      1229036\n    \n    \n      354\n      36119\n      Westchester\n      2015\n      93495\n      83942\n      659258\n    \n    \n      174\n      36059\n      Nassau\n      2015\n      79301\n      120229\n      931785\n    \n    \n      270\n      36091\n      Saratoga\n      2015\n      61407\n      18696\n      156862\n    \n    \n      306\n      36103\n      Suffolk\n      2015\n      61203\n      138167\n      1022970\n    \n    \n      2016\n      181\n      36061\n      New York\n      2016\n      163112\n      158011\n      1237623\n    \n    \n      355\n      36119\n      Westchester\n      2016\n      96251\n      85574\n      661283\n    \n    \n      175\n      36059\n      Nassau\n      2016\n      81500\n      121621\n      934765\n    \n    \n      307\n      36103\n      Suffolk\n      2016\n      63757\n      138819\n      1024860\n    \n    \n      271\n      36091\n      Saratoga\n      2016\n      63065\n      18928\n      158211\n    \n    \n      2017\n      182\n      36061\n      New York\n      2017\n      179655\n      155089\n      1259137\n    \n    \n      356\n      36119\n      Westchester\n      2017\n      102861\n      86345\n      668102\n    \n    \n      176\n      36059\n      Nassau\n      2017\n      85859\n      122061\n      942504\n    \n    \n      308\n      36103\n      Suffolk\n      2017\n      66429\n      138040\n      1030141\n    \n    \n      272\n      36091\n      Saratoga\n      2017\n      65490\n      18899\n      160285\n    \n    \n      2018\n      183\n      36061\n      New York\n      2018\n      184539\n      149638\n      1247071\n    \n    \n      357\n      36119\n      Westchester\n      2018\n      107252\n      85757\n      665958\n    \n    \n      177\n      36059\n      Nassau\n      2018\n      89242\n      120849\n      940610\n    \n    \n      273\n      36091\n      Saratoga\n      2018\n      70010\n      18915\n      161493\n    \n    \n      309\n      36103\n      Suffolk\n      2018\n      69209\n      136173\n      1028820\n    \n    \n      2019\n      184\n      36061\n      New York\n      2019\n      187213\n      147692\n      1249365\n    \n    \n      358\n      36119\n      Westchester\n      2019\n      112037\n      85499\n      668290\n    \n    \n      178\n      36059\n      Nassau\n      2019\n      92159\n      119754\n      943013\n    \n    \n      274\n      36091\n      Saratoga\n      2019\n      72219\n      19003\n      163010\n    \n    \n      310\n      36103\n      Suffolk\n      2019\n      72180\n      134777\n      1030842\n    \n    \n      2020\n      185\n      36061\n      New York\n      2020\n      191220\n      145611\n      1250303\n    \n    \n      359\n      36119\n      Westchester\n      2020\n      115386\n      85113\n      670717\n    \n    \n      179\n      36059\n      Nassau\n      2020\n      96253\n      118047\n      945100\n    \n    \n      275\n      36091\n      Saratoga\n      2020\n      77398\n      18725\n      164817\n    \n    \n      311\n      36103\n      Suffolk\n      2020\n      76713\n      133732\n      1033886\n    \n  \n\n\n\n\n\n\ncf) SeriesGroupBy.nlargest()\n\n# set the 'county_name' column as the index\ndf_ny.set_index('county_name', inplace=True) # to know what counties are corresponding to selected rows after `SeriesGroupBy.nlargest()` operation.\n\n\n# Groups the DataFrame 'df_ny' by 'year' and selects the 'pincp' column of each group.\n# For each group, selects the top 5 records based on the values in the 'pincp' column.\nq1cf = df_ny.groupby('year')['pincp'].nlargest(5)\n\nq1cf\n\nyear  county_name\n2015  New York       152793\n      Westchester     93495\n      Nassau          79301\n      Saratoga        61407\n      Suffolk         61203\n2016  New York       163112\n      Westchester     96251\n      Nassau          81500\n      Suffolk         63757\n      Saratoga        63065\n2017  New York       179655\n      Westchester    102861\n      Nassau          85859\n      Suffolk         66429\n      Saratoga        65490\n2018  New York       184539\n      Westchester    107252\n      Nassau          89242\n      Saratoga        70010\n      Suffolk         69209\n2019  New York       187213\n      Westchester    112037\n      Nassau          92159\n      Saratoga        72219\n      Suffolk         72180\n2020  New York       191220\n      Westchester    115386\n      Nassau          96253\n      Saratoga        77398\n      Suffolk         76713\nName: pincp, dtype: int64\n\n\n\n\nQ1e\n\nVisualize the yearly trend of the mean level of pincp.\n\n\n# Groups the DataFrame 'df_ny' by 'year' and selects the 'pincp' column of each group.\n# Calculates the mean of the 'pincp' column for each group.\nq1e = df_ny.groupby('year')['pincp'].mean()\n\n# Plots the resulting Series using the default line plot.\nq1e.plot()\n\n<AxesSubplot:xlabel='year'>"
  },
  {
    "objectID": "mba-ch7-tree-based-methods.html#classification-and-regression-tree-cart",
    "href": "mba-ch7-tree-based-methods.html#classification-and-regression-tree-cart",
    "title": "Tree-based Methods",
    "section": "Classification And Regression Tree (CART)",
    "text": "Classification And Regression Tree (CART)\n\n\n\n\n\n\n\n\n\n\nTree-logic uses a series of steps to come to a conclusion.\nThe trick is to have mini-decisions combine for good choices.\nEach decision is a node, and the final prediction is a leaf node.\n\n\nDecision trees are useful for both classification and regression.\n\nDecision trees can take any type of data, numerical or categorical.\nDecision trees make fewer assumptions about the relationship between x and y.\n\nE.g., linear model assumes the linear relationship between x and y.\nDecision trees naturally express certain kinds of interactions among the input variables: those of the form “IF x is true AND y is true, THEN….”\n\n\n\n\nClassification trees have class probabilities at the leaves.\n\nProbability I’ll be in heavy rain is 0.9 (so take an umbrella).\n\nRegression trees have a mean response at the leaves.\n\nThe expected amount of rain is 2 inches (so take an umbrella).\n\nCART: Classification and Regression Trees.\n\n\n\nWe need a way to estimate the sequence of decisions.\n\nHow many are they?\nWhat is the order?\n\nCART grows the tree through a sequence of splits:\n\nGiven any set (node) of data, we can find the optimal split (the error minimizing split) and divide into two child sets.\nWe then look at each child set, and again find the optimal split to divide it into two homogeneous subsets.\nThe children become parents, and we look again for the optimal split on their new children (the grandchildren!).\n\nWe stop splitting and growing when the size of the leaf nodes hits some minimum threshold (e.g., say no less than 10 observations per leaf).\n\n\nObjective at each split: find the best variable to partition the data into one of two regions, \\(R_1\\) & \\(R_2\\), to minimize the error between the actual response, \\(y_i\\), and the node’s predicted constant, \\(c_i\\)\n\nFor regression we minimize the sum of squared errors (SSE):\n\n\\[\nS S E=\\sum_{i \\in R_{1}}\\left(y_{i}-c_{1}\\right)^{2}+\\sum_{i \\in R_{2}}\\left(y_{i}-c_{2}\\right)^{2}\n\\]\n\nFor classification trees we minimize the node’s impurity the Gini index\n\nwhere \\(p_k\\) is the proportion of observations in the node belonging to class \\(k\\) out of \\(K\\) total classes\nwant to minimize \\(Gini\\): small values indicate a node has primarily one class (is more pure)\nGini impurity measures the degree of a particular variable being wrongly classified when it is randomly chosen.\n\n\n\\[\nGini = 1 - \\sum_k^K p_k^2\n\\]\n\n\nNBC Show Data\n\nThe dataset (nbc and demog) is from NBC’s TV pilots:\n\nGross Ratings Points (GRP): estimated total viewership, which measures broadcast marketability.\nProjected Engagement (PE): a more suitable measure of audience.\nAfter watching a show, viewer is quizzed on order and detail.\nThis measures their engagement with the show (and ads!).\n\n\n\nlibrary(tidyverse)\nnbc <- read_csv('https://bcdanl.github.io/data/nbc_show.csv')\nnbc$Genre <- as.factor(nbc$Genre)\n\n\n\n\n\n  \n\n\n\n\nskim(nbc)\n\n\nData summary\n\n\nName\nnbc\n\n\nNumber of rows\n40\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nShow\n0\n1\n4\n34\n0\n40\n0\n\n\nNetwork\n0\n1\n2\n5\n0\n14\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGenre\n0\n1\nFALSE\n3\nDra: 19, Rea: 17, Sit: 4\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPE\n0\n1\n72.68\n12.03\n30.0\n64.58\n77.06\n80.16\n89.29\n▁▁▃▃▇\n\n\nGRP\n0\n1\n823.90\n683.42\n7.5\n301.45\n647.90\n1200.45\n2773.80\n▇▅▂▂▁\n\n\nDuration\n0\n1\n50.25\n14.23\n30.0\n30.00\n60.00\n60.00\n60.00\n▃▁▁▁▇\n\n\n\n\n\n\nggplot(nbc) +\n  geom_point(aes(x = GRP, y = PE, color = Genre),\n             alpha = .75)\n\n\n\n\n\nConsider a classification tree to predict Genre from demographics.\n\nOutput from tree shows a series of decision nodes and the proportion in each Genre at these nodes, down to the leaves.\n\n\n\ndemog <- read_csv(\n  'https://bcdanl.github.io/data/nbc_demog.csv'\n)\n\n\n\n\n\n  \n\n\n\n\nskim(demog)\n\n\nData summary\n\n\nName\ndemog\n\n\nNumber of rows\n40\n\n\nNumber of columns\n57\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n56\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nShow\n0\n1\n4\n34\n0\n40\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTERRITORY.EAST.CENTRAL\n0\n1\n13.51\n2.78\n6.16\n11.73\n14.05\n15.36\n19.98\n▁▃▆▇▁\n\n\nTERRITORY.NORTHEAST\n0\n1\n20.35\n3.17\n14.01\n18.04\n20.31\n22.49\n27.80\n▂▇▇▅▂\n\n\nTERRITORY.PACIFIC\n0\n1\n17.42\n3.67\n9.47\n15.24\n17.72\n19.68\n28.79\n▃▃▇▁▁\n\n\nTERRITORY.SOUTHEAST\n0\n1\n21.56\n3.68\n16.23\n19.21\n21.32\n22.92\n37.11\n▇▇▂▁▁\n\n\nTERRITORY.SOUTHWEST\n0\n1\n11.83\n2.29\n7.68\n10.47\n11.58\n13.45\n16.24\n▃▇▇▅▅\n\n\nTERRITORY.WEST.CENTRAL\n0\n1\n15.33\n3.15\n9.64\n12.99\n15.01\n16.93\n22.69\n▂▇▇▂▂\n\n\nCOUNTY.SIZE.A\n0\n1\n39.27\n6.01\n28.36\n35.14\n39.55\n42.75\n51.55\n▃▇▇▃▃\n\n\nCOUNTY.SIZE.B\n0\n1\n32.14\n3.29\n22.30\n30.65\n32.04\n33.69\n38.60\n▁▁▇▆▂\n\n\nCOUNTY.SIZE.C\n0\n1\n14.67\n2.73\n8.24\n13.00\n14.66\n16.37\n20.55\n▂▅▇▇▂\n\n\nCOUNTY.SIZE.D\n0\n1\n13.93\n3.80\n4.47\n11.92\n14.11\n16.60\n24.95\n▂▆▇▅▁\n\n\nWIRED.CABLE.W.PAY\n0\n1\n34.63\n6.01\n24.79\n30.34\n34.74\n36.97\n48.91\n▅▅▇▂▂\n\n\nWIRED.CABLE.W.O.PAY\n0\n1\n29.58\n5.33\n21.95\n25.73\n28.45\n34.24\n43.60\n▇▇▅▂▂\n\n\nDBS.OWNER\n0\n1\n23.60\n4.85\n10.03\n20.47\n24.85\n26.50\n31.83\n▁▂▃▇▃\n\n\nBROADCAST.ONLY\n0\n1\n12.72\n11.42\n0.00\n0.00\n14.04\n20.96\n37.96\n▇▃▆▂▁\n\n\nVIDEO.GAME.OWNER\n0\n1\n57.20\n4.91\n49.02\n53.61\n56.03\n60.48\n67.87\n▂▇▃▂▂\n\n\nDVD.OWNER\n0\n1\n92.77\n2.15\n87.44\n91.44\n92.73\n94.23\n98.46\n▁▇▇▇▁\n\n\nVCR.OWNER\n0\n1\n84.37\n3.42\n74.14\n82.66\n85.40\n86.55\n90.40\n▁▁▃▇▂\n\n\nX1.TV.SET\n0\n1\n11.53\n3.28\n1.54\n9.90\n11.11\n13.95\n19.30\n▁▂▇▂▂\n\n\nX2.TV.SETS\n0\n1\n27.03\n2.71\n21.72\n25.29\n27.18\n28.49\n34.05\n▂▆▇▃▁\n\n\nX3.TV.SETS\n0\n1\n28.37\n3.43\n21.72\n26.53\n28.14\n29.67\n39.85\n▂▇▅▁▁\n\n\nX4..TV.SETS\n0\n1\n33.08\n3.41\n27.48\n31.14\n32.64\n34.70\n43.72\n▃▇▃▁▁\n\n\nX1.PERSON\n0\n1\n10.92\n3.54\n1.78\n9.16\n10.90\n12.99\n21.98\n▁▃▇▂▁\n\n\nX2.PERSONS\n0\n1\n25.15\n2.69\n19.54\n23.32\n25.43\n26.58\n30.58\n▃▃▇▇▃\n\n\nX3.PERSONS\n0\n1\n22.84\n2.00\n18.85\n21.61\n22.61\n23.62\n28.41\n▂▇▆▂▂\n\n\nX4..PERSONS\n0\n1\n41.09\n4.72\n28.41\n39.12\n40.59\n42.96\n56.75\n▁▇▇▂▁\n\n\nHOH..25\n0\n1\n6.52\n3.78\n0.83\n4.34\n6.00\n7.96\n20.36\n▆▇▂▁▁\n\n\nHOH.25.34\n0\n1\n25.07\n4.75\n16.17\n21.21\n26.06\n28.69\n32.47\n▅▃▅▇▅\n\n\nHOH.35.44\n0\n1\n33.70\n4.58\n19.13\n31.34\n33.93\n36.08\n45.02\n▁▁▇▆▁\n\n\nHOH.45.54\n0\n1\n26.01\n4.17\n14.76\n23.03\n25.59\n28.89\n33.18\n▁▂▇▅▅\n\n\nHOH.55.64\n0\n1\n5.70\n1.60\n2.49\n4.40\n6.00\n7.00\n8.37\n▃▆▅▇▆\n\n\nHOH.65.\n0\n1\n3.01\n1.09\n0.55\n2.46\n3.03\n3.85\n4.92\n▁▅▇▅▃\n\n\nX1.3.YRS.COLLEGE\n0\n1\n32.90\n2.70\n26.18\n31.67\n32.63\n34.39\n41.94\n▂▇▇▁▁\n\n\nX4..YRS.COLLEGE\n0\n1\n29.72\n8.94\n11.16\n24.29\n29.54\n35.60\n47.39\n▂▃▇▃▂\n\n\nX4.YRS.H.S.\n0\n1\n29.50\n5.90\n16.22\n25.11\n29.68\n33.23\n42.57\n▂▅▇▅▂\n\n\nWHITE.COLLAR\n0\n1\n50.85\n7.38\n30.92\n47.97\n51.62\n56.47\n60.99\n▁▃▅▇▇\n\n\nBLUE.COLLAR\n0\n1\n30.20\n4.93\n22.05\n27.63\n29.59\n32.95\n46.99\n▃▇▃▁▁\n\n\nNOT.IN.LABOR.FORCE\n0\n1\n18.95\n4.66\n8.86\n16.08\n17.57\n22.09\n31.64\n▁▇▅▁▁\n\n\nBLACK\n0\n1\n14.00\n5.66\n2.56\n10.74\n12.90\n16.48\n35.34\n▂▇▆▁▁\n\n\nWHITE\n0\n1\n78.54\n7.03\n53.96\n75.71\n78.21\n83.15\n91.53\n▁▁▅▇▃\n\n\nOTHER\n0\n1\n7.46\n2.29\n4.06\n5.98\n6.65\n8.55\n12.58\n▅▇▃▂▂\n\n\nANY.CHILDREN.2.5\n0\n1\n18.17\n3.13\n12.16\n15.89\n17.98\n20.06\n25.21\n▃▅▇▃▂\n\n\nANY.CHILDREN.6.11\n0\n1\n22.75\n3.98\n13.64\n21.03\n22.70\n24.60\n36.49\n▂▇▇▁▁\n\n\nANY.CHILDREN.12.17\n0\n1\n24.37\n4.53\n17.80\n22.30\n24.11\n25.57\n40.40\n▃▇▁▁▁\n\n\nANY.CATS\n0\n1\n33.98\n3.76\n26.32\n31.31\n33.98\n36.49\n41.59\n▂▇▅▇▂\n\n\nANY.DOGS\n0\n1\n48.46\n4.47\n36.77\n46.77\n48.53\n50.67\n62.94\n▁▅▇▂▁\n\n\nMALE.HOH\n0\n1\n48.87\n5.05\n36.26\n46.02\n48.83\n51.89\n56.89\n▂▂▇▇▆\n\n\nFEMALE.HOH\n0\n1\n51.14\n5.06\n43.10\n48.12\n51.13\n53.99\n63.97\n▆▇▇▂▂\n\n\nINCOME.30.74K.\n0\n1\n41.74\n3.55\n31.04\n40.23\n41.48\n43.64\n54.32\n▁▃▇▁▁\n\n\nINCOME.75K.\n0\n1\n37.22\n8.82\n20.11\n29.95\n37.78\n43.89\n56.87\n▃▅▅▇▁\n\n\nHISPANIC.ORIGIN\n0\n1\n8.18\n3.34\n3.85\n5.60\n7.85\n9.75\n19.56\n▇▆▂▁▁\n\n\nNON.HISPANIC.ORIGIN\n0\n1\n91.82\n3.35\n80.34\n90.21\n92.12\n94.42\n96.15\n▁▁▂▆▇\n\n\nHOME.IS.OWNED\n0\n1\n70.27\n7.56\n52.59\n67.42\n73.01\n75.54\n80.88\n▃▂▃▇▇\n\n\nHOME.IS.RENTED\n0\n1\n29.72\n7.55\n19.12\n24.46\n26.99\n32.58\n47.39\n▇▇▃▂▃\n\n\nPC.NON.OWNER\n0\n1\n15.53\n5.99\n5.80\n11.80\n14.00\n19.82\n28.37\n▃▇▃▃▂\n\n\nPC.OWNER.WITH.INTERNET.ACCESS\n0\n1\n75.70\n7.65\n59.35\n70.90\n77.11\n80.58\n88.17\n▃▃▆▇▅\n\n\nPC.OWNER.WITHOUT.INTERNET.ACCESS\n0\n1\n8.77\n2.57\n2.03\n7.29\n8.35\n9.78\n15.20\n▁▃▇▂▂\n\n\n\n\n\n\n# install.packages(c(\"tree\",\"randomForest\",\"ranger\", \"rpart\", \"vip\", \"pdp\", \"caret\"))\nlibrary(tree)\ngenretree <- tree(nbc$Genre ~ ., \n                  data = demog[,-1], \n                  mincut = 1)\nnbc$genrepred <- predict(genretree, \n                         newdata = demog[,-1], \n                         type = \"class\")\n\n\n# tree plot (dendrogram)\nplot(genretree, col=8, lwd=2)\ntext(genretree, label=\"yprob\")\n\n\n\n\nConsider predicting engagement from ratings and genre.\n\nLeaf predictions are expected engagement.\n\n\n# mincut=1 allows for leaves containing a single show,\n# with expected engagement that single show's PE.\nnbctree <- tree(PE ~ Genre + GRP, data=nbc[,-1], mincut=1)\nnbc$PEpred <- predict(nbctree, newdata=nbc[,-1])\n\n## tree plot (dendrogram)\nplot(nbctree, col=8, lwd=2)\ntext(nbctree)\n\n\n\nggplot(nbc) +\n  geom_point(aes(x = GRP, y = PE, color = Genre) ) +\n  geom_line(aes(x = GRP, y = PEpred, color = Genre) )\n\n\n\n\n\nPE increases with GRP, but in jumps!"
  },
  {
    "objectID": "mba-ch7-tree-based-methods.html#cv-tree",
    "href": "mba-ch7-tree-based-methods.html#cv-tree",
    "title": "Tree-based Methods",
    "section": "CV Tree",
    "text": "CV Tree\n\nThe biggest challenge with CART models is avoiding overfit.\n\nFor CART, the usual solution is to rely on cross validation (CV).\nThe way to cross-validate the fully fitted tree is to prune it by removing split rules from the bottom up:\n\nAt each step, remove the split that contributes least to deviance reduction.\n\nThis is a reverse to CART’s growth process.\n\nPruning yields candidate tree.\n\nEach prune step produces a candidate tree model, and we can compare their out-of-sample prediction performance through CV.\n\n\n\n\nBoston Housing Data\n\nThe MASS package includes the Boston data.frame, which has 506 observations and 14 variables.\n\ncrim: per capita crime rate by town.\nzn: proportion of residential land zoned for lots over 25,000 sq.ft.\nindus: proportion of non-retail business acres per town.\nchas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\nnox: nitrogen oxides concentration (parts per 10 million).\nrm: average number of rooms per dwelling.\nage: proportion of owner-occupied units built prior to 1940.\ndis: weighted mean of distances to five Boston employment centres.\nrad: index of accessibility to radial highways.\ntax: full-value property-tax rate per $10,000.\nptratio: pupil-teacher ratio by town.\nblack: \\(1000(Bk - 0.63)^2\\) where \\(Bk\\) is the proportion of blacks by town.\nlstat: lower status of the population (percent).\nmedv: median value of owner-occupied homes in $1000s.\n\nFor more details about the data set, try ?Boston.\nThe goal is to predict housing values.\n\n\nlibrary(MASS)\n?Boston\nBoston <- MASS::Boston\n\n\n\n\n\n  \n\n\n\n\nskim(MASS::Boston)\n\n\nData summary\n\n\nName\nMASS::Boston\n\n\nNumber of rows\n506\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncrim\n0\n1\n3.61\n8.60\n0.01\n0.08\n0.26\n3.68\n88.98\n▇▁▁▁▁\n\n\nzn\n0\n1\n11.36\n23.32\n0.00\n0.00\n0.00\n12.50\n100.00\n▇▁▁▁▁\n\n\nindus\n0\n1\n11.14\n6.86\n0.46\n5.19\n9.69\n18.10\n27.74\n▇▆▁▇▁\n\n\nchas\n0\n1\n0.07\n0.25\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nnox\n0\n1\n0.55\n0.12\n0.38\n0.45\n0.54\n0.62\n0.87\n▇▇▆▅▁\n\n\nrm\n0\n1\n6.28\n0.70\n3.56\n5.89\n6.21\n6.62\n8.78\n▁▂▇▂▁\n\n\nage\n0\n1\n68.57\n28.15\n2.90\n45.02\n77.50\n94.07\n100.00\n▂▂▂▃▇\n\n\ndis\n0\n1\n3.80\n2.11\n1.13\n2.10\n3.21\n5.19\n12.13\n▇▅▂▁▁\n\n\nrad\n0\n1\n9.55\n8.71\n1.00\n4.00\n5.00\n24.00\n24.00\n▇▂▁▁▃\n\n\ntax\n0\n1\n408.24\n168.54\n187.00\n279.00\n330.00\n666.00\n711.00\n▇▇▃▁▇\n\n\nptratio\n0\n1\n18.46\n2.16\n12.60\n17.40\n19.05\n20.20\n22.00\n▁▃▅▅▇\n\n\nblack\n0\n1\n356.67\n91.29\n0.32\n375.38\n391.44\n396.22\n396.90\n▁▁▁▁▇\n\n\nlstat\n0\n1\n12.65\n7.14\n1.73\n6.95\n11.36\n16.96\n37.97\n▇▇▅▂▁\n\n\nmedv\n0\n1\n22.53\n9.20\n5.00\n17.02\n21.20\n25.00\n50.00\n▂▇▅▁▁\n\n\n\n\n\n\nSpliting training and testing data\n\n\nset.seed(42120532)\nindex <- sample(nrow(Boston),nrow(Boston)*0.80)\nBoston.train <- Boston[index,]\nBoston.test <- Boston[-index,]\n\n\nA bit of data visualization\n\n\nBoston_vis <- Boston %>%\n  gather(-medv, key = \"var\", value = \"value\") %>% \n  filter(var != \"chas\")\n\nggplot(Boston_vis, aes(x = value, y = medv)) +\n  geom_point(alpha = .33) +\n  geom_smooth() +\n  facet_wrap(~ var, scales = \"free\") \n\n\n\nggplot(Boston_vis, aes(y = value)) +\n  geom_boxplot(outlier.color = \"red\", outlier.shape = 1) +\n  facet_wrap(~ var, scales = \"free\") \n\n\n\nggplot(Boston_vis, aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ var, scales = \"free\") \n\n\n\n\n\nrpart()\n\nRuns 10-fold CV tree to tune \\(\\alpha\\) (CP) for pruning.\nSelects the number of terminal nodes via 1-SE rule.\n\n\n\nlibrary(rpart)\nboston_tree <- rpart(medv ~ .,\n                     data = Boston.train, method  = \"anova\")\nboston_tree\n\nn= 404 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 404 34343.3600 22.47178  \n   2) lstat>=9.725 237  5424.9600 17.30084  \n     4) lstat>=18.825 67  1000.6320 12.28358  \n       8) nox>=0.603 51   417.5804 10.88039 *\n       9) nox< 0.603 16   162.5594 16.75625 *\n     5) lstat< 18.825 170  2073.0290 19.27824  \n      10) lstat>=14.395 73   656.7096 17.12877 *\n      11) lstat< 14.395 97   825.2184 20.89588 *\n   3) lstat< 9.725 167 13588.0500 29.81018  \n     6) rm< 7.4525 143  5781.9690 27.19790  \n      12) dis>=1.95265 135  3329.1170 26.35185  \n        24) rm< 6.722 88   927.6799 23.74886 *\n        25) rm>=6.722 47   688.8094 31.22553 *\n      13) dis< 1.95265 8   725.5350 41.47500 *\n     7) rm>=7.4525 24  1015.9250 45.37500 *\n\n\n\nprintcp displays cp table for Fitted rpart() object:\n\n\nprintcp(boston_tree)\n\n\nRegression tree:\nrpart(formula = medv ~ ., data = Boston.train, method = \"anova\")\n\nVariables actually used in tree construction:\n[1] dis   lstat nox   rm   \n\nRoot node error: 34343/404 = 85.008\n\nn= 404 \n\n        CP nsplit rel error  xerror     xstd\n1 0.446385      0   1.00000 1.00634 0.095395\n2 0.197714      1   0.55362 0.65150 0.067953\n3 0.068464      2   0.35590 0.39758 0.050182\n4 0.050296      3   0.28744 0.32195 0.047053\n5 0.049868      4   0.23714 0.32917 0.048158\n6 0.017212      5   0.18727 0.29925 0.045811\n7 0.012244      6   0.17006 0.27112 0.046339\n8 0.010000      7   0.15782 0.27249 0.046513\n\n\n\nrpart.plot() plots the estimated tree structure from an rpart() object.\n\nWith method = \"anova\" (a continuous outcome variable), each node shows:\n\nthe predicted value;\nthe percentage of observations in the node.\n\n\n\n\nlibrary(rpart.plot)\nrpart.plot(boston_tree)\n\n\n\n\n\nWith method = \"class\" (a binary outcome variable), each node will show:\n\nthe predicted class;\nthe predicted probability;\nthe percentage of observations in the node.\n\n\n\n\nplotcp() gives a visual representation of the cross-validation results in an rpart() object.\n\nThe size of a decision tree is the number of leaf nodes (non-terminal nodes) in the tree.\n\n\n\nplotcp(boston_tree)\n\n\n\n\n\nWhat about the full tree? (cp = 0)\n\nThe control parameter in rpart() allows for controlling the rpart fit. (see rpart.fit)\n\ncp: complexity parameter. the minimum improvement in the model needed at each node.\n\nThe higher the cp, the smaller the size of tree.\n\nxval: number of cross-validations\n\n\n\n\nfull_boston_tree <- rpart(formula = medv ~ .,\n                       data = Boston.train, method  = \"anova\", \n                       control = list(cp = 0, xval = 10))\n\n\nrpart.plot(full_boston_tree)\n\n\n\n\n\nCompare the full tree with the pruned tree.\n\nWhich variable is not included in the pruned tree?\n\n\n\nplotcp(full_boston_tree)\n\n\n\n\n\nWe can train the CV trees with the caret package as well:\n\n\nlibrary(caret)\ncaret_boston_tree <- train(medv ~ .,\n                        data = Boston.train, method = \"rpart\",\n                        trControl = trainControl(method = \"cv\", number = 10),\n                        tuneLength = 20)\nggplot(caret_boston_tree)\n\n\n\n\n\nrpart.plot(caret_boston_tree$finalModel)"
  },
  {
    "objectID": "mba-ch7-tree-based-methods.html#random-forest",
    "href": "mba-ch7-tree-based-methods.html#random-forest",
    "title": "Tree-based Methods",
    "section": "Random Forest",
    "text": "Random Forest\n\nWhy should we try other tree models?\n\nCART automatically learns non-linear response functions and will discover interactions between variables.\n\nUnfortunately, it is tough to avoid overfit with CART.\nHigh variance, i.e. split a dataset in half and grow tress in each half, the result will be very different\n\n\nCART generally results in higher test set error rates.\nReal structure of the tree is not easily chosen via cross-validation (CV).\n\nOne way to mitigate the shortcomings of CART is bootstrap aggregation, or bagging.\n\n\n\n\nBagging Algorithm\n\n\n\n\n\n\n\n\n\n\nBootstrap is random sampling with replacement.\nAggregation is combining the results from many trees together, each constructed with a different bootstrapped sample of the data.\n\n\n\n\n\n\n\n\n\n\n\nReal structure that persists across datasets shows up in the average.\nA bagged ensemble of trees is also less likely to overfit the data.\nTo generate a prediction for a new point:\n\nRegression: take the average across the trees\nClassification: take the majority vote across the trees\n\nassuming each tree predicts a single class (could use probabilities instead…)\n\n\nBagging improves prediction accuracy via wisdom of the crowds but at the expense of interpretability.\n\nEasy to read one tree, but how do we read 500 trees?\nHowever, we can still use the measures of variable importance and partial dependence to summarize our models.\n\n\n\n\n\nRandom Forest Algorithm\n\nRandom forests are an extension of bagging.\n\nAt each split, the algorithm limits the variables considered to a random subset \\(m_{try}\\) of the given \\(p\\) number of variables.\nIt introduce \\(m_{try}\\) as a tuning parameter: typically use \\(p/3\\) for regression or \\(\\sqrt{p}\\) for classification.\n\n\n\n\n\n\n\n\n\n\n\n\nSplit-variable randomization adds more randomness to make each tree more independent of each other.\nThe final ensemble of trees is bagged to make the random forest predictions.\n\n\n\nSince the trees are constructed via bootstrapped data (samples with replacements), each sample is likely to have duplicate observations.\nOut-of-bag (OOB), original observations not contained in a single bootstrap sample, can be used to make out-of-sample predictive performance of the model.\n\n\n\nranger package is a popular & fast implementation (see randomForest for the original).\n\nLet’s consider the estimation with randomForest first.\n\n\n\nlibrary(randomForest)\nbag.boston <- randomForest(medv ~ ., data = Boston.train, \n                           mtry=13, ntree = 50,\n                           importance =TRUE)\nbag.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston.train, mtry = 13,      ntree = 50, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 50\nNo. of variables tried at each split: 13\n\n          Mean of squared residuals: 10.94723\n                    % Var explained: 87.12\n\nplot(bag.boston)\n\n\n\n\n\nNow Let’s consider the estimation with ranger.\n\n\nlibrary(ranger)\nbag.boston_ranger <- ranger(medv ~ ., data = Boston.train, \n                            mtry = 13, num.trees = 50,\n                            importance = \"impurity\")\nbag.boston_ranger\n\nRanger result\n\nCall:\n ranger(medv ~ ., data = Boston.train, mtry = 13, num.trees = 50,      importance = \"impurity\") \n\nType:                             Regression \nNumber of trees:                  50 \nSample size:                      404 \nNumber of independent variables:  13 \nMtry:                             13 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       10.58043 \nR squared (OOB):                  0.8758446 \n\n\n\n\nCV Tree vs. Random Forest\n\nWe can compare the performance of the CV CART and the RF via MSE.\nMSE from CV Tree\n\n\n# prediction\nboston.train.pred.CART <- predict(boston_tree, Boston.train)\nboston.test.pred.CART <- predict(boston_tree, Boston.test)\n\n# MSE\nmean((Boston.test$medv - boston.test.pred.CART)^2)\n\n[1] 22.94158\n\nmean((Boston.train$medv - boston.train.pred.CART)^2)\n\n[1] 13.41588\n\n\n\nMSE from Random Forest\n\n\n# prediction\nboston.train.pred.RF <- predict(bag.boston_ranger, Boston.train)$predictions\nboston.test.pred.RF <- predict(bag.boston_ranger, Boston.test)$predictions\n\n# MSE\nmean((Boston.test$medv - boston.test.pred.RF)^2)\n\n[1] 11.24414\n\nmean((Boston.train$medv - boston.train.pred.RF)^2)\n\n[1] 1.987043\n\n\n\n\n\n\nVariable Importance in the Tree-based Models\n\nVariable importance is measured based on reduction in SSE.\n\nMean Decrease Accuracy (% increase in MSE): This shows how much our model accuracy decreases if we leave out that variable.\nMean Decrease Gini (Increase in Node Purity) : This is a measure of variable importance based on the Gini impurity index used for the calculating the splits in trees.\n\n\n\n\nOut-of-bag samples for datum x1\n\n\n\n\n\n\n\n\nCalculating variable importance of variable v1\n\n\n\n\n\n\n\n\nSince we set importance not to equal to \"none\" when using rpart, caret, and ranger, we can evaluate variable importance using the left-out sample.\n\n\nvip(caret_boston_tree, geom = \"point\")\n\n\n\n\n\nvip(full_boston_tree, geom = \"point\")\n\n\n\n\n\nvip(boston_tree, geom = \"point\")\n\n\n\n\n\nvip(bag.boston_ranger, geom = \"point\")\n\n\n\n\n\n\nWe can also summarize the relationship between a predictor and the predicted outcome using a partial dependence plot\n\n\nlibrary(pdp)\n# predictor, lstat\npartial(bag.boston_ranger, pred.var = \"lstat\") %>% autoplot()\n\n\n\n\n\n# predictor, rm\npartial(bag.boston_ranger, pred.var = \"rm\") %>% autoplot()\n\n\n\n\n\n# predictor, rad\npartial(bag.boston_ranger, pred.var = \"rad\") %>% autoplot()"
  },
  {
    "objectID": "mba-ch7-tree-based-methods.html#references",
    "href": "mba-ch7-tree-based-methods.html#references",
    "title": "Tree-based Methods",
    "section": "References",
    "text": "References\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani and Jerome Friedman.\n\n\n\n\n\nModern Business Analytics by Matt Taddy, Leslie Hendrix, and Matthew Harding.\nPractical Data Science with R by Nina Zumel and John Mount.\nSummer Undergraduate Research Experience (SURE) 2022 in Statistics at Carnegie Mellon University by Ron Yurko."
  },
  {
    "objectID": "DANL210_hw3q.html",
    "href": "DANL210_hw3q.html",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 3",
    "section": "",
    "text": "Direction for Homework Assignment 3\n\nGo to the following IMDB website for\n\nTop Comedy Movies and TV Shows (sorted by popularity). https://www.imdb.com/search/title/?genres=comedy&explore=title_type,genres\n\nProvide your Python Selenium codes to scrape the table for Top 200 Comedy Movies and TV Shows.\n\nYou should create the following variables in the data frame:\n\nranking\ntitle\n\nyears\n\ncertificate\nruntime\ngenre\n\nrating\n\nmetascore\n\nplot\n\ndirector_stars\n\nvotes\n\n\nSave the data frame in the csv file."
  },
  {
    "objectID": "quarto_basic_example.html",
    "href": "quarto_basic_example.html",
    "title": "Best ML Model Ever!",
    "section": "",
    "text": "“The truth is rarely pure and never simple.”\n— Oscar Wilde\nThe number of variables is 4; the number of observations is 28,947.\nRoses are red, violets are blue.\nHere is one."
  },
  {
    "objectID": "quarto_basic_example.html#model-equations",
    "href": "quarto_basic_example.html#model-equations",
    "title": "Best ML Model Ever!",
    "section": "Model Equations",
    "text": "Model Equations\n\nAdd some model equation, for example, \\(Y_{i} = \\beta_{0} + \\beta_{1}X_{1,i} + \\beta_{2} X_{2i}^{2} + \\beta_{3}X_{1,i}\\times X_{2,i} + \\epsilon_{i}\\).\nFor the model equation, better to use $$. \\[y_{i} = \\beta_{1} * x_{1i} + \\beta_{2} * x_{1i}^{2} + \\epsilon_{i}\\]\n\n\\[\\begin{align}\ny_{i} = \\beta_{1} * x_{1i} \\\\ + \\beta_{2} * x_{1i}^{2} + \\epsilon_{i}\n\\end{align}\\]"
  },
  {
    "objectID": "quarto_basic_example.html#latex-tips",
    "href": "quarto_basic_example.html#latex-tips",
    "title": "Best ML Model Ever!",
    "section": "LaTeX tips",
    "text": "LaTeX tips\n\nWe can use & to align a math equation with multiple lines:\n\n\\[\\begin{align}\ny_{i} =\\;& \\beta_{1} * x_{1i} \\\\\n      &+ \\beta_{2} * x_{1i}^{2} + \\epsilon_{i}\n\\end{align}\\]\n\nIn latex equation, you can add horizontal space by using the followings:\n\n\\,\n\\;\nquad\nqquad"
  },
  {
    "objectID": "quarto_basic_example.html#inserting-figures",
    "href": "quarto_basic_example.html#inserting-figures",
    "title": "Best ML Model Ever!",
    "section": "Inserting Figures",
    "text": "Inserting Figures\n\nknitr::include_graphics(\"lec_figs/choe_grade_hist.png\")\n\n\n\n\nFigure 1: Historical Distribution of Letter Grades in Choe’s courses."
  },
  {
    "objectID": "DANL210_hw3a.html",
    "href": "DANL210_hw3a.html",
    "title": "DANL 210: Data Preparation and ManagementHomework Assignment 3",
    "section": "",
    "text": "Go to the following IMDB website for\n\nTop Comedy Movies and TV Shows (sorted by popularity). https://www.imdb.com/search/title/?genres=comedy&explore=title_type,genres\n\nProvide your Python Selenium codes to scrape the table for Top 200 Comedy Movies and TV Shows.\n\nYou should create the following variables in the data frame:\n\nranking\ntitle\n\nyears\n\ncertificate\nruntime\ngenre\n\nrating\n\nmetascore\n\nplot\n\ndirector_stars\n\nvotes\n\n\nSave the data frame in the csv file.\n\n\n\n\nimport os\nwd_path = '/Users/byeong-hakchoe/Google Drive/suny-geneseo/spring2023/lecture_codes/'\nos.chdir(wd_path)  \nos.getcwd()\n\n\n\n\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nimport time\nimport pandas as pd\nimport math\n\n\n\n\n\noptions = Options()\noptions.add_argument(\"window-size=1400,1200\")\ndriver = webdriver.Chrome(chrome_options = options, \n                          executable_path = 'chromedriver') ## the file of chromedriver is in the working directory.\n\n\n\n\n\ndriver.set_window_size(1300, 1800)\ndriver.set_window_position(1250, 0)\n\n\n\n\n\nurl = 'https://www.imdb.com/search/title/?genres=comedy&explore=title_type,genres'\ndriver.get(url)\ntime.sleep(4)\n#body = driver.find_element_by_css_selector('body')\n#body.send_keys(Keys.PAGE_DOWN)\n\nn_title = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[1]/div[2]/span[1]')\nn_title.location_once_scrolled_into_view\n\nn_title = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[1]/div[2]/span[1]').text\nn_title = n_title.split('of ')\nn_title = n_title[1]\nn_title = n_title.split(' titles')\nn_title = n_title[0]\nn_title = n_title.replace(',', '')\nn_title = int(n_title)\nn_step = math.floor(n_title / 50)\n\n\n\n\n\ndf = pd.DataFrame()\nfor k in range(0, 4):  # range(0, n_step) \n    #l = 1 + 50*k\n    #driver.get(\"https://www.imdb.com/search/title/?genres=comedy&start=\"+str(l)+\"&explore=title_type,genres&ref_=adv_nxt\")\n    for j in range(1, 51): #51\n        ranking = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[3]/div/div['+str(j)+']/div[3]/h3/span[1]').text\n        ranking = ranking.replace('.', '')\n        ranking = int(ranking)\n        ranking = pd.DataFrame([ranking]) \n        ranking.columns = ['ranking']   \n        \n        title = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[3]/div/div['+str(j)+']/div[3]/h3/a').text\n        title = pd.DataFrame([title]) \n        title.columns = ['title']\n        \n        years = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[3]/div/div['+str(j)+']/div[3]/h3/span[2]').text\n        years = years.replace('(', '')\n        years = years.replace(')', '')\n        years = pd.DataFrame([years]) \n        years.columns = ['years']\n\n        certificate_runtime_genre = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[3]/div/div['+str(j)+']/div[3]/p[1]')\n        \n        try:\n            certificate = certificate_runtime_genre.find_element(By.CLASS_NAME, 'certificate').text\n            certificate = pd.DataFrame([certificate]) \n            certificate.columns = ['certificate']\n        except:\n            certificate = ''\n            certificate = pd.DataFrame([certificate]) \n            certificate.columns = ['certificate']\n            \n        try:\n            runtime = certificate_runtime_genre.find_element(By.CLASS_NAME, 'runtime').text\n            runtime = runtime.replace(' min', '')\n            runtime = int(runtime)\n            runtime = pd.DataFrame([runtime]) \n            runtime.columns = ['runtime_minutes']\n        except:\n            runtime = ''\n            runtime = pd.DataFrame([runtime]) \n            runtime.columns = ['runtime_minutes']\n            \n        try:\n            genre = certificate_runtime_genre.find_element(By.CLASS_NAME, 'genre').text\n            genre = pd.DataFrame([genre]) \n            genre.columns = ['genre']\n        except:\n            genre = ''\n            genre = pd.DataFrame([genre]) \n            genre.columns = ['genre']\n        \n        try:\n            rating = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[3]/div/div['+str(j)+']/div[3]/div/div[1]').text\n            rating = float(rating)\n            rating = pd.DataFrame([rating]) \n            rating.columns = ['rating']\n        except:\n            rating = ''\n            rating = pd.DataFrame([rating]) \n            rating.columns = ['rating']\n\n        try:\n            metascore = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[3]/div/div['+str(j)+']/div[3]/div/div[3]/span').text\n            metascore = int(metascore)\n            metascore = pd.DataFrame([metascore]) \n            metascore.columns = ['metascore']\n        except:\n            metascore = ''\n            metascore = pd.DataFrame([metascore]) \n            metascore.columns = ['metascore']\n            \n        try:        \n            plot = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[3]/div/div['+str(j)+']/div[3]/p[2]').text\n            plot = pd.DataFrame([plot]) \n            plot.columns = ['plot']\n        except:\n            plot = ''\n            plot = pd.DataFrame([plot]) \n            plot.columns = ['plot']\n        \n        try:\n            director_stars = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[3]/div/div['+str(j)+']/div[3]/p[3]').text\n            director_stars = pd.DataFrame([director_stars]) \n            director_stars.columns = ['director_stars']\n        except:\n            director_stars = ''\n            director_stars = pd.DataFrame([director_stars]) \n            director_stars.columns = ['director_stars']\n            \n        try:\n            votes = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[3]/div/div['+str(j)+']/div[3]/p[4]/span[2]').text\n            votes = votes.replace(',', '')\n            votes = int(votes)\n            votes = pd.DataFrame([votes]) \n            votes.columns = ['votes']\n        except:\n            votes = ''\n            votes = pd.DataFrame([votes]) \n            votes.columns = ['votes']\n            \n        data = pd.concat([ranking, title, years, certificate, runtime, genre, \n                        rating, metascore, plot, director_stars, votes], axis=1) \n        df = df.append(data)\n\n        \n    try:\n        if k == 0:\n            next_field = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[4]/a')\n        else:\n            next_field = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[4]/a[2]')\n        next_field.click()\n        time.sleep(5)\n    except:\n        pass\n        \n    df = df.fillna('')\n            \ndf.to_csv(\"data/imdb_comedy_2023_0501.csv\",\n          index = False, encoding='utf-8-sig')"
  },
  {
    "objectID": "DANL200_hw4q.html",
    "href": "DANL200_hw4q.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 4",
    "section": "",
    "text": "Load R packages you need for Homework Assignment 4"
  },
  {
    "objectID": "DANL200_hw4q.html#variable-description",
    "href": "DANL200_hw4q.html#variable-description",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 4",
    "section": "Variable Description",
    "text": "Variable Description\n\nhh: an identifier of the purchasing household;\nX_purchase_desc: details on the purchased item;\nquantity: the number of items purchased;\nbrand: Bud Light, Busch Light, Coors Light, Miller Lite, or Natural Light;\nspent: total dollar value of purchase;\nbeer_floz: total volume of beer, in fluid ounces;\nprice_per_floz: price per fl.oz. (i.e., beer spent/beer floz);\ncontainer: the type of container;\npromo: Whether the item was promoted (coupon or otherwise);\nmarket: Scan-track market (or state if rural);\ndemographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher."
  },
  {
    "objectID": "DANL200_hw4q.html#q1a",
    "href": "DANL200_hw4q.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 4",
    "section": "Q1a",
    "text": "Q1a\n\nIn the data.frame, beer_mkts, keep the observations with only 'CAN' or 'NON REFILLABLE BOTTLE' type containers.\nRandomly divide the data.frame, beer_mkts, into training and test data.frames, dtrain and dtest, respectively.\nMake sure that you can replicate the randomization.\n\nApproximately 60% of observations in beer_mkts go to dtrain.\nThe rest of observations in beer_mkts go to dtest."
  },
  {
    "objectID": "DANL200_hw4q.html#q1b",
    "href": "DANL200_hw4q.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 4",
    "section": "Q1b",
    "text": "Q1b\nConsider the following formulas.\n\nformula_1 <- \n  log(price_per_floz) ~  \n        log(beer_floz) + brand + promo + market + container\n\nformula_2 <- \n  log(price_per_floz) ~  \n        log(beer_floz) * brand + promo + market + container \n\nformula_3 <- \n  log(price_per_floz) ~  \n        log(beer_floz) * brand * promo + market + container\n\n\nFor each of the formulas above, train the linear regression model using the dtrain data.frame.\n\nUse stargazer() to summarize all the regression results in one table.\n\n\n\nstargazer( model_1, model_2, model_3, \n           type = 'text', \n           omit = c('market') )"
  },
  {
    "objectID": "DANL200_hw4q.html#q1c",
    "href": "DANL200_hw4q.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 4",
    "section": "Q1c",
    "text": "Q1c\n\nAcross the three models in Q1b, how is the percentage change in the price of beer sensitive to the percentage change in the volume of beer purchases for each brand?"
  },
  {
    "objectID": "DANL200_hw4q.html#q1d",
    "href": "DANL200_hw4q.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 4",
    "section": "Q1d",
    "text": "Q1d\n\nConsider the following data.frames:\n\n\nthe observations with brand == 'BUD LIGHT' only in the dtrain data.frame;\n\n\nthe observations with brand == 'BUSCH LIGHT' only in the dtrain data.frame;\n\n\nthe observations with brand == 'COORS LIGHT' only in the dtrain data.frame;\n\n\nthe observations with brand == 'MILLER LITE' only in the dtrain data.frame;\n\n\nthe observations with brand == 'NATURAL LIGHT' in the dtrain data.frame.\n\n\nConsider the forumla, formula0:\n\n\nformula0 <- \n  log(price_per_floz) ~ log(beer_floz) \n\n\nTrain the linear regression model with formula0 using each of the data.frames above.\nHow is the percentage change in the price of beer sensitive to the percentage change in the volume of beer purchases for each brand?\nAre these sensitivities the same as the ones from the linear regression model with the following formula, formula_int, and the data, dtrain?\n\n\nformula_int <- \n  log(price_per_floz) ~ log(beer_floz) * brand"
  },
  {
    "objectID": "DANL200_hw4q.html#q1e",
    "href": "DANL200_hw4q.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 4",
    "section": "Q1e",
    "text": "Q1e\n\nCalculate the mean squared error (MSE) for each model in Q1b.\nWhich model do you prefer most? Why?"
  },
  {
    "objectID": "DANL200_hw3a.html",
    "href": "DANL200_hw3a.html",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "",
    "text": "Load R packages you need for Homework Assignment 3"
  },
  {
    "objectID": "DANL200_hw3a.html#variable-description",
    "href": "DANL200_hw3a.html#variable-description",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Variable Description",
    "text": "Variable Description\n\nhh: an identifier of the purchasing household;\nX_purchase_desc: details on the purchased item;\nquantity: the number of items purchased;\nbrand: Bud Light, Busch Light, Coors Light, Miller Lite, or Natural Light;\nspent: total dollar value of purchase;\nbeer_floz: total volume of beer, in fluid ounces;\nprice_per_floz: price per fl.oz. (i.e., beer spent/beer floz);\ncontainer: the type of container;\npromo: Whether the item was promoted (coupon or otherwise);\nmarket: Scan-track market (or state if rural);\ndemographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher."
  },
  {
    "objectID": "DANL200_hw3a.html#q1a",
    "href": "DANL200_hw3a.html#q1a",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1a",
    "text": "Q1a\nIn the data.frame, beer_mkts, keep the observations with only ‘CAN’ or ‘NON REFILLABLE BOTTLE’ type containers.\n\nbeer_mkts <- beer_mkts %>% \n  filter(container == 'CAN' | container == 'NON REFILLABLE BOTTLE')"
  },
  {
    "objectID": "DANL200_hw3a.html#q1b",
    "href": "DANL200_hw3a.html#q1b",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1b",
    "text": "Q1b\nIn the data.frame, beer_mkts, convert the character variable, market, to the factor variable with the first level ‘BUFFALO-ROCHESTER’.\n\nbeer_mkts <- beer_mkts %>% \n  mutate(market = factor(market),\n         market = relevel(market,\n                          'BUFFALO-ROCHESTER'))"
  },
  {
    "objectID": "DANL200_hw3a.html#q1c",
    "href": "DANL200_hw3a.html#q1c",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1c",
    "text": "Q1c\n\nCreate the data.frame, beer_mkts_NY, that include\n\nonly the following markets,\n\n\nBUFFALO-ROCHESTER;\nALBANY;\nSYRACUSE;\nSURBURBAN NY;\nEXURBAN NY;\nURBAN NY;\n\n\nonly the following beer brands\n\n\nBUD LIGHT;\nCOORS LIGHT;\nMILLER LITE.\n\nProvide both (1) ggplot with beer_mkts_NY and (2) a comment to describe how the relationship between log(price_per_floz) and log(beer_floz) varies by market and brand.\n\n\nbeer_mkts_NY <- beer_mkts %>% \n  filter(market %in% c('BUFFALO-ROCHESTER',\n                       'ALBANY', 'SYRACUSE',\n                       'SURBURBAN NY',\n                       'EXURBAN NY', 'URBAN NY') )%>% \n  filter(brand %in% c('BUD LIGHT',\n                       'COORS LIGHT', 'MILLER LITE') )\n\n\nggplot(beer_mkts_NY, aes(x = log(beer_floz),\n                         y = log(price_per_floz))) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = lm) +\n  facet_grid(market ~ brand)\n\n\n\n\n\nAcross the subregions, the demand for BUD LIGHT in URBAN NY seems to be the most sensitive to change in price.\nAcross the subregions, the demand for BUD LIGHT in ALBANY seems to be the least sensitive to change in price.\nAcross the subregions, the demand for COORS LIGHT in URBAN NY seems to be the most sensitive to change in price.\nAcross the subregions, the demand for COORS LIGHT in EXURBAN NY seems to be the least sensitive to change in price.\nAcross the subregions, the demand for MILLER LITE in EXURBAN NY seems to be the most sensitive to change in price.\nAcross the subregions, the demand for MILLER LITE in Albany seems to be the least sensitive to change in price."
  },
  {
    "objectID": "DANL200_hw3a.html#q1d",
    "href": "DANL200_hw3a.html#q1d",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1d",
    "text": "Q1d\n\nRandomly divide the data.frame, beer_mkts, into training and test data.frames, dtrain and dtest, respectively.\nMake sure that you can replicate the randomization.\n\nApproximately 60% of observations in beer_mkts go to dtrain.\nThe rest of observations in beer_mkts go to dtest.\n\n\n\nset.seed(123)\ngp <- runif(nrow(beer_mkts))\ndtrain <- beer_mkts %>% \n  filter(gp < .6)\ndtest <- beer_mkts %>% \n  filter(gp >= .6)"
  },
  {
    "objectID": "DANL200_hw3a.html#q1e",
    "href": "DANL200_hw3a.html#q1e",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1e",
    "text": "Q1e\nUse dtrain to train the linear regression model with the following formula.\n\nformula <- \n  log(price_per_floz) ~  \n    log(beer_floz) + market + container + brand\n\n\nProvide the summary of the linear regression.\n\n\nmodel <- lm(data = dtrain, formula)\n\n\nstargazer(model, \n          type = 'text', \n          keep = c('log(beer_floz)', \n                   'brand',\n                   'Albany', 'NY',\n                   'Syracus', \"NEW YORK\"\n                   ))\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlog(price_per_floz)\n\n\n\n\n\n\n\n\nmarketALBANY\n\n\n0.038***\n\n\n\n\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\nmarketEXURBAN NY\n\n\n0.193***\n\n\n\n\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\nmarketRURAL NEW YORK\n\n\n-0.050\n\n\n\n\n\n\n(0.060)\n\n\n\n\n\n\n\n\n\n\nmarketSURBURBAN NY\n\n\n0.054***\n\n\n\n\n\n\n(0.011)\n\n\n\n\n\n\n\n\n\n\nmarketURBAN NY\n\n\n0.159***\n\n\n\n\n\n\n(0.012)\n\n\n\n\n\n\n\n\n\n\nbrandBUSCH LIGHT\n\n\n-0.260***\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\nbrandCOORS LIGHT\n\n\n-0.003\n\n\n\n\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\nbrandMILLER LITE\n\n\n-0.012***\n\n\n\n\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\nbrandNATURAL LIGHT\n\n\n-0.318***\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n43,508\n\n\n\n\nR2\n\n\n0.553\n\n\n\n\nAdjusted R2\n\n\n0.552\n\n\n\n\nResidual Std. Error\n\n\n0.168 (df = 43410)\n\n\n\n\nF Statistic\n\n\n553.426*** (df = 97; 43410)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01"
  },
  {
    "objectID": "DANL200_hw3a.html#q1f",
    "href": "DANL200_hw3a.html#q1f",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1f",
    "text": "Q1f\nFrom the result in Q1e, interpret the beta estimates of the following variables. 1. marketALBANY 2. marketEXURBAN NY 3. marketRURAL NEW YORK 4. marketSURBURBAN NY 5. marketSYRACUSE 6. marketURBAN NY 7. brandBUSCH LIGHT\n\nb <- tidy(model) %>% \n  filter(term %in% c('marketALBANY', 'marketEXURBAN NY', 'marketRURAL NEW YORK', 'marketSURBURBAN NY', 'marketSYRACUSE', 'marketURBAN NY', 'brandBUSCH LIGHT', 'log(beer_floz)')) %>% \n  mutate(star = ifelse(p.value <= .1, TRUE, FALSE),\n         exp_b_perc = (exp(estimate) - 1) * 100 ) \n\n\nAll else being equal, price_per_floz in ALBANY is greater than that in BUFFALO-ROCHESTER by 3.91% on average.\nAll else being equal, price_per_floz in EXURBAN NY is greater than that in BUFFALO-ROCHESTER by 21.3% on average.\nThe beta estimate for marketRURAL NEW YORK is not statistically significant, so that we cannot reject the null hypothesis \\(\\beta_{\\text{marketRURAL NEW YORK}} \\,=\\,0\\).\nAll else being equal, price_per_floz in SURBURBAN NY is greater than that in BUFFALO-ROCHESTER by 5.51% on average.\nAll else being equal, price_per_floz in SYRACUSE is smaller than that in BUFFALO-ROCHESTER by 3.64% on average.\nAll else being equal, price_per_floz in URBAN NY is greater than that in BUFFALO-ROCHESTER by 17.23% on average.\nAll else being equal, price_per_floz for BUSCH LIGHT is smaller than that for BUD LIGHT by 22.93% on average."
  },
  {
    "objectID": "DANL200_hw3a.html#q1g",
    "href": "DANL200_hw3a.html#q1g",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1g",
    "text": "Q1g\nFrom the result in Q1e, interpret the beta estimate for log(beer_floz).\n\nAll else being equal, one percent increase in beer_floz is associated with a decrease in price_per_floz by 0.14% on average."
  },
  {
    "objectID": "DANL200_hw3a.html#q1h",
    "href": "DANL200_hw3a.html#q1h",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1h",
    "text": "Q1h\nMake a prediction on the price of one 12 fl.oz bottle of beer using the dtest data.frame and the regression result in Q1e.\n\ndtest <- dtest %>% \n  mutate(pred_log = predict(model, newdata = dtest),\n         pred = exp(pred_log),\n         pred_12floz = 12 * pred,\n         price_12floz = 12 * price_per_floz\n         )"
  },
  {
    "objectID": "DANL200_hw3a.html#q1i",
    "href": "DANL200_hw3a.html#q1i",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1i",
    "text": "Q1i\nProvide both (1) ggplot and (2) a simple comment to describe how the distribution of the predicted price of one 12 fl.oz bottle of beer varies by brand.\n\nggplot(dtest) +\n  geom_histogram(aes(x = pred_12floz, fill = brand),\n                 binwidth = .0125,\n                 show.legend = F) +\n  facet_grid(brand ~.)\n\n\n\n\n\nBUD LIGHT, COORS LIGHT, and MILLER LITE tend to be more expensive than BUSCH LIGHT and NATURAL LIGHT."
  },
  {
    "objectID": "DANL200_hw3a.html#q1j",
    "href": "DANL200_hw3a.html#q1j",
    "title": "DANL 200: Introduction to Data AnalyticsHomework Assignment 3",
    "section": "Q1j",
    "text": "Q1j\n\nUse ggplot to draw a residual plot.\nMake a simple comment on the residual plot.\n\n\nggplot(data = dtest, aes(x = pred_log, \n                         y = log(price_per_floz) - pred_log )) +\n  geom_point(color = 'grey60', alpha = .1) +\n  geom_smooth(color = 'blue') +\n  geom_hline(aes(yintercept = 0), color = 'red')\n\n\n\n\n\nOverall, the predictions are correct.\nThe residuals bounce randomly and form a cloud roughly around the perfect prediction line, implying that the statistical assumption on errors—\\(\\epsilon_{i}\\sim N(0, \\sigma^{2})\\)—seems to hold."
  }
]